<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Julia Package Listing - Testing Information</title>

    <style>/* -- Mix of julialang.org, Bootstrap 3, and custom -- */
body {
  background-color: white;
  font-family: Georgia, 'Liberation Serif', serif;
  font-size: 14px;
  color: #333;
  line-height: 1.42857143;
}
.site {
  max-width: 785px;
  margin: 2.5em auto 2em;
  padding: 0 1.5em;
}
a {
  color: #428bca;
  text-decoration: none;
}
h1, h2, h3, h4, h5, h6 {
  font-family: inherit;
  font-weight: 500;
  line-height: 1.1;
  color: inherit;
}
h1, h2, h3 {
    margin-top: 20px;
    margin-bottom: 10px;
}
h2 {
  font-size: 30px;
}
h3 {
    font-size: 24px;
}
h4 {
  font-size: 18px;
  margin-top: 10px;
  margin-bottom: 10px;
}
.titlebox {
  text-align: center;
  font-size: 120%;
  margin-top: 3em;
}
.ok     { background-color: #11AA11; } /*Tests passing*/
.fail   { background-color: #DD3333; } /*Tests failed*/
.skip   { background-color: #3333DD; } /*Tests skipped*/
.kill   { background-color: #222222; } /*Tests interrupted*/
.statusbox {
  width: 12px;
  height: 12px;
  display: inline-block;
}
hr {
  margin-top: 10px;
  margin-bottom: 0px;
  border: 0;
  border-top: 1px solid #eee;
}
.pkglisting h2 { margin-bottom: 0px; }
pre {margin: 0;}
@media (min-width: 785px) {
  .pkglisting {
    display: table;
    width: 100%;
  }
  .pkgnamedesc {
    display: table-cell;
    width: 50%;
  }
  .pkgvertest {
    display: table-cell;
    width:50%;
    text-align: right
  }
}
/* collapsible sections */
.collapsible {
  font-family: Georgia, 'Liberation Serif', serif;
  font-size: 14px;
  background: none;
  border: none;
  margin: 0;
  padding: 0;
  cursor: pointer;
}
.content {
  display: none;
  overflow: hidden;
}
</style>
  </head>

  <body>
  <div class="site">
    <!-- HEADER -->
    <div class="titlebox">
        <svg version="1.1" id="Layer_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" width="210px" height="142px" viewBox="0 0 310 216" enable-background="new 0 0 310 216" xml:space="preserve">

<!-- blue dot -->
<circle fill="#6b85dd" stroke="#4266d5" stroke-width="3" cx="50.5" cy="58.665" r="16.5"></circle>
<!-- red dot -->
<circle fill="#d66661" stroke="#c93d39" stroke-width="3" cx="212.459" cy="60.249" r="16.5"></circle>
<!-- green dot -->
<circle fill="#6bab5b" stroke="#3b972e" stroke-width="3" cx="233.834" cy="23.874" r="16.5"></circle>
<!-- purple dot -->
<circle fill="#aa7dc0" stroke="#945bb0" stroke-width="3" cx="255.459" cy="59.999" r="16.5"></circle>

<!-- "j" -->
<path fill="#252525" d="M37.216,138.427c0-15.839,0.006-31.679-0.018-47.517c-0.001-0.827,0.169-1.234,1.043-1.47
    c7.876-2.127,15.739-4.308,23.606-6.47c1.33-0.366,1.333-0.36,1.333,1.019c0,25.758,0.015,51.517-0.012,77.274
    c-0.006,5.514,0.245,11.032-0.272,16.543c-0.628,6.69-2.15,13.092-6.438,18.506c-3.781,4.771-8.898,7.25-14.767,8.338
    c-6.599,1.222-13.251,1.552-19.934,0.938c-4.616-0.423-9.045-1.486-12.844-4.363c-2.863-2.168-4.454-4.935-3.745-8.603
    c0.736-3.806,3.348-5.978,6.861-7.127c2.262-0.74,4.628-0.872,6.994-0.53c1.823,0.264,3.42,1.023,4.779,2.288
    c1.38,1.284,2.641,2.674,3.778,4.177c0.872,1.15,1.793,2.256,2.991,3.086c2.055,1.426,4,0.965,5.213-1.216
    c0.819-1.473,0.997-3.106,1.173-4.731c0.255-2.348,0.255-4.707,0.256-7.062C37.218,167.145,37.216,152.786,37.216,138.427z"></path>

<!-- "u" -->
<path fill="#252525" d="M125.536,162.479c-2.908,2.385-5.783,4.312-8.88,5.904c-10.348,5.323-20.514,4.521-30.324-1.253
    c-6.71-3.95-11.012-9.849-12.52-17.606c-0.236-1.213-0.363-2.438-0.363-3.688c0.01-19.797,0.017-39.593-0.02-59.39
    c-0.002-1.102,0.285-1.357,1.363-1.351c7.798,0.049,15.597,0.044,23.396,0.003c0.95-0.005,1.177,0.25,1.175,1.183
    c-0.027,19.356-0.025,38.713-0.018,58.07c0.002,6.34,3.599,10.934,9.672,12.42c2.13,0.521,4.19,0.396,6.173-0.6
    c4.26-2.139,7.457-5.427,10.116-9.307c0.333-0.487,0.224-1,0.224-1.51c0.007-19.635,0.016-39.271-0.02-58.904
    c-0.002-1.083,0.255-1.369,1.353-1.361c7.838,0.052,15.677,0.045,23.515,0.004c0.916-0.005,1.103,0.244,1.102,1.124
    c-0.025,27.677-0.026,55.353,0.002,83.024c0.001,0.938-0.278,1.099-1.139,1.095c-7.918-0.028-15.837-0.028-23.756-0.001
    c-0.815,0.003-1.1-0.166-1.073-1.037C125.581,167.117,125.536,164.928,125.536,162.479z"></path>

<!-- "l" -->
<path fill="#252525" d="M187.423,107.08c0,20.637-0.011,41.273,0.026,61.91c0.003,1.119-0.309,1.361-1.381,1.355
    c-7.799-0.052-15.598-0.047-23.396-0.008c-0.898,0.008-1.117-0.222-1.115-1.115c0.021-39.074,0.021-78.147,0-117.226
    c0-0.811,0.189-1.169,1.006-1.392c7.871-2.149,15.73-4.327,23.584-6.545c1.045-0.295,1.308-0.17,1.306,0.985
    C187.412,65.727,187.423,86.403,187.423,107.08z"></path>

<!-- "i" -->
<path fill="#252525" d="M223.46,126.477c0,14.155-0.011,28.312,0.021,42.467c0.002,1.027-0.164,1.418-1.332,1.408
    c-7.838-0.061-15.676-0.047-23.516-0.01c-0.881,0.004-1.121-0.189-1.119-1.104c0.026-26.153,0.025-52.307,0-78.458
    c0-0.776,0.203-1.101,0.941-1.302c7.984-2.172,15.972-4.35,23.938-6.596c1.049-0.296,1.08,0.031,1.078,0.886
    C223.454,98.004,223.46,112.239,223.46,126.477z"></path>

<!-- "a" -->
<path fill="#252525" d="M277.695,163.6c-0.786,0.646-1.404,1.125-2,1.635c-4.375,3.746-9.42,5.898-15.16,6.42
    c-5.792,0.527-11.479,0.244-16.934-2.047c-12.08-5.071-15.554-17.188-11.938-27.448c1.799-5.111,5.472-8.868,9.831-11.94
    c5.681-4.003,12.009-6.732,18.504-9.074c5.576-2.014,11.186-3.939,16.955-5.347c0.445-0.104,0.773-0.243,0.757-0.854
    c-0.136-4.389,0.261-8.79-0.479-13.165c-1.225-7.209-6.617-10.013-12.895-9.348c-0.516,0.055-1.029,0.129-1.536,0.241
    c-4.877,1.081-7.312,4.413-7.374,10.127c-0.02,1.729-0.229,3.418-0.693,5.084c-0.906,3.229-2.969,5.354-6.168,6.266
    c-3.422,0.979-6.893,0.998-10.23-0.305c-6.529-2.543-8.877-10.164-5.12-16.512c2.249-3.799,5.606-6.4,9.461-8.405
    c6.238-3.246,12.914-4.974,19.896-5.537c7.565-0.61,15.096-0.366,22.49,1.507c4.285,1.085,8.312,2.776,11.744,5.657
    c4.473,3.749,6.776,8.647,6.812,14.374c0.139,21.477,0.096,42.951,0.143,64.428c0.002,0.799-0.248,0.983-1.021,0.98
    c-8.035-0.025-16.074-0.023-24.113-0.001c-0.716,0.002-0.973-0.146-0.941-0.915C277.736,167.562,277.695,165.698,277.695,163.6z
     M277.695,126.393c-4.793,2.104-9.25,4.373-13.287,7.408c-2.151,1.618-4.033,3.483-5.732,5.581
    c-4.229,5.226-1.988,13.343,1.693,16.599c1.592,1.406,3.359,1.906,5.419,1.521c1.621-0.307,3.149-0.857,4.549-1.734
    c1.521-0.951,2.949-2.072,4.539-2.887c2.31-1.18,2.97-2.861,2.894-5.445C277.561,140.484,277.695,133.527,277.695,126.393z"></path>

</svg>

        <h1>
          <a name="GaussianMixtures">GaussianMixtures</a>
        </h1>

        <p>
          <a href="../index.html#GaussianMixtures">← Back to package list</a>
        </p>

    </div>

    <p>
      If you think that there is an error in how your package is being tested or represented, please file an issue at <a href="https://github.com/JuliaComputing/NewPkgEval.jl">NewPkgEval.jl</a>, making sure to read the FAQ first.
    </p>


    <h3>Results with Julia v1.0.5</h3>

    <p>
      Testing was <strong>successful</strong>.
      Last evaluation was 1 day, 15 hours ago and took 10 minutes, 19 seconds.
    </p>

    <p>
      Click <a href="/home/tim/Julia/pkg/NewPkgEval/site/build/logs/GaussianMixtures/1.0.5.log">here</a> to download the log file.
      
    </p>

      <button class="collapsible">Click here to show the log contents.</button>
      <div class="content">
      <pre> Resolving package versions...
 Installed GaussianMixtures ─── v0.3.0
 Installed LegacyStrings ────── v0.4.1
 Installed Blosc ────────────── v0.5.1
 Installed BinDeps ──────────── v0.8.10
 Installed Arpack ───────────── v0.3.1
 Installed SpecialFunctions ─── v0.8.0
 Installed BinaryProvider ───── v0.5.8
 Installed DataStructures ───── v0.17.6
 Installed DataAPI ──────────── v1.1.0
 Installed URIParser ────────── v0.4.0
 Installed ScikitLearnBase ──── v0.5.0
 Installed PDMats ───────────── v0.9.10
 Installed Distances ────────── v0.8.2
 Installed HDF5 ─────────────── v0.12.5
 Installed StaticArrays ─────── v0.12.1
 Installed Distributions ────── v0.21.8
 Installed SortingAlgorithms ── v0.3.1
 Installed JLD ──────────────── v0.9.1
 Installed Missings ─────────── v0.4.3
 Installed StatsBase ────────── v0.32.0
 Installed FileIO ───────────── v1.0.7
 Installed OrderedCollections ─ v1.1.0
 Installed NearestNeighbors ─── v0.4.3
 Installed Rmath ────────────── v0.5.1
 Installed CMakeWrapper ─────── v0.2.3
 Installed CMake ────────────── v1.1.2
 Installed StatsFuns ────────── v0.9.0
 Installed QuadGK ───────────── v2.1.1
 Installed Parameters ───────── v0.12.0
 Installed Clustering ───────── v0.13.3
 Installed Compat ───────────── v2.2.0
  Updating `~&#x2F;.julia&#x2F;environments&#x2F;v1.0&#x2F;Project.toml`
  [cc18c42c] + GaussianMixtures v0.3.0
  Updating `~&#x2F;.julia&#x2F;environments&#x2F;v1.0&#x2F;Manifest.toml`
  [7d9fca2a] + Arpack v0.3.1
  [9e28174c] + BinDeps v0.8.10
  [b99e7846] + BinaryProvider v0.5.8
  [a74b3585] + Blosc v0.5.1
  [631607c0] + CMake v1.1.2
  [d5fb7624] + CMakeWrapper v0.2.3
  [aaaa29a8] + Clustering v0.13.3
  [34da2185] + Compat v2.2.0
  [9a962f9c] + DataAPI v1.1.0
  [864edb3b] + DataStructures v0.17.6
  [b4f34e82] + Distances v0.8.2
  [31c24e10] + Distributions v0.21.8
  [5789e2e9] + FileIO v1.0.7
  [cc18c42c] + GaussianMixtures v0.3.0
  [f67ccb44] + HDF5 v0.12.5
  [4138dd39] + JLD v0.9.1
  [1b4a561d] + LegacyStrings v0.4.1
  [e1d29d7a] + Missings v0.4.3
  [b8a86587] + NearestNeighbors v0.4.3
  [bac558e1] + OrderedCollections v1.1.0
  [90014a1f] + PDMats v0.9.10
  [d96e819e] + Parameters v0.12.0
  [1fd47b50] + QuadGK v2.1.1
  [79098fc4] + Rmath v0.5.1
  [6e75b9c4] + ScikitLearnBase v0.5.0
  [a2af1166] + SortingAlgorithms v0.3.1
  [276daf66] + SpecialFunctions v0.8.0
  [90137ffa] + StaticArrays v0.12.1
  [2913bbd2] + StatsBase v0.32.0
  [4c63d2b9] + StatsFuns v0.9.0
  [30578b45] + URIParser v0.4.0
  [2a0f44e3] + Base64 
  [ade2ca70] + Dates 
  [8bb1440f] + DelimitedFiles 
  [8ba89e20] + Distributed 
  [b77e0a4c] + InteractiveUtils 
  [76f85450] + LibGit2 
  [8f399da3] + Libdl 
  [37e2e46d] + LinearAlgebra 
  [56ddb016] + Logging 
  [d6f4376e] + Markdown 
  [a63ad114] + Mmap 
  [44cfe95a] + Pkg 
  [de0858da] + Printf 
  [9abbd945] + Profile 
  [3fa0cd96] + REPL 
  [9a3f8284] + Random 
  [ea8e919c] + SHA 
  [9e88b42a] + Serialization 
  [1a1011a3] + SharedArrays 
  [6462fe0b] + Sockets 
  [2f01184e] + SparseArrays 
  [10745b16] + Statistics 
  [4607b0f0] + SuiteSparse 
  [8dfed614] + Test 
  [cf7118a7] + UUIDs 
  [4ec0a83e] + Unicode 
  Building Arpack ──────────→ `~&#x2F;.julia&#x2F;packages&#x2F;Arpack&#x2F;cu5By&#x2F;deps&#x2F;build.log`
  Building CMake ───────────→ `~&#x2F;.julia&#x2F;packages&#x2F;CMake&#x2F;nSK2r&#x2F;deps&#x2F;build.log`
  Building Blosc ───────────→ `~&#x2F;.julia&#x2F;packages&#x2F;Blosc&#x2F;lzFr0&#x2F;deps&#x2F;build.log`
  Building SpecialFunctions → `~&#x2F;.julia&#x2F;packages&#x2F;SpecialFunctions&#x2F;ne2iw&#x2F;deps&#x2F;build.log`
  Building HDF5 ────────────→ `~&#x2F;.julia&#x2F;packages&#x2F;HDF5&#x2F;Zh9on&#x2F;deps&#x2F;build.log`
  Building Rmath ───────────→ `~&#x2F;.julia&#x2F;packages&#x2F;Rmath&#x2F;4wt82&#x2F;deps&#x2F;build.log`
   Testing GaussianMixtures
    Status `&#x2F;tmp&#x2F;tmpa0SLbh&#x2F;Manifest.toml`
  [7d9fca2a] Arpack v0.3.1
  [9e28174c] BinDeps v0.8.10
  [b99e7846] BinaryProvider v0.5.8
  [a74b3585] Blosc v0.5.1
  [631607c0] CMake v1.1.2
  [d5fb7624] CMakeWrapper v0.2.3
  [aaaa29a8] Clustering v0.13.3
  [34da2185] Compat v2.2.0
  [9a962f9c] DataAPI v1.1.0
  [864edb3b] DataStructures v0.17.6
  [b4f34e82] Distances v0.8.2
  [31c24e10] Distributions v0.21.8
  [5789e2e9] FileIO v1.0.7
  [cc18c42c] GaussianMixtures v0.3.0
  [f67ccb44] HDF5 v0.12.5
  [4138dd39] JLD v0.9.1
  [1b4a561d] LegacyStrings v0.4.1
  [e1d29d7a] Missings v0.4.3
  [b8a86587] NearestNeighbors v0.4.3
  [bac558e1] OrderedCollections v1.1.0
  [90014a1f] PDMats v0.9.10
  [d96e819e] Parameters v0.12.0
  [1fd47b50] QuadGK v2.1.1
  [79098fc4] Rmath v0.5.1
  [6e75b9c4] ScikitLearnBase v0.5.0
  [a2af1166] SortingAlgorithms v0.3.1
  [276daf66] SpecialFunctions v0.8.0
  [90137ffa] StaticArrays v0.12.1
  [2913bbd2] StatsBase v0.32.0
  [4c63d2b9] StatsFuns v0.9.0
  [30578b45] URIParser v0.4.0
  [2a0f44e3] Base64  [`@stdlib&#x2F;Base64`]
  [ade2ca70] Dates  [`@stdlib&#x2F;Dates`]
  [8bb1440f] DelimitedFiles  [`@stdlib&#x2F;DelimitedFiles`]
  [8ba89e20] Distributed  [`@stdlib&#x2F;Distributed`]
  [b77e0a4c] InteractiveUtils  [`@stdlib&#x2F;InteractiveUtils`]
  [76f85450] LibGit2  [`@stdlib&#x2F;LibGit2`]
  [8f399da3] Libdl  [`@stdlib&#x2F;Libdl`]
  [37e2e46d] LinearAlgebra  [`@stdlib&#x2F;LinearAlgebra`]
  [56ddb016] Logging  [`@stdlib&#x2F;Logging`]
  [d6f4376e] Markdown  [`@stdlib&#x2F;Markdown`]
  [a63ad114] Mmap  [`@stdlib&#x2F;Mmap`]
  [44cfe95a] Pkg  [`@stdlib&#x2F;Pkg`]
  [de0858da] Printf  [`@stdlib&#x2F;Printf`]
  [9abbd945] Profile  [`@stdlib&#x2F;Profile`]
  [3fa0cd96] REPL  [`@stdlib&#x2F;REPL`]
  [9a3f8284] Random  [`@stdlib&#x2F;Random`]
  [ea8e919c] SHA  [`@stdlib&#x2F;SHA`]
  [9e88b42a] Serialization  [`@stdlib&#x2F;Serialization`]
  [1a1011a3] SharedArrays  [`@stdlib&#x2F;SharedArrays`]
  [6462fe0b] Sockets  [`@stdlib&#x2F;Sockets`]
  [2f01184e] SparseArrays  [`@stdlib&#x2F;SparseArrays`]
  [10745b16] Statistics  [`@stdlib&#x2F;Statistics`]
  [4607b0f0] SuiteSparse  [`@stdlib&#x2F;SuiteSparse`]
  [8dfed614] Test  [`@stdlib&#x2F;Test`]
  [cf7118a7] UUIDs  [`@stdlib&#x2F;UUIDs`]
  [4ec0a83e] Unicode  [`@stdlib&#x2F;Unicode`]
[ Info: Testing Data
(100000, -1.946852905862491e7, [75114.6, 24885.4], [12806.0 -4479.47 28305.1; -13293.5 4373.38 -28623.5], Array{Float64,2}[[71824.8 1278.23 -7992.24; 1278.23 74680.5 2381.81; -7992.24 2381.81 57845.9], [28565.3 -1267.14 8134.49; -1267.14 25197.8 -2591.57; 8134.49 -2591.57 42502.9]])
┌ Warning: rmprocs: process 1 not removed
└ @ Distributed &#x2F;buildworker&#x2F;worker&#x2F;package_linux64&#x2F;build&#x2F;usr&#x2F;share&#x2F;julia&#x2F;stdlib&#x2F;v1.0&#x2F;Distributed&#x2F;src&#x2F;cluster.jl:932
[ Info: Initializing GMM, 8 Gaussians LinearAlgebra.diag covariance 2 dimensions using 272 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.371584e+03
      1       1.114086e+03      -2.574984e+02 |        7
      2       1.071514e+03      -4.257160e+01 |        4
      3       1.028673e+03      -4.284113e+01 |        4
      4       9.717730e+02      -5.690031e+01 |        0
      5       9.717730e+02       0.000000e+00 |        0
K-means converged with 5 iterations (objv = 971.7730182956766)
┌ Info: K-means with 272 data points using 5 iterations
└ 11.3 data points per parameter
[ Info: Running 0 iterations EM on full cov GMM with 8 Gaussians in 2 dimensions
┌ Info: EM with 272 data points 0 iterations avll -2.070196
└ 5.8 data points per parameter
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = lowerbound(::VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at bayes.jl:221
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;bayes.jl:221
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = lowerbound(::VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at bayes.jl:221
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;bayes.jl:221
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = _broadcast_getindex at broadcast.jl:582 [inlined]
└ @ Core .&#x2F;broadcast.jl:582
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = lowerbound(::VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at bayes.jl:230
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;bayes.jl:230
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = _broadcast_getindex at broadcast.jl:582 [inlined]
└ @ Core .&#x2F;broadcast.jl:582
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = _broadcast_getindex at broadcast.jl:582 [inlined]
└ @ Core .&#x2F;broadcast.jl:582
[ Info: iteration 1, lowerbound -3.858629
[ Info: iteration 2, lowerbound -3.758382
[ Info: iteration 3, lowerbound -3.656562
[ Info: iteration 4, lowerbound -3.534972
[ Info: iteration 5, lowerbound -3.402960
[ Info: iteration 6, lowerbound -3.281427
[ Info: dropping number of Gaussions to 6
[ Info: iteration 7, lowerbound -3.171704
[ Info: iteration 8, lowerbound -3.065315
[ Info: iteration 9, lowerbound -2.978923
[ Info: iteration 10, lowerbound -2.913565
[ Info: dropping number of Gaussions to 5
[ Info: iteration 11, lowerbound -2.861031
[ Info: dropping number of Gaussions to 4
[ Info: iteration 12, lowerbound -2.817605
[ Info: iteration 13, lowerbound -2.796878
[ Info: dropping number of Gaussions to 3
[ Info: iteration 14, lowerbound -2.788687
[ Info: iteration 15, lowerbound -2.779531
[ Info: iteration 16, lowerbound -2.771293
[ Info: iteration 17, lowerbound -2.758434
[ Info: iteration 18, lowerbound -2.738408
[ Info: iteration 19, lowerbound -2.707956
[ Info: iteration 20, lowerbound -2.663954
[ Info: iteration 21, lowerbound -2.605653
[ Info: iteration 22, lowerbound -2.537735
[ Info: iteration 23, lowerbound -2.470429
[ Info: iteration 24, lowerbound -2.413427
[ Info: iteration 25, lowerbound -2.369692
[ Info: iteration 26, lowerbound -2.337342
[ Info: iteration 27, lowerbound -2.315777
[ Info: iteration 28, lowerbound -2.307451
[ Info: dropping number of Gaussions to 2
[ Info: iteration 29, lowerbound -2.302964
[ Info: iteration 30, lowerbound -2.299262
[ Info: iteration 31, lowerbound -2.299257
[ Info: iteration 32, lowerbound -2.299255
[ Info: iteration 33, lowerbound -2.299254
[ Info: iteration 34, lowerbound -2.299253
[ Info: iteration 35, lowerbound -2.299253
[ Info: iteration 36, lowerbound -2.299253
[ Info: iteration 37, lowerbound -2.299253
[ Info: iteration 38, lowerbound -2.299253
[ Info: iteration 39, lowerbound -2.299253
[ Info: iteration 40, lowerbound -2.299253
[ Info: iteration 41, lowerbound -2.299253
[ Info: iteration 42, lowerbound -2.299253
[ Info: iteration 43, lowerbound -2.299253
[ Info: iteration 44, lowerbound -2.299253
[ Info: iteration 45, lowerbound -2.299253
[ Info: iteration 46, lowerbound -2.299253
[ Info: iteration 47, lowerbound -2.299253
[ Info: iteration 48, lowerbound -2.299253
[ Info: iteration 49, lowerbound -2.299253
[ Info: iteration 50, lowerbound -2.299253
[ Info: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
History[Sat Nov 23 19:54:55 2019: Initializing GMM, 8 Gaussians LinearAlgebra.diag covariance 2 dimensions using 272 data points
, Sat Nov 23 19:55:02 2019: K-means with 272 data points using 5 iterations
11.3 data points per parameter
, Sat Nov 23 19:55:04 2019: EM with 272 data points 0 iterations avll -2.070196
5.8 data points per parameter
, Sat Nov 23 19:55:06 2019: GMM converted to Variational GMM
, Sat Nov 23 19:55:15 2019: iteration 1, lowerbound -3.858629
, Sat Nov 23 19:55:15 2019: iteration 2, lowerbound -3.758382
, Sat Nov 23 19:55:15 2019: iteration 3, lowerbound -3.656562
, Sat Nov 23 19:55:15 2019: iteration 4, lowerbound -3.534972
, Sat Nov 23 19:55:15 2019: iteration 5, lowerbound -3.402960
, Sat Nov 23 19:55:15 2019: iteration 6, lowerbound -3.281427
, Sat Nov 23 19:55:16 2019: dropping number of Gaussions to 6
, Sat Nov 23 19:55:16 2019: iteration 7, lowerbound -3.171704
, Sat Nov 23 19:55:16 2019: iteration 8, lowerbound -3.065315
, Sat Nov 23 19:55:16 2019: iteration 9, lowerbound -2.978923
, Sat Nov 23 19:55:16 2019: iteration 10, lowerbound -2.913565
, Sat Nov 23 19:55:16 2019: dropping number of Gaussions to 5
, Sat Nov 23 19:55:16 2019: iteration 11, lowerbound -2.861031
, Sat Nov 23 19:55:16 2019: dropping number of Gaussions to 4
, Sat Nov 23 19:55:16 2019: iteration 12, lowerbound -2.817605
, Sat Nov 23 19:55:16 2019: iteration 13, lowerbound -2.796878
, Sat Nov 23 19:55:16 2019: dropping number of Gaussions to 3
, Sat Nov 23 19:55:16 2019: iteration 14, lowerbound -2.788687
, Sat Nov 23 19:55:16 2019: iteration 15, lowerbound -2.779531
, Sat Nov 23 19:55:16 2019: iteration 16, lowerbound -2.771293
, Sat Nov 23 19:55:16 2019: iteration 17, lowerbound -2.758434
, Sat Nov 23 19:55:16 2019: iteration 18, lowerbound -2.738408
, Sat Nov 23 19:55:16 2019: iteration 19, lowerbound -2.707956
, Sat Nov 23 19:55:16 2019: iteration 20, lowerbound -2.663954
, Sat Nov 23 19:55:16 2019: iteration 21, lowerbound -2.605653
, Sat Nov 23 19:55:16 2019: iteration 22, lowerbound -2.537735
, Sat Nov 23 19:55:16 2019: iteration 23, lowerbound -2.470429
, Sat Nov 23 19:55:16 2019: iteration 24, lowerbound -2.413427
, Sat Nov 23 19:55:16 2019: iteration 25, lowerbound -2.369692
, Sat Nov 23 19:55:16 2019: iteration 26, lowerbound -2.337342
, Sat Nov 23 19:55:16 2019: iteration 27, lowerbound -2.315777
, Sat Nov 23 19:55:16 2019: iteration 28, lowerbound -2.307451
, Sat Nov 23 19:55:16 2019: dropping number of Gaussions to 2
, Sat Nov 23 19:55:16 2019: iteration 29, lowerbound -2.302964
, Sat Nov 23 19:55:16 2019: iteration 30, lowerbound -2.299262
, Sat Nov 23 19:55:16 2019: iteration 31, lowerbound -2.299257
, Sat Nov 23 19:55:16 2019: iteration 32, lowerbound -2.299255
, Sat Nov 23 19:55:16 2019: iteration 33, lowerbound -2.299254
, Sat Nov 23 19:55:16 2019: iteration 34, lowerbound -2.299253
, Sat Nov 23 19:55:16 2019: iteration 35, lowerbound -2.299253
, Sat Nov 23 19:55:16 2019: iteration 36, lowerbound -2.299253
, Sat Nov 23 19:55:16 2019: iteration 37, lowerbound -2.299253
, Sat Nov 23 19:55:16 2019: iteration 38, lowerbound -2.299253
, Sat Nov 23 19:55:16 2019: iteration 39, lowerbound -2.299253
, Sat Nov 23 19:55:16 2019: iteration 40, lowerbound -2.299253
, Sat Nov 23 19:55:16 2019: iteration 41, lowerbound -2.299253
, Sat Nov 23 19:55:16 2019: iteration 42, lowerbound -2.299253
, Sat Nov 23 19:55:16 2019: iteration 43, lowerbound -2.299253
, Sat Nov 23 19:55:16 2019: iteration 44, lowerbound -2.299253
, Sat Nov 23 19:55:16 2019: iteration 45, lowerbound -2.299253
, Sat Nov 23 19:55:16 2019: iteration 46, lowerbound -2.299253
, Sat Nov 23 19:55:16 2019: iteration 47, lowerbound -2.299253
, Sat Nov 23 19:55:16 2019: iteration 48, lowerbound -2.299253
, Sat Nov 23 19:55:16 2019: iteration 49, lowerbound -2.299253
, Sat Nov 23 19:55:16 2019: iteration 50, lowerbound -2.299253
, Sat Nov 23 19:55:16 2019: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
]
α = [95.9549, 178.045]
β = [95.9549, 178.045]
m = [2.00023 53.852; 4.2503 79.2869]
ν = [97.9549, 180.045]
W = LinearAlgebra.UpperTriangular{Float64,Array{Float64,2}}[[0.375876 -0.00895312; 0.0 0.0127487], [0.184042 -0.00764405; 0.0 0.00858171]]
Kind: diag, size256
nx: 100000 sum(zeroth order stats): 99999.99999999999
avll from stats: -0.9916278690941359
avll from llpg:  -0.9916278690941364
avll direct:     -0.9916278690941364
sum posterior: 100000.0
Kind: full, size16
nx: 100000 sum(zeroth order stats): 100000.0
avll from stats: -0.9867088309206853
avll from llpg:  -0.9867088309206853
avll direct:     -0.9867088309206853
sum posterior: 100000.0
32×26 Array{Float64,2}:
  0.048693     0.186746     0.0988134    0.200585      0.0216456    0.0461786    0.0328706    0.160656     0.0192066    0.0251619    0.0513792     0.108305    -0.093561      0.110163     0.0804151   0.0278864   -0.155489     0.00181861   0.0892984   -0.0104739    0.0793037   -0.0613534    0.0172228   -0.0452041   -0.0668471    0.0602885 
  0.0878262    0.0613516    0.0935727    0.103139      0.0535115   -0.105491     0.00080434   0.0235253    0.125332    -0.101959    -0.0304149    -0.0823408   -0.110695      0.0168105    0.121218   -0.0549859    0.192962    -0.0377879   -0.0486241    0.163849    -0.0866308   -0.0133731    0.0427117    0.088522     0.0206138    0.0479038 
  0.0127178    0.0206538    0.193082    -0.0541816     0.179788    -0.0214977    0.0257732    0.0628038    0.0373012    0.00270238   0.0416249    -0.0752069   -0.26496      -0.0858564    0.0457309  -0.244629     0.108536    -0.0320279    0.0759333    0.0185032   -0.0338462    0.0227196    0.0539096   -0.0992045    0.108674    -0.0610104 
  0.107539    -0.121909    -0.123883     0.183992      0.149081     0.0901466   -0.0964101   -0.0213753    0.12271     -0.0104608   -0.0308564     0.00736474   0.0489577     0.0379605   -0.111723   -0.0514762    0.00570332  -0.0600061    0.0434111    0.049714     0.0450052    0.0560051   -0.0134004    0.0369792   -0.149087     0.0762568 
  0.0965566   -0.0220113    0.0820686   -0.0952926    -0.0482607   -0.0396224   -0.10672     -0.0703866    0.129299     0.0710165    0.0200158     0.0823668    0.0666144    -0.0680391   -0.162127   -0.0527931   -0.00293873   0.030541     0.0494105   -0.126379    -0.0631143   -0.10405     -0.0714655   -0.0239234   -0.0403615   -0.0186756 
  0.099916     0.192444    -0.0868753   -0.0847525     0.126167    -0.0273675    0.0795677   -0.0413885   -0.00806916   0.0579988    0.0340315    -0.0023632    0.0980399    -0.153929    -0.194141   -0.139765    -0.130682     0.135572     0.00380575   0.15613      0.115347     0.014626     0.0582455    0.0654242    0.0143367    0.136676  
  0.100418     0.0087446   -0.155674     0.0664594    -0.0187629   -0.00011118   0.0372944   -0.202752     0.116686    -0.117876     0.0140351    -0.0809387    0.0597165    -0.149198     0.11749    -0.129378    -0.0643587   -0.063109     0.0365426   -0.00127775   0.0794904    0.134578    -0.152415    -0.0213521    0.108365     0.0372755 
  0.0679875    0.145995     0.0534171    0.00911499    0.0962684   -0.109895    -0.0554386    0.0843731   -0.00307629   0.0619708    0.144478      0.0433041   -0.085796      0.124156     0.0124412  -0.116254     0.135747     0.0976739   -0.119851     0.220074    -0.0117631   -0.0317896   -0.194772     0.0203473   -0.176109    -0.129918  
  0.242827    -0.0655977   -0.193313     0.0408512     0.0738864   -0.00936154   0.0822375    0.179616    -0.0736241   -0.0375105   -0.0247313     0.0117582    0.0187774    -0.103842     0.128105   -0.0509751   -0.0679616   -0.259737     0.247449     0.185769     0.19254     -0.0155489   -0.00927386   0.0826546   -0.0892143   -0.134925  
 -0.0955144   -0.234153     0.0907507   -0.301102      0.0222655    0.0884255    0.0895742   -0.00605817   0.243841     0.090311     0.0638714     0.0834766   -0.0229682    -0.031539    -0.0465638  -0.0546356   -0.173363    -0.0254795   -0.160736     0.120576    -0.021484    -0.0294498    0.0467716    0.0192881   -0.0265807    0.0223567 
 -0.0264299   -0.128294    -0.201486    -0.124691     -0.071998    -0.0171522    0.0575338   -0.177966     0.0221706    0.118588    -0.0290245    -0.0261832   -0.0704934     0.137671    -0.0894213   0.0106759    0.0301858    0.0203576   -0.09527      0.103202    -0.00351119   0.0129755    0.0938199   -0.0628466    0.150794    -0.0661523 
  0.0837653   -0.00683584   0.236665     0.00748199    0.0354765    0.0835528    0.0423351   -0.151858     0.00255285  -0.0175707    0.0866621     0.03035      0.178898     -0.0966174    0.0400298  -0.0796592    0.019932     0.209064    -0.0222805    0.0860222    0.105565     0.0301858   -0.0661139    0.13605      0.0401694   -0.0394829 
  0.162922    -0.0010023   -0.037662    -0.0910314    -0.0240015   -0.156261     0.0960466    0.243248     0.00547415   0.0201065    0.0594338    -0.130992     0.104661      0.0456186   -0.142844    0.00338098   0.0440145   -0.0818451   -0.0181128    0.119621    -0.178605     0.0811492    0.150505     0.115377     0.06984     -0.106646  
  0.146456    -0.107037     0.141487     0.000679392   0.193187    -0.0718992    0.0130163    0.0847552    0.0685869   -0.102053     0.0668039    -0.122086    -0.0389274    -0.0024026   -0.183998    0.00185522  -0.0158295   -0.0751747    0.0879075    0.131118     0.113456     0.00125782  -0.190454    -0.0850188    0.0770591    0.0827684 
  0.101917     0.0597322   -0.20524     -0.00650136   -0.100665    -0.157285     0.0307854   -0.122472     0.00864314  -0.0474037   -0.0665147     0.0977601   -0.107071      0.172745     0.107281    0.0231465    0.0667219    0.196103     0.125418     0.00143604   0.113739     0.0742398   -0.00448003   0.0187135   -0.135485    -0.0696866 
 -0.0499259    0.0997497   -0.040864     0.0128762     0.0906912   -0.0202556   -0.0208409    0.0770991   -0.106098    -0.0117231   -0.0804294    -0.0433051   -0.00965736   -0.00946759   0.044008    0.0669656    0.0114332    0.0990318    0.128601    -0.144791     0.0159275   -0.0512263   -0.0594804    0.0519659    0.0277682   -0.136622  
 -0.0808396   -0.017864    -0.0526892   -0.0613854     0.0498533    0.0194058    0.00700274   0.235834     0.107711    -0.200315    -0.0310642     0.00513353  -0.115411      0.0454186    0.102236   -0.0074463   -0.0501503    0.00310689  -0.0140375   -0.0397284   -0.0930522   -0.068218     0.0609472    0.12952     -0.0941705    0.155025  
  0.0356971   -0.0662592    0.044282    -0.0270704    -0.0979272    0.129079     0.00762225  -0.137411     0.0248741    0.00821991  -0.215543     -0.100015    -0.0257432    -0.137909     0.0496249   0.107256     0.173042     0.0922128    0.0390916   -0.0239895    0.164615    -0.0818766    0.148685     0.0760131   -0.0488568   -0.120265  
 -0.0562018   -0.0262775    0.140416     0.134217      0.0530665    0.0923456    0.0151168   -0.0224252    0.0163356    0.16437      0.114599      0.0114989    0.0128335     0.0587477   -0.159242   -0.00636347  -0.114888    -0.0988249    0.106822    -0.0518297    0.0592939   -0.121143    -0.0839652   -0.176352    -0.269871     0.0882975 
 -0.0930346   -0.0666634   -0.058211    -0.0119996    -0.107902     0.0148273   -0.21507      0.00962547  -0.163486    -0.058401    -0.0956678    -0.0230138   -0.0371997     0.158224    -0.122115    0.116503    -0.0648922   -0.0638103   -0.266255    -0.0566135   -0.0872444    0.259392    -0.186409    -0.0152339   -0.00100696  -0.0124093 
 -0.0876047   -0.0409934   -0.169002    -0.0229837     0.0951768    0.0244054   -0.0678496    0.00215001   0.0746942    0.0338847    0.000496746   0.069205    -0.0113653    -0.0451063    0.128907   -0.111419     0.120285     0.0320913    0.132336    -0.236457     0.123802    -0.0393579   -0.0136195   -0.106719     0.263089     0.0297131 
  0.00183002   0.158965    -0.00319227   0.0282549    -0.0487922    0.107419    -0.0913741    0.0503685    0.00522158   0.00591003   0.096693      0.0641843    0.000353991   0.0151297    0.0857568   0.0611403   -0.167134     0.0312118    0.0461671    0.0164238    0.19874      0.0678518    0.0962948   -0.0310543   -0.0481042   -0.0708989 
  0.0766103    0.109115    -0.0341083    0.0520233    -0.0896715    0.0610523   -0.152104    -0.124271     0.126113     0.126        0.0233974     0.0498603   -0.0807364    -0.057851    -0.0139269   0.13076      0.0788188   -0.239891    -0.0137706   -0.0591346   -0.0806416   -0.0333381   -0.136161     0.0274749   -0.0741079    0.00919945
  0.120478     0.149471     0.0362894   -0.0796469    -0.126926     0.0621026   -0.0362275    0.10219      0.032788     0.140416    -0.161331      3.99935e-5   0.0353229     0.05282     -0.103156    0.131118    -0.0676907    0.0233639   -0.0415673    0.0542111    0.0425684    0.0101466    0.170549    -0.0320933    0.0653796   -0.166127  
 -0.117531    -0.0027291    0.0779525    0.115171     -0.0663013    0.100944    -0.10325      0.0110204   -0.0697267    0.0137529   -0.0334934     0.121176    -0.0121764     0.0418027   -0.126607   -0.0302938    0.0551419   -0.0056529   -0.103182    -0.125908    -0.191159     0.0681754    0.0471684    0.0147571    0.0513219    0.128733  
  0.133858    -0.0129189    0.107279     0.0484586    -0.0449086   -0.110109    -0.203434     0.0810419   -0.0146096    0.0304098   -0.108048     -0.0841663   -0.0449172     0.158526    -0.054551   -0.237655     0.0933758    0.0278207   -0.0237578    0.157699     0.0293697   -0.108353    -0.116485     0.091838     0.066932     0.127407  
 -0.155093     0.0148596   -0.0762878   -0.117173      0.0945261    0.0333388    0.170293     0.0264181   -0.150729    -0.0470993    0.103186      0.110917     0.0746201     0.16334     -0.129213   -0.0404192    0.109652    -0.0727405    0.146406    -0.0642087   -0.0922647   -0.0719618   -0.0102185    0.201759    -0.112651     0.0272841 
 -0.110551     0.167219    -0.0408842   -0.0878025    -0.0400573   -0.0762354   -0.0128202    0.0212937   -0.128439    -0.0125812   -0.112617     -0.0182166    0.0613693    -0.02692      0.267      -0.0420107    0.0107603   -0.0674438   -0.0493928    0.110842     0.0693951   -0.00965112   0.0448135   -0.00077227   0.0500803   -0.0494926 
 -0.218987    -0.0313508   -0.013602     0.314743      0.11189      0.205347     0.0176986   -0.0463378    0.0460432    0.126169     0.00550211    0.0683502    0.16255       0.0163575    0.115862   -0.148045    -0.230011    -0.170527     0.0473361   -0.108391     0.178132     0.0955082   -0.0462501   -0.159154     0.00508133   0.0841505 
  0.0198616    0.0300303    0.00101149  -0.0336668    -0.0563821   -0.0899013    0.0726927    0.0413668    0.0347516    0.0237678    0.176659     -0.0447916   -0.0839414    -0.109781    -0.0884866   0.0949322    0.0480725    0.0104314   -0.126024     0.0566811    0.0236233   -0.0275934    0.123116     0.235305    -0.00408422   0.00135197
 -0.0262711    0.0341453   -0.164926     0.0582256     0.0237063   -0.0338694    0.00346878   0.0446157    0.0211939    0.0159922    0.240528     -0.16689      0.121574     -0.00808985   0.121765    0.00363647   0.0946843   -0.0959134   -0.0424399    0.00700644   0.0350449   -0.0523579   -0.0643446    0.106723    -0.0146202   -0.0660931 
  0.244003     0.107278     0.0303762    0.0650433    -0.00175169   0.127515     0.00246889   0.0282045    0.166932     0.0266967   -0.112954      0.246673    -0.0775601     0.0562277   -0.028513    0.0911258   -0.0978698   -0.0416917    0.136508    -0.217658    -0.0826166   -0.0682484    0.185687     0.0484095   -0.00944128   0.0459102 kind diag, method split
┌ Info: 0: avll = 
└   tll[1] = -1.3881295283631823
[ Info: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.388189
[ Info: iteration 2, average log likelihood -1.388127
[ Info: iteration 3, average log likelihood -1.387787
[ Info: iteration 4, average log likelihood -1.384074
[ Info: iteration 5, average log likelihood -1.372222
[ Info: iteration 6, average log likelihood -1.364795
[ Info: iteration 7, average log likelihood -1.362799
[ Info: iteration 8, average log likelihood -1.361656
[ Info: iteration 9, average log likelihood -1.360732
[ Info: iteration 10, average log likelihood -1.359595
[ Info: iteration 11, average log likelihood -1.357432
[ Info: iteration 12, average log likelihood -1.354696
[ Info: iteration 13, average log likelihood -1.353048
[ Info: iteration 14, average log likelihood -1.352110
[ Info: iteration 15, average log likelihood -1.351545
[ Info: iteration 16, average log likelihood -1.351210
[ Info: iteration 17, average log likelihood -1.351004
[ Info: iteration 18, average log likelihood -1.350861
[ Info: iteration 19, average log likelihood -1.350751
[ Info: iteration 20, average log likelihood -1.350661
[ Info: iteration 21, average log likelihood -1.350587
[ Info: iteration 22, average log likelihood -1.350525
[ Info: iteration 23, average log likelihood -1.350471
[ Info: iteration 24, average log likelihood -1.350421
[ Info: iteration 25, average log likelihood -1.350377
[ Info: iteration 26, average log likelihood -1.350339
[ Info: iteration 27, average log likelihood -1.350309
[ Info: iteration 28, average log likelihood -1.350286
[ Info: iteration 29, average log likelihood -1.350272
[ Info: iteration 30, average log likelihood -1.350264
[ Info: iteration 31, average log likelihood -1.350259
[ Info: iteration 32, average log likelihood -1.350256
[ Info: iteration 33, average log likelihood -1.350254
[ Info: iteration 34, average log likelihood -1.350253
[ Info: iteration 35, average log likelihood -1.350252
[ Info: iteration 36, average log likelihood -1.350252
[ Info: iteration 37, average log likelihood -1.350251
[ Info: iteration 38, average log likelihood -1.350251
[ Info: iteration 39, average log likelihood -1.350251
[ Info: iteration 40, average log likelihood -1.350251
[ Info: iteration 41, average log likelihood -1.350251
[ Info: iteration 42, average log likelihood -1.350251
[ Info: iteration 43, average log likelihood -1.350251
[ Info: iteration 44, average log likelihood -1.350251
[ Info: iteration 45, average log likelihood -1.350251
[ Info: iteration 46, average log likelihood -1.350251
[ Info: iteration 47, average log likelihood -1.350251
[ Info: iteration 48, average log likelihood -1.350251
[ Info: iteration 49, average log likelihood -1.350251
[ Info: iteration 50, average log likelihood -1.350251
┌ Info: EM with 100000 data points 50 iterations avll -1.350251
└ 952.4 data points per parameter
┌ Info: 1
│   : avll =  = &quot;: avll = &quot;
│   avll =
│    50-element Array{Float64,1}:
│     -1.388189351381744 
│     -1.3881271429682251
│      ⋮                 
└     -1.350250909507219 
[ Info: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.350364
[ Info: iteration 2, average log likelihood -1.350262
[ Info: iteration 3, average log likelihood -1.349890
[ Info: iteration 4, average log likelihood -1.346814
[ Info: iteration 5, average log likelihood -1.336471
[ Info: iteration 6, average log likelihood -1.326279
[ Info: iteration 7, average log likelihood -1.321065
[ Info: iteration 8, average log likelihood -1.318012
[ Info: iteration 9, average log likelihood -1.315630
[ Info: iteration 10, average log likelihood -1.313655
[ Info: iteration 11, average log likelihood -1.312074
[ Info: iteration 12, average log likelihood -1.310758
[ Info: iteration 13, average log likelihood -1.309636
[ Info: iteration 14, average log likelihood -1.308731
[ Info: iteration 15, average log likelihood -1.308030
[ Info: iteration 16, average log likelihood -1.307493
[ Info: iteration 17, average log likelihood -1.307111
[ Info: iteration 18, average log likelihood -1.306857
[ Info: iteration 19, average log likelihood -1.306691
[ Info: iteration 20, average log likelihood -1.306579
[ Info: iteration 21, average log likelihood -1.306501
[ Info: iteration 22, average log likelihood -1.306445
[ Info: iteration 23, average log likelihood -1.306403
[ Info: iteration 24, average log likelihood -1.306372
[ Info: iteration 25, average log likelihood -1.306347
[ Info: iteration 26, average log likelihood -1.306326
[ Info: iteration 27, average log likelihood -1.306309
[ Info: iteration 28, average log likelihood -1.306295
[ Info: iteration 29, average log likelihood -1.306282
[ Info: iteration 30, average log likelihood -1.306271
[ Info: iteration 31, average log likelihood -1.306261
[ Info: iteration 32, average log likelihood -1.306252
[ Info: iteration 33, average log likelihood -1.306244
[ Info: iteration 34, average log likelihood -1.306236
[ Info: iteration 35, average log likelihood -1.306229
[ Info: iteration 36, average log likelihood -1.306222
[ Info: iteration 37, average log likelihood -1.306216
[ Info: iteration 38, average log likelihood -1.306210
[ Info: iteration 39, average log likelihood -1.306204
[ Info: iteration 40, average log likelihood -1.306199
[ Info: iteration 41, average log likelihood -1.306194
[ Info: iteration 42, average log likelihood -1.306189
[ Info: iteration 43, average log likelihood -1.306185
[ Info: iteration 44, average log likelihood -1.306180
[ Info: iteration 45, average log likelihood -1.306176
[ Info: iteration 46, average log likelihood -1.306172
[ Info: iteration 47, average log likelihood -1.306169
[ Info: iteration 48, average log likelihood -1.306165
[ Info: iteration 49, average log likelihood -1.306161
[ Info: iteration 50, average log likelihood -1.306157
┌ Info: EM with 100000 data points 50 iterations avll -1.306157
└ 473.9 data points per parameter
┌ Info: 2
│   : avll =  = &quot;: avll = &quot;
│   avll =
│    50-element Array{Float64,1}:
│     -1.3503636460811985
│     -1.3502617863964785
│      ⋮                 
└     -1.3061569788895189
[ Info: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.306308
[ Info: iteration 2, average log likelihood -1.306158
[ Info: iteration 3, average log likelihood -1.305719
[ Info: iteration 4, average log likelihood -1.301731
[ Info: iteration 5, average log likelihood -1.286642
[ Info: iteration 6, average log likelihood -1.270591
[ Info: iteration 7, average log likelihood -1.264604
[ Info: iteration 8, average log likelihood -1.262162
[ Info: iteration 9, average log likelihood -1.260356
[ Info: iteration 10, average log likelihood -1.258701
[ Info: iteration 11, average log likelihood -1.257069
[ Info: iteration 12, average log likelihood -1.255642
[ Info: iteration 13, average log likelihood -1.254725
[ Info: iteration 14, average log likelihood -1.254162
[ Info: iteration 15, average log likelihood -1.253647
[ Info: iteration 16, average log likelihood -1.252955
[ Info: iteration 17, average log likelihood -1.251892
[ Info: iteration 18, average log likelihood -1.250669
[ Info: iteration 19, average log likelihood -1.249766
[ Info: iteration 20, average log likelihood -1.248971
[ Info: iteration 21, average log likelihood -1.248053
[ Info: iteration 22, average log likelihood -1.246987
[ Info: iteration 23, average log likelihood -1.245926
[ Info: iteration 24, average log likelihood -1.245305
[ Info: iteration 25, average log likelihood -1.245047
[ Info: iteration 26, average log likelihood -1.244894
[ Info: iteration 27, average log likelihood -1.244778
[ Info: iteration 28, average log likelihood -1.244686
[ Info: iteration 29, average log likelihood -1.244608
[ Info: iteration 30, average log likelihood -1.244538
[ Info: iteration 31, average log likelihood -1.244473
[ Info: iteration 32, average log likelihood -1.244409
[ Info: iteration 33, average log likelihood -1.244345
[ Info: iteration 34, average log likelihood -1.244278
[ Info: iteration 35, average log likelihood -1.244208
[ Info: iteration 36, average log likelihood -1.244134
[ Info: iteration 37, average log likelihood -1.244054
[ Info: iteration 38, average log likelihood -1.243971
[ Info: iteration 39, average log likelihood -1.243888
[ Info: iteration 40, average log likelihood -1.243816
[ Info: iteration 41, average log likelihood -1.243761
[ Info: iteration 42, average log likelihood -1.243724
[ Info: iteration 43, average log likelihood -1.243699
[ Info: iteration 44, average log likelihood -1.243683
[ Info: iteration 45, average log likelihood -1.243673
[ Info: iteration 46, average log likelihood -1.243667
[ Info: iteration 47, average log likelihood -1.243664
[ Info: iteration 48, average log likelihood -1.243661
[ Info: iteration 49, average log likelihood -1.243660
[ Info: iteration 50, average log likelihood -1.243659
┌ Info: EM with 100000 data points 50 iterations avll -1.243659
└ 236.4 data points per parameter
┌ Info: 3
│   : avll =  = &quot;: avll = &quot;
│   avll =
│    50-element Array{Float64,1}:
│     -1.3063075395238937
│     -1.3061581872427823
│      ⋮                 
└     -1.2436587557593826
[ Info: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.243847
[ Info: iteration 2, average log likelihood -1.243642
[ Info: iteration 3, average log likelihood -1.242888
[ Info: iteration 4, average log likelihood -1.232686
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     11
│     12
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 5, average log likelihood -1.194835
[ Info: iteration 6, average log likelihood -1.182557
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      3
│      4
│     11
│     12
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 7, average log likelihood -1.157240
[ Info: iteration 8, average log likelihood -1.181206
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     11
│     12
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 9, average log likelihood -1.160179
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     3
│     4
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 10, average log likelihood -1.160584
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     11
│     12
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 11, average log likelihood -1.162050
[ Info: iteration 12, average log likelihood -1.166660
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      3
│      4
│     11
│     12
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 13, average log likelihood -1.144329
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     15
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 14, average log likelihood -1.170810
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     11
│     12
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 15, average log likelihood -1.157816
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     3
│     4
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 16, average log likelihood -1.155377
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     11
│     12
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 17, average log likelihood -1.156536
[ Info: iteration 18, average log likelihood -1.161396
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      3
│      4
│     11
│     12
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 19, average log likelihood -1.139755
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     15
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 20, average log likelihood -1.167145
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     11
│     12
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 21, average log likelihood -1.154961
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     3
│     4
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 22, average log likelihood -1.153914
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     11
│     12
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 23, average log likelihood -1.155971
[ Info: iteration 24, average log likelihood -1.161139
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      3
│      4
│     11
│     12
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 25, average log likelihood -1.139592
[ Info: iteration 26, average log likelihood -1.166908
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     11
│     12
│     15
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 27, average log likelihood -1.148860
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     3
│     4
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 28, average log likelihood -1.157465
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     11
│     12
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 29, average log likelihood -1.157145
[ Info: iteration 30, average log likelihood -1.161958
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      3
│      4
│     11
│     12
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 31, average log likelihood -1.139685
[ Info: iteration 32, average log likelihood -1.166706
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     11
│     12
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 33, average log likelihood -1.148601
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      3
│      4
│     15
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 34, average log likelihood -1.151238
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     11
│     12
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 35, average log likelihood -1.159997
[ Info: iteration 36, average log likelihood -1.161923
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      3
│      4
│     11
│     12
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 37, average log likelihood -1.138657
[ Info: iteration 38, average log likelihood -1.164935
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     11
│     12
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 39, average log likelihood -1.146575
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     3
│     4
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 40, average log likelihood -1.149282
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     11
│     12
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 41, average log likelihood -1.152541
[ Info: iteration 42, average log likelihood -1.158604
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      3
│      4
│     11
│     12
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 43, average log likelihood -1.137174
[ Info: iteration 44, average log likelihood -1.164620
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     11
│     12
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 45, average log likelihood -1.146528
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     3
│     4
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 46, average log likelihood -1.149254
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     11
│     12
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 47, average log likelihood -1.152538
[ Info: iteration 48, average log likelihood -1.158599
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      3
│      4
│     11
│     12
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 49, average log likelihood -1.137166
[ Info: iteration 50, average log likelihood -1.164620
┌ Info: EM with 100000 data points 50 iterations avll -1.164620
└ 118.1 data points per parameter
┌ Info: 4
│   : avll =  = &quot;: avll = &quot;
│   avll =
│    50-element Array{Float64,1}:
│     -1.24384739480293  
│     -1.2436420546311475
│      ⋮                 
└     -1.164619739819566 
[ Info: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│     21
│     22
│     23
│     24
│     29
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 1, average log likelihood -1.146776
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│      5
│      6
│      7
│      8
│      ⋮
│     23
│     24
│     29
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 2, average log likelihood -1.137013
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│     21
│     22
│     23
│     24
│     29
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 3, average log likelihood -1.144636
┌ Warning: Variances had to be floored 
│   ind =
│    10-element Array{Int64,1}:
│      5
│      6
│      7
│      8
│      ⋮
│     23
│     24
│     29
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 4, average log likelihood -1.115111
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│      7
│      8
│     11
│     21
│      ⋮
│     24
│     29
│     30
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 5, average log likelihood -1.085025
┌ Warning: Variances had to be floored 
│   ind =
│    14-element Array{Int64,1}:
│      5
│      6
│      7
│      8
│      ⋮
│     29
│     30
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 6, average log likelihood -1.053361
┌ Warning: Variances had to be floored 
│   ind =
│    11-element Array{Int64,1}:
│      1
│      2
│      7
│      8
│      ⋮
│     24
│     29
│     30
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 7, average log likelihood -1.074235
┌ Warning: Variances had to be floored 
│   ind =
│    11-element Array{Int64,1}:
│      5
│      6
│      7
│      8
│      ⋮
│     24
│     29
│     30
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 8, average log likelihood -1.070643
┌ Warning: Variances had to be floored 
│   ind =
│    11-element Array{Int64,1}:
│      7
│      8
│     11
│     18
│      ⋮
│     29
│     30
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 9, average log likelihood -1.061959
┌ Warning: Variances had to be floored 
│   ind =
│    12-element Array{Int64,1}:
│      5
│      6
│      7
│      8
│      ⋮
│     24
│     29
│     30
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 10, average log likelihood -1.063139
┌ Warning: Variances had to be floored 
│   ind =
│    11-element Array{Int64,1}:
│      1
│      2
│      7
│      8
│      ⋮
│     24
│     29
│     30
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 11, average log likelihood -1.057617
┌ Warning: Variances had to be floored 
│   ind =
│    13-element Array{Int64,1}:
│      5
│      6
│      7
│      8
│      ⋮
│     29
│     30
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 12, average log likelihood -1.063035
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│      7
│      8
│     11
│     21
│      ⋮
│     24
│     29
│     30
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 13, average log likelihood -1.079426
┌ Warning: Variances had to be floored 
│   ind =
│    12-element Array{Int64,1}:
│      5
│      6
│      7
│      8
│      ⋮
│     24
│     29
│     30
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 14, average log likelihood -1.048725
┌ Warning: Variances had to be floored 
│   ind =
│    13-element Array{Int64,1}:
│      1
│      2
│      7
│      8
│      ⋮
│     29
│     30
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 15, average log likelihood -1.051361
┌ Warning: Variances had to be floored 
│   ind =
│    11-element Array{Int64,1}:
│      5
│      6
│      7
│      8
│      ⋮
│     24
│     29
│     30
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 16, average log likelihood -1.081798
┌ Warning: Variances had to be floored 
│   ind =
│    10-element Array{Int64,1}:
│      7
│      8
│     11
│     18
│      ⋮
│     24
│     29
│     30
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 17, average log likelihood -1.065554
┌ Warning: Variances had to be floored 
│   ind =
│    13-element Array{Int64,1}:
│      5
│      6
│      7
│      8
│      ⋮
│     29
│     30
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 18, average log likelihood -1.055679
┌ Warning: Variances had to be floored 
│   ind =
│    11-element Array{Int64,1}:
│      1
│      2
│      7
│      8
│      ⋮
│     24
│     29
│     30
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 19, average log likelihood -1.062504
┌ Warning: Variances had to be floored 
│   ind =
│    11-element Array{Int64,1}:
│      5
│      6
│      8
│     11
│      ⋮
│     24
│     29
│     30
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 20, average log likelihood -1.063594
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│      7
│     11
│     21
│     22
│      ⋮
│     29
│     30
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 21, average log likelihood -1.067174
┌ Warning: Variances had to be floored 
│   ind =
│    12-element Array{Int64,1}:
│      5
│      6
│      7
│     10
│      ⋮
│     24
│     29
│     30
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 22, average log likelihood -1.054278
┌ Warning: Variances had to be floored 
│   ind =
│    10-element Array{Int64,1}:
│      1
│      2
│      8
│     11
│      ⋮
│     24
│     29
│     30
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 23, average log likelihood -1.062710
┌ Warning: Variances had to be floored 
│   ind =
│    11-element Array{Int64,1}:
│      5
│      6
│      7
│     11
│      ⋮
│     29
│     30
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 24, average log likelihood -1.064305
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│      8
│     11
│     18
│     21
│      ⋮
│     24
│     29
│     30
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 25, average log likelihood -1.065827
┌ Warning: Variances had to be floored 
│   ind =
│    11-element Array{Int64,1}:
│      5
│      6
│      7
│     10
│      ⋮
│     24
│     29
│     30
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 26, average log likelihood -1.052460
┌ Warning: Variances had to be floored 
│   ind =
│    11-element Array{Int64,1}:
│      1
│      2
│      7
│     11
│      ⋮
│     29
│     30
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 27, average log likelihood -1.055485
┌ Warning: Variances had to be floored 
│   ind =
│    11-element Array{Int64,1}:
│      5
│      6
│      8
│     11
│      ⋮
│     24
│     29
│     30
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 28, average log likelihood -1.067382
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      7
│     11
│     21
│     22
│     23
│     24
│     29
│     30
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 29, average log likelihood -1.070349
┌ Warning: Variances had to be floored 
│   ind =
│    14-element Array{Int64,1}:
│      5
│      6
│      7
│      8
│      ⋮
│     29
│     30
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 30, average log likelihood -1.047115
┌ Warning: Variances had to be floored 
│   ind =
│    11-element Array{Int64,1}:
│      1
│      2
│      7
│      8
│      ⋮
│     24
│     29
│     30
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 31, average log likelihood -1.070849
┌ Warning: Variances had to be floored 
│   ind =
│    10-element Array{Int64,1}:
│      5
│      6
│      7
│     11
│      ⋮
│     24
│     29
│     30
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 32, average log likelihood -1.067944
┌ Warning: Variances had to be floored 
│   ind =
│    10-element Array{Int64,1}:
│      8
│     11
│     18
│     21
│      ⋮
│     29
│     30
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 33, average log likelihood -1.055272
┌ Warning: Variances had to be floored 
│   ind =
│    12-element Array{Int64,1}:
│      5
│      6
│      7
│      8
│      ⋮
│     24
│     29
│     30
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 34, average log likelihood -1.062436
┌ Warning: Variances had to be floored 
│   ind =
│    11-element Array{Int64,1}:
│      1
│      2
│      7
│      8
│      ⋮
│     24
│     29
│     30
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 35, average log likelihood -1.056634
┌ Warning: Variances had to be floored 
│   ind =
│    13-element Array{Int64,1}:
│      5
│      6
│      7
│      8
│      ⋮
│     29
│     30
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 36, average log likelihood -1.062253
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│      7
│      8
│     11
│     21
│      ⋮
│     24
│     29
│     30
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 37, average log likelihood -1.079435
┌ Warning: Variances had to be floored 
│   ind =
│    13-element Array{Int64,1}:
│      5
│      6
│      7
│      8
│      ⋮
│     24
│     29
│     30
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 38, average log likelihood -1.048599
┌ Warning: Variances had to be floored 
│   ind =
│    12-element Array{Int64,1}:
│      1
│      2
│      7
│      8
│      ⋮
│     29
│     30
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 39, average log likelihood -1.063932
┌ Warning: Variances had to be floored 
│   ind =
│    11-element Array{Int64,1}:
│      5
│      6
│      7
│      8
│      ⋮
│     24
│     29
│     30
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 40, average log likelihood -1.073699
┌ Warning: Variances had to be floored 
│   ind =
│    10-element Array{Int64,1}:
│      7
│      8
│     11
│     18
│      ⋮
│     24
│     29
│     30
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 41, average log likelihood -1.061181
┌ Warning: Variances had to be floored 
│   ind =
│    13-element Array{Int64,1}:
│      5
│      6
│      7
│      8
│      ⋮
│     29
│     30
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 42, average log likelihood -1.055669
┌ Warning: Variances had to be floored 
│   ind =
│    11-element Array{Int64,1}:
│      1
│      2
│      7
│      8
│      ⋮
│     24
│     29
│     30
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 43, average log likelihood -1.062421
┌ Warning: Variances had to be floored 
│   ind =
│    12-element Array{Int64,1}:
│      5
│      6
│      7
│      8
│      ⋮
│     24
│     29
│     30
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 44, average log likelihood -1.063546
┌ Warning: Variances had to be floored 
│   ind =
│    10-element Array{Int64,1}:
│      7
│      8
│     11
│     21
│      ⋮
│     29
│     30
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 45, average log likelihood -1.072494
┌ Warning: Variances had to be floored 
│   ind =
│    13-element Array{Int64,1}:
│      5
│      6
│      7
│      8
│      ⋮
│     24
│     29
│     30
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 46, average log likelihood -1.054345
┌ Warning: Variances had to be floored 
│   ind =
│    11-element Array{Int64,1}:
│      1
│      2
│      7
│      8
│      ⋮
│     24
│     29
│     30
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 47, average log likelihood -1.065123
┌ Warning: Variances had to be floored 
│   ind =
│    12-element Array{Int64,1}:
│      5
│      6
│      7
│      8
│      ⋮
│     29
│     30
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 48, average log likelihood -1.066807
┌ Warning: Variances had to be floored 
│   ind =
│    10-element Array{Int64,1}:
│      7
│      8
│     11
│     18
│      ⋮
│     24
│     29
│     30
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 49, average log likelihood -1.066847
┌ Warning: Variances had to be floored 
│   ind =
│    12-element Array{Int64,1}:
│      5
│      6
│      7
│      8
│      ⋮
│     24
│     29
│     30
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 50, average log likelihood -1.056893
┌ Info: EM with 100000 data points 50 iterations avll -1.056893
└ 59.0 data points per parameter
┌ Info: 5
│   : avll =  = &quot;: avll = &quot;
│   avll =
│    50-element Array{Float64,1}:
│     -1.1467759446024544
│     -1.1370128564996014
│      ⋮                 
└     -1.056892737787392 
32×26 Array{Float64,2}:
┌ Info: Total log likelihood: 
│   tll =
│    251-element Array{Float64,1}:
│     -1.3881295283631823
│     -1.388189351381744 
│     -1.3881271429682251
│     -1.3877869701913343
│      ⋮                 
│     -1.0668070740481594
│     -1.066847170066357 
└     -1.056892737787392 
  0.164271    -0.00217273  -0.0470836    -0.0875273   -0.0198937   -0.151989     0.0890534    0.205701     0.00176484    0.0236011     0.0606491   -0.127702      0.103047      0.0519955   -0.145655     0.00267426   0.046684   -0.0974009  -0.0284963    0.11993     -0.169751     0.109918     0.151456    0.117805      0.0466175   -0.106229  
  0.0293548    0.0281487    0.192475     -0.0718471    0.16037     -0.0227561    0.0269263    0.0703363    0.0346584     0.0061993     0.046528    -0.0713214    -0.245939     -0.0886919    0.0511628   -0.246649     0.117217   -0.0338022   0.0768632    0.0196098   -0.035502     0.0220404    0.0378458  -0.10039       0.112127    -0.0839882 
 -0.221682    -0.0441564   -0.245942      0.0818001    0.0643442    0.0235111   -0.130789    -0.0979242    0.0900224     0.0223796    -0.0308715    0.0640131    -0.0421585    -0.0327608    0.114441    -0.117101     0.149427   -0.0401973   0.0683691   -0.221629     0.18205     -0.673096    -0.0585357  -0.038132      0.305341     0.0571348 
  0.0198496   -0.0404453   -0.142881     -0.130136     0.0893661    0.023679     0.0199041    0.134793     0.0678725     0.0510673     0.016995     0.073841      0.034278     -0.0552616    0.115659    -0.110724     0.0828793   0.146341    0.193251    -0.248892     0.0554651    0.59742      0.0472035  -0.11587       0.197544    -0.014937  
 -0.00923364   0.168183    -0.00816671   -0.089778    -0.0499092    0.107732    -0.132549     0.0441416   -0.00768133    0.00588448    0.123217     0.0569073    -0.0547229     0.0118391    0.154552     0.11975     -0.224988    0.0261771   0.0735157    0.00689506   0.210618     0.0241936    0.123005   -0.0885666    -0.0916086   -0.077677  
  0.00350999   0.165609    -0.0215082     0.203757    -0.0543409    0.106433    -0.099053     0.0541658   -0.0144909     0.005863      0.00972408   0.0505668     0.13638       0.0156758   -0.0968709   -0.173914     0.0206315   0.0320984  -0.0884954    0.0142523    0.198367    -0.0169726   -0.117878    0.131286      0.0116762   -0.0610595 
  0.109743     0.0796517    0.15025      -0.572502    -0.774243    -0.120813     0.0931068   -0.122177    -0.00413736   -0.0881618    -0.0609498    0.111718     -0.0947901     0.179249     0.104544     0.0229222    0.065235    0.175944    0.129242     0.128834     0.900583     0.0584427    0.0866921   0.012048     -0.107088    -0.0688262 
  0.107351     0.0630209   -0.291069      0.167693     0.0889674   -0.164109     0.00686502  -0.122174     0.0123471    -0.0296877    -0.0631119    0.0913191    -0.10442       0.165982     0.103088     0.0216699    0.0672532   0.189104    0.125488    -0.041784    -0.0263471    0.0746184   -0.0341011   0.0151453    -0.148424    -0.0656847 
 -0.206386    -0.0581994   -0.0318823     0.342554     0.100155     0.21094      0.0191086   -0.0462548    0.0665698     0.127828      0.0258186    0.0518134     0.153167     -0.00565347   0.118086    -0.191255    -0.233354   -0.203167    0.0455344   -0.111803     0.176885     0.0818646   -0.0461895  -0.155102      0.034587     0.0768493 
 -0.0577248   -0.0202277    0.147106      0.147537     0.0375862    0.0668008    0.00466503  -0.0166021    0.00475135    0.158716      0.130112     0.0195745     0.010358      0.0339075   -0.148081    -0.00936164  -0.114822   -0.107143    0.0943238   -0.0382493    0.0575839   -0.117395    -0.0767883  -0.125987     -0.267883     0.0882441 
 -0.102952    -0.226315     0.109412     -0.355666     0.0165518    0.0886357    0.0866638   -0.0109731    0.274608      0.0621841     0.0557136    0.0883636    -0.0215974    -0.0371451   -0.0171205   -0.0468889   -0.186632   -0.004093   -0.164482     0.120385    -0.0458557   -0.0177801    0.0504654   0.0192928    -0.027856     0.0289776 
  0.122632    -0.123487    -0.0853136     0.185454     0.160246     0.0902024   -0.120752    -0.011397     0.121732     -0.0119539    -0.0364737   -0.00706086    0.048694      0.0383469   -0.110702    -0.0446706    0.0102438  -0.130522    0.0430403    0.038076     0.0854097    0.0967334   -0.0132768   0.0376225    -0.145279     0.0766265 
 -0.0265258    0.00796202   0.0341949    -0.0465545    0.0175532   -0.0490932    0.0024156    0.0478033   -0.0798785    -0.010198     -0.0143589    0.0278665     0.0231812     0.169351    -0.0972069   -0.147992     0.0921594  -0.0321742   0.0548935    0.0655097   -0.0380209   -0.119574    -0.0602079   0.149724     -0.0283713    0.0952983 
  0.231124     0.111905     0.00129607    0.0711062    0.0303848    0.162179     0.00173358   0.0202174    0.162686      0.000793785  -0.118723     0.247745     -0.0547203     0.043147    -0.0315029    0.0660787   -0.0958159  -0.0285237   0.14009     -0.21382     -0.0874701   -0.0766208    0.177635    0.0537255     0.0163714    0.0598181 
  0.00418019   0.110173    -0.0669658    -0.0680398    0.0737064   -0.00790518   0.0486407    0.0683401    0.0536978    -0.0404707     0.00122556   0.000669051  -0.00199737   -0.0593235   -0.060058    -0.0621205   -0.0973834   0.0589129  -0.00593364   0.0569808    0.0322111   -0.0217303    0.0513507   0.0959148    -0.0526476    0.130788  
  0.0481874   -0.0429006    0.153843     -0.0127689   -0.0274704    0.116433     0.0334708   -0.156327     0.0328194     0.0263095    -0.0457662   -0.0402755     0.0835068    -0.124065     0.0442745    0.00647213   0.0787184   0.15805     0.0097873    0.0324375    0.129392    -0.024406     0.0316213   0.103901      0.00182608  -0.0738665 
 -0.0922817   -0.0659432   -0.0579884    -0.0175533   -0.105668     0.0143332   -0.194822    -0.0621814   -0.163487     -0.0375275    -0.0945857   -0.055381     -0.0363243     0.153013    -0.173983     0.0901242   -0.0578259  -0.0311994  -0.27667     -0.0548746   -0.073145     0.256177    -0.188333   -0.0144636     0.00491873   0.0164052 
  0.0160103   -0.00334398   0.0010625    -0.046273    -0.0627744   -0.0550228    0.0909152    0.0256054    0.0336679     0.0777544     0.176346    -0.0295349    -0.0981574    -0.130302    -0.0764795    0.0864955    0.0466519   0.0309308  -0.113541     0.0709108    0.0817688   -0.0240619    0.115851    0.229004     -0.0113755    0.00274105
  0.0584112    0.121543     0.0355247    -0.139428    -0.113872    -0.0582343   -0.0302895    0.0951955   -0.0876547    -0.223133     -0.173795    -0.0948981     0.0408818     0.0515924   -0.0888445    0.200605    -0.089507    0.0388864  -0.0854886    0.053876    -0.00770896   0.0357552    0.17206    -0.0261122     0.0110997   -0.167734  
  0.178699     0.152886     0.0320951    -0.0411441   -0.127842     0.0931806   -0.0476245    0.0990942    0.0773621     0.481738     -0.146959     0.0771977     0.143827      0.0749817   -0.172137     0.0846659   -0.0509684   0.0286378  -0.0258873    0.0589381    0.0885982   -0.0614832    0.174574   -0.0358959     0.233245    -0.18309   
  0.0840187   -0.0224596    0.0741349    -0.180393    -0.0514537   -0.0389212   -0.0862624   -0.0758956   -0.0459351     0.146569      0.0192133    0.1225        0.0691571    -0.253328    -0.16271     -0.0831454   -0.027397    0.0189596   0.0466864   -0.573599    -0.10343     -0.160893    -0.0724704   0.0406073    -0.0753341    0.12531   
  0.0691148   -0.0223853    0.0846401    -0.0152791   -0.0478582   -0.0309899   -0.149623    -0.101368     0.310428      0.00360691    0.0195937   -0.026436      0.0702719     0.165142    -0.204555    -0.0145341   -0.0377331   0.021301    0.0514238    0.664555    -0.0747221   -0.0279382   -0.0729787  -0.106899      0.010242    -0.0957782 
 -0.201751     0.167692    -0.040786     -0.0867641    0.140354     0.0374212   -0.0919525    0.312926    -0.113543     -0.0270926    -0.116881    -0.0779307     0.13263      -0.0237706    0.221662     0.0368299   -0.0443757   0.122673   -0.264633     0.11265      0.00426137   0.00748346   0.217121   -0.00123733    0.0463802   -0.112483  
  0.0293496    0.167413    -0.0407505    -0.0861051   -0.198987    -0.184724    -0.0223216   -0.307238    -0.16275      -0.0235803    -0.112026     0.0269864    -0.00370426   -0.0160892    0.278833    -0.102783     0.0359955  -0.230029    0.161107     0.118242     0.0504361   -0.00540437  -0.130832   -0.000488076   0.0285944   -0.0987328 
 -0.0960118    0.011873    -0.0580082     0.0777509   -0.00837949   0.0416508   -0.0450691    0.0260409   -0.00900717    0.00614108    0.0899397   -0.0253129     0.0495423     0.00922213  -0.0217524   -0.0162616    0.0683127  -0.0414651  -0.0671355   -0.083302    -0.0759918    0.0154637    0.0011523   0.0548066     0.0203014    0.0188018 
  0.0125966    0.054201    -0.0369668     0.0268687   -0.0431316    0.0264727    0.0490461   -0.00810406   0.0175095     0.0890788     0.0371956    0.0425655    -0.0943702     0.166371    -0.00896113   0.0359746   -0.0604089   0.0190424   0.00208058   0.0565494    0.0402596    0.00678322   0.0489316  -0.0543075     0.0236551   -0.0014724 
  0.0754432    0.0828004    0.0671396     0.10347     -0.00869053  -0.0385845   -0.0675834   -0.00993244   0.13723      -0.0195254    -0.0121669   -0.0352027    -0.106828     -0.0105169    0.0647945    0.032391     0.147148   -0.124729   -0.0345211    0.074876    -0.0744774   -0.0290599   -0.0381518   0.0619707    -0.02382      0.0353626 
  0.130378    -0.0403578   -0.000642765   0.0302598    0.108228    -0.0296817    0.0472489   -0.0562251    0.0703893    -0.117488      0.0465623   -0.0988869     0.0113126    -0.07786     -0.0557504   -0.0431748   -0.0107067  -0.056012    0.0785066    0.0646444    0.0991916    0.0185356   -0.171066   -0.0493476     0.0729842    0.0634889 
  0.0619282    0.0928827    0.0723072    -0.0472023    0.0491047   -0.100433     0.202966     0.142247     0.0219988    -0.120465      0.0662647    0.0472678    -0.0458009    -0.166513     0.0142839   -0.115121     0.133595    0.110389   -0.0938126    0.372911    -0.0129736   -0.0379586   -0.206817    0.0802907    -0.0513617   -0.152776  
  0.0659339    0.18532      0.0353671     0.0719054    0.0786551   -0.125996    -0.206948     0.0313559    0.000204342   0.315251      0.206873     0.035152     -0.12386       0.501709    -0.0184973   -0.0837979    0.137378    0.0722611  -0.1268       0.0416871   -0.00881679  -0.0209487   -0.177787   -0.142763     -0.212059    -0.097522  
 -0.0544819    0.0772164   -0.0372968     0.00552734   0.0907848   -0.0218636   -0.015934     0.0734333   -0.10511       0.00258613   -0.0575878   -0.0481889     0.0280695    -0.0120652   -0.0415112    0.0741962    0.0117724   0.0329936   0.128361    -0.141412     0.0200978   -0.0526373   -0.0770855   0.143377      0.0263744   -0.139732  
  0.258164    -0.0572239   -0.187103      0.0446937    0.0785613   -0.0094807    0.0936297    0.117715    -0.0665307    -0.0437615    -0.0246958    0.0135845     0.000133954  -0.110441     0.125397    -0.0699104   -0.0848843  -0.241323    0.251051     0.174067     0.191331    -0.0230322   -0.0212125   0.0762769    -0.103457    -0.133454  [ Info: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
┌ Warning: Variances had to be floored 
│   ind =
│    12-element Array{Int64,1}:
│      1
│      2
│      7
│      8
│      ⋮
│     29
│     30
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 1, average log likelihood -1.055531
┌ Warning: Variances had to be floored 
│   ind =
│    15-element Array{Int64,1}:
│      1
│      2
│      5
│      6
│      ⋮
│     29
│     30
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 2, average log likelihood -1.041709
┌ Warning: Variances had to be floored 
│   ind =
│    13-element Array{Int64,1}:
│      1
│      2
│      7
│      8
│      ⋮
│     29
│     30
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 3, average log likelihood -1.047879
┌ Warning: Variances had to be floored 
│   ind =
│    16-element Array{Int64,1}:
│      1
│      2
│      5
│      6
│      ⋮
│     29
│     30
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 4, average log likelihood -1.042210
┌ Warning: Variances had to be floored 
│   ind =
│    12-element Array{Int64,1}:
│      1
│      2
│      7
│      8
│      ⋮
│     29
│     30
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 5, average log likelihood -1.055483
┌ Warning: Variances had to be floored 
│   ind =
│    15-element Array{Int64,1}:
│      1
│      2
│      5
│      6
│      ⋮
│     29
│     30
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 6, average log likelihood -1.041624
┌ Warning: Variances had to be floored 
│   ind =
│    13-element Array{Int64,1}:
│      1
│      2
│      7
│      8
│      ⋮
│     29
│     30
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 7, average log likelihood -1.047877
┌ Warning: Variances had to be floored 
│   ind =
│    16-element Array{Int64,1}:
│      1
│      2
│      5
│      6
│      ⋮
│     29
│     30
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 8, average log likelihood -1.042206
┌ Warning: Variances had to be floored 
│   ind =
│    12-element Array{Int64,1}:
│      1
│      2
│      7
│      8
│      ⋮
│     29
│     30
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 9, average log likelihood -1.055482
┌ Warning: Variances had to be floored 
│   ind =
│    15-element Array{Int64,1}:
│      1
│      2
│      5
│      6
│      ⋮
│     29
│     30
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 10, average log likelihood -1.041621
┌ Info: EM with 100000 data points 10 iterations avll -1.041621
└ 59.0 data points per parameter
kind diag, method kmeans
[ Info: Initializing GMM, 32 Gaussians LinearAlgebra.diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       8.152460e+05
      1       6.503105e+05      -1.649355e+05 |       32
      2       6.184806e+05      -3.182993e+04 |       32
      3       6.002600e+05      -1.822054e+04 |       32
      4       5.908114e+05      -9.448594e+03 |       32
      5       5.861540e+05      -4.657398e+03 |       32
      6       5.831275e+05      -3.026509e+03 |       32
      7       5.807420e+05      -2.385558e+03 |       32
      8       5.788943e+05      -1.847648e+03 |       32
      9       5.774273e+05      -1.467040e+03 |       32
     10       5.762834e+05      -1.143860e+03 |       32
     11       5.753674e+05      -9.160160e+02 |       32
     12       5.746774e+05      -6.899867e+02 |       32
     13       5.743109e+05      -3.664708e+02 |       32
     14       5.741161e+05      -1.948369e+02 |       32
     15       5.739549e+05      -1.611718e+02 |       32
     16       5.737842e+05      -1.707000e+02 |       32
     17       5.735933e+05      -1.908959e+02 |       32
     18       5.734025e+05      -1.908272e+02 |       32
     19       5.732200e+05      -1.824626e+02 |       32
     20       5.730723e+05      -1.477824e+02 |       32
     21       5.729667e+05      -1.055936e+02 |       32
     22       5.728953e+05      -7.136834e+01 |       30
     23       5.728454e+05      -4.991407e+01 |       32
     24       5.728083e+05      -3.713044e+01 |       31
     25       5.727757e+05      -3.253291e+01 |       30
     26       5.727496e+05      -2.615924e+01 |       30
     27       5.727225e+05      -2.706022e+01 |       31
     28       5.726983e+05      -2.419056e+01 |       29
     29       5.726738e+05      -2.451819e+01 |       28
     30       5.726505e+05      -2.333835e+01 |       30
     31       5.726285e+05      -2.196535e+01 |       30
     32       5.726113e+05      -1.721734e+01 |       31
     33       5.725926e+05      -1.870730e+01 |       29
     34       5.725695e+05      -2.306981e+01 |       31
     35       5.725423e+05      -2.724767e+01 |       31
     36       5.725140e+05      -2.822253e+01 |       31
     37       5.724788e+05      -3.524512e+01 |       31
     38       5.724304e+05      -4.841880e+01 |       31
     39       5.723695e+05      -6.082547e+01 |       32
     40       5.722946e+05      -7.492254e+01 |       32
     41       5.721703e+05      -1.243354e+02 |       32
     42       5.720089e+05      -1.614143e+02 |       32
     43       5.718482e+05      -1.606481e+02 |       32
     44       5.716971e+05      -1.511359e+02 |       32
     45       5.714919e+05      -2.052332e+02 |       32
     46       5.712134e+05      -2.784477e+02 |       32
     47       5.708567e+05      -3.566772e+02 |       32
     48       5.705955e+05      -2.612697e+02 |       32
     49       5.704717e+05      -1.237803e+02 |       32
     50       5.704321e+05      -3.962163e+01 |       30
K-means terminated without convergence after 50 iterations (objv = 570432.0561226343)
┌ Info: K-means with 32000 data points using 50 iterations
└ 37.0 data points per parameter
[ Info: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.298272
[ Info: iteration 2, average log likelihood -1.266783
[ Info: iteration 3, average log likelihood -1.235425
[ Info: iteration 4, average log likelihood -1.201367
[ Info: iteration 5, average log likelihood -1.168049
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     23
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 6, average log likelihood -1.131907
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     25
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 7, average log likelihood -1.099776
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│     12
│     20
│     27
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 8, average log likelihood -1.062166
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      2
│      4
│      6
│     24
│     26
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 9, average log likelihood -1.085198
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     8
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 10, average log likelihood -1.103944
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│     12
│     13
│     16
│     20
│     23
│     25
│     29
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 11, average log likelihood -1.050608
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      3
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 12, average log likelihood -1.116701
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      2
│     26
│     27
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 13, average log likelihood -1.078383
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      4
│      8
│     20
│     24
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 14, average log likelihood -1.067339
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      6
│     12
│     16
│     25
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 15, average log likelihood -1.070921
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     13
│     23
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 16, average log likelihood -1.068270
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      2
│      3
│     18
│     20
│     26
│     27
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 17, average log likelihood -1.050238
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      8
│     12
│     24
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 18, average log likelihood -1.093802
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     4
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 19, average log likelihood -1.085561
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│     13
│     16
│     23
│     25
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 20, average log likelihood -1.036989
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      2
│      6
│     12
│     18
│     20
│     26
│     27
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 21, average log likelihood -1.041725
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      8
│     24
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 22, average log likelihood -1.122404
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     3
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 23, average log likelihood -1.084112
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      4
│     12
│     13
│     16
│     18
│     20
│     23
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 24, average log likelihood -1.025833
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      2
│      6
│     27
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 25, average log likelihood -1.105262
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      8
│     25
│     26
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 26, average log likelihood -1.075614
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     12
│     20
│     24
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 27, average log likelihood -1.059803
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      3
│      4
│     13
│     16
│     18
│     23
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 28, average log likelihood -1.048330
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     2
│     5
│     6
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 29, average log likelihood -1.094039
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      8
│     12
│     20
│     26
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 30, average log likelihood -1.076733
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     18
│     24
│     27
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 31, average log likelihood -1.073114
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      2
│      3
│     13
│     16
│     23
│     25
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 32, average log likelihood -1.053175
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     12
│     20
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 33, average log likelihood -1.096106
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     4
│     8
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 34, average log likelihood -1.076853
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│     18
│     24
│     26
│     27
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 35, average log likelihood -1.045037
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      3
│     12
│     13
│     16
│     20
│     23
│     25
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 36, average log likelihood -1.058976
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     6
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 37, average log likelihood -1.111401
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     5
│     8
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 38, average log likelihood -1.064079
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      4
│     12
│     18
│     20
│     24
│     26
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 39, average log likelihood -1.032165
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      3
│     13
│     16
│     23
│     25
│     27
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 40, average log likelihood -1.085646
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     2
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 41, average log likelihood -1.106783
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      8
│     12
│     20
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 42, average log likelihood -1.065040
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     18
│     24
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 43, average log likelihood -1.059199
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      3
│      6
│     13
│     16
│     23
│     25
│     26
│     27
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 44, average log likelihood -1.040039
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     12
│     20
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 45, average log likelihood -1.101843
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     4
│     8
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 46, average log likelihood -1.081914
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      2
│      5
│     18
│     24
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 47, average log likelihood -1.042325
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      3
│     12
│     13
│     16
│     20
│     23
│     25
│     26
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 48, average log likelihood -1.055896
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     27
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 49, average log likelihood -1.111485
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     8
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 50, average log likelihood -1.069007
┌ Info: EM with 100000 data points 50 iterations avll -1.069007
└ 59.0 data points per parameter
32×26 Array{Float64,2}:
 -0.133574    -0.00146676   0.0788996    0.0899353   -0.068661     0.101935    -0.0956413    0.019656   -0.0407713    -0.00471348   -0.0413765    0.120009    -0.0126277    0.0343176    -0.151855    -0.0198137    0.0361694    0.00239827  -0.100359    -0.0969511   -0.183406     0.075871    0.0441839    0.0169698     0.0512643    0.111022   
  0.0337696    0.055236     0.072978    -0.153586     0.0852085   -0.0119191    0.0289661   -0.0273983   0.0521112     0.0212931     0.0185529   -0.0352423   -0.190249    -0.0189035     0.0426488   -0.115427     0.0517723    0.0169022    0.0516267    0.0333663    0.0183852    0.0136362   0.0143966   -0.058228      0.0512231   -0.0497171  
  0.161739     0.0440774   -0.152153     0.0539811    0.00722045  -0.0122275    0.135424    -0.171681    0.0527417    -0.114094     -0.0116437   -0.0597761    0.0951229   -0.157584      0.13451     -0.153377    -0.0540326   -0.102686     0.092456     0.0485736    0.105594     0.138325   -0.129176    -0.0065213     0.102955    -0.0124477  
  0.0679901    0.134914     0.056358     0.00750052   0.0533286   -0.107054    -0.00222407   0.0833685   0.0120998     0.099307      0.130683     0.041386    -0.0867215    0.155985     -0.00317158  -0.0935483    0.134142     0.0812385   -0.106355     0.195911    -0.0111305   -0.0295021  -0.191404    -0.0284317    -0.125506    -0.123678   
  0.210361    -0.162498    -0.13148      0.0132694    0.0691537   -0.0955295    0.177035     0.248505   -0.0310354    -0.0215709     0.00744614  -0.0439124    0.019233    -0.0622849     0.0231329    0.00774077  -0.00861769  -0.27185      0.178692     0.166248     0.0600316   -0.0129039   0.0658508    0.0797152    -0.0579038   -0.083791   
  0.104539     0.0654795   -0.199348     0.00918951  -0.0804274   -0.153074     0.0223695   -0.122852    0.00673718   -0.0427848    -0.0640801    0.0935144   -0.111012     0.170752      0.106879     0.0215953    0.0675528    0.185572     0.125743    -0.0112895    0.162591     0.070667   -0.00541283   0.0142886    -0.140041    -0.0669464  
  0.0898721   -0.0111317    0.227541     0.00292417   0.0330412    0.0783033    0.0520825   -0.149829    0.0339119     0.0581608     0.100271     0.0337321    0.182721    -0.0920315     0.0330009   -0.0994807   -0.0182931    0.172761    -0.0172821    0.0905094    0.108384     0.0291024  -0.0581679    0.131635      0.0422399   -0.0398141  
  0.00627954  -0.0734922    0.0629751   -0.0305402   -0.0796664    0.15754      0.0132632   -0.156482    0.0455861    -0.00585048   -0.215908    -0.130425    -0.0267225   -0.15405       0.0519272    0.13082      0.169661     0.127547     0.0380141   -0.0245446    0.162298    -0.081422    0.147206     0.0735808    -0.0489178   -0.121862   
 -0.0421971   -0.117305    -0.205236    -0.134273    -0.0881526    0.00695196   0.0615893   -0.153774    0.020925      0.128492     -0.0169814   -0.0197905   -0.0688937    0.180214     -0.108014     0.0266818    0.0318657    0.0212414   -0.0966545    0.114222    -0.00523263   0.0289943   0.0936971   -0.0639405     0.148481    -0.0656164  
 -0.0785042   -0.00252323  -0.0521444   -0.0573829    0.0291845   -5.57917e-5   0.0126797    0.2154      0.102866     -0.167238     -0.0185377    0.00315801  -0.116633     0.0368708     0.0808258   -0.0164379   -0.0397902    0.00680272  -0.0126443   -0.0379747   -0.0288356   -0.0626482   0.0534101    0.12878      -0.10901      0.151831   
  0.135215    -0.0804721    0.141984    -0.00678808   0.20656     -0.0531336    0.00252365   0.0824863   0.0305463    -0.0947996     0.0684289   -0.112867    -0.0298547   -0.0307719    -0.175869     0.0429284    0.0188152   -0.0214395    0.102973     0.135522     0.0975192   -0.0479467  -0.159991    -0.0409963     0.0410773    0.0605364  
 -0.0940119    0.163546    -0.0416147   -0.0842334   -0.018322    -0.0790726   -0.0575352    0.0425704  -0.143196     -0.0278107    -0.109464    -0.023201     0.062879    -0.0134863     0.254585    -0.0190802    0.0058409   -0.0612153   -0.062051     0.121402     0.0353251    0.0027625   0.0688446    0.00457344    0.0365104   -0.106517   
  0.118087    -0.0667212   -0.140497    -0.117072    -0.00403364  -0.0956234    0.0952671    0.225186    0.0201871     0.017015      0.0602007   -0.098601     0.0662934    0.0460679    -0.147278    -0.0178857    0.00552801  -0.069496    -0.0796598    0.114212    -0.18932      0.101616    0.135965     0.096361      0.0503215   -0.0863459  
  0.123046     0.138815     0.0344089   -0.0910103   -0.116009     0.0166088   -0.0378547    0.09962    -0.000944929   0.145187     -0.160794    -0.00181297   0.096587     0.0630785    -0.13883      0.139781    -0.0715311    0.032429    -0.0542745    0.0604938    0.0361512   -0.012245    0.174157    -0.0321904     0.122215    -0.17487    
  0.0560454    0.132687     0.115463     0.167614     0.0283549   -0.030567     0.0213124    0.0840997   0.0813811    -0.0301046     0.0214547    0.00205978  -0.105666     0.0855378     0.0996514    0.0148278    0.0293899   -0.0138652    0.0175651    0.0793006   -0.0054544   -0.0146396   0.0147713    0.0255028    -0.0388689    0.0480743  
 -0.0844656   -0.20914      0.185646    -0.538598     0.0260011    0.0847201    0.0955077   -0.0433052   0.393923      0.0676798     0.061747     0.0922799   -0.032578    -0.0667468     0.0619867   -0.0516834   -0.194969     0.0386092   -0.134755     0.105114    -0.0110188   -0.0184251   0.0304224    0.015855     -0.0189752    0.0347677  
 -0.205192    -0.055978    -0.0313329    0.340763     0.10049      0.211395     0.0193611   -0.0457174   0.0699049     0.128369      0.028641     0.0487335    0.152867    -0.00576997    0.118458    -0.199355    -0.236104    -0.209861     0.0463434   -0.110944     0.184659     0.0831902  -0.0466541   -0.157935      0.0363822    0.0775495  
  0.0169111   -0.0272233    0.00313234  -0.0564375   -0.06199     -0.0622495    0.0838557    0.03877     0.0376955     0.0658208     0.178888    -0.0200583   -0.101442    -0.114506     -0.0862849    0.0910301    0.0447409    0.0325533   -0.11908      0.073825     0.0732419   -0.0285719   0.119582     0.231863     -0.0173639    0.000114201
 -0.0482407    0.075836    -0.0425885    0.0079245    0.0928972   -0.0227717   -0.0169521    0.0792591  -0.103912      0.000415489  -0.0544726   -0.0471489    0.02801     -0.0132711    -0.0389394    0.075758     0.00577802   0.0203916    0.1305      -0.138454     0.0257925   -0.0571787  -0.0761743    0.13797       0.0236282   -0.139821   
  0.0737324   -0.0195625    0.0774669   -0.0986484   -0.0487589   -0.0357969   -0.11798     -0.0858804   0.115821      0.0728973     0.0171193    0.0521987    0.0688677   -0.0532031    -0.190474    -0.0472621   -0.0344138    0.0230796    0.0405576    0.00944779  -0.0972937   -0.0993866  -0.0732467   -0.0250518    -0.034326     0.0181311  
  0.237588     0.123877     0.00532955   0.0763209    0.0266402    0.16648      0.00151697   0.0242431   0.16272       0.00945878   -0.1209       0.253417    -0.0596354    0.043339     -0.0268965    0.0745507   -0.102098    -0.0290677    0.141835    -0.215868    -0.0792374   -0.0787639   0.177571     0.0541917     0.0180735    0.059791   
 -0.0547959    0.0570727   -0.100197    -0.0155766    0.0154225    0.0637536   -0.0865553    0.0309006   0.0357858     0.0202007     0.0391853    0.0606439   -0.011454    -0.0186762     0.101927    -0.0351542   -0.0197514    0.0387149    0.0822071   -0.112774     0.160453    -0.0262907   0.0337829   -0.0549689     0.0933108   -0.024611   
  0.0944496    0.122143    -0.0709662    0.128577    -0.185024     0.0881177   -0.194069    -0.159324    0.116709      0.113465      0.0289203    0.0473947   -0.091457    -0.0698482    -0.00722407   0.104756     0.0724405   -0.390097    -0.00179611  -0.0543789   -0.0560373   -0.024151   -0.11927     -0.000728031  -0.104321     0.0160933  
 -0.058828    -0.0392123    0.154989     0.110613     0.0341235    0.0687404    0.0143142   -0.0169239   0.0396669     0.158325      0.121937     0.0309284    0.00822126   0.032966     -0.147135    -0.0145887   -0.125038    -0.0996652    0.069428    -0.0168817    0.0482604   -0.103599   -0.0653469   -0.107265     -0.245542     0.0806344  
  0.0502094    0.0380341    0.184302    -0.0752316    0.151567    -0.0280125    0.0227443    0.102232    0.0351802     0.00855804    0.0553406   -0.0648208   -0.226392    -0.121145      0.0520613   -0.262325     0.134012    -0.0404245    0.0829412    0.0401602   -0.0529976    0.0272627   0.0455219   -0.0872492     0.106777    -0.10096    
  0.107066     0.218421    -0.0842795   -0.0741913    0.135048    -0.0315681    0.0931502   -0.0551505  -0.0111073     0.0540528     0.0318318   -0.00221218   0.114588    -0.152798     -0.187072    -0.133424    -0.210018     0.12393      0.00891453   0.163787     0.104258     0.016001    0.0479859    0.0648225     0.0047445    0.131463   
  0.254841    -0.0725195   -0.186326     0.0571548    0.0533076   -0.0232352    0.0953536    0.0969609  -0.0275904    -0.0443083    -0.0280816   -0.00112977  -0.0116      -0.101115      0.118326    -0.0324853   -0.0941557   -0.229357     0.221202     0.171408     0.148409    -0.0483137  -0.0391181    0.0652939    -0.116418    -0.0769924  
  0.119237    -0.125235    -0.0834553    0.183873     0.155618     0.0905456   -0.116558    -0.0104032   0.12105      -0.00963503   -0.0334119   -0.00601181   0.0474122    0.0382129    -0.104045    -0.0445437    0.00790715  -0.135935     0.0409183    0.0409994    0.0797704    0.0942944  -0.0116098    0.0371788    -0.143664     0.0752206  
 -0.0920994   -0.063899    -0.0556645   -0.0236406   -0.1023       0.0144628   -0.185584    -0.0562739  -0.159171     -0.033842     -0.0807593   -0.0537003   -0.0358621    0.146076     [ Info: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
-0.174594     0.0876862   -0.0573003   -0.0303087   -0.26075     -0.0495803   -0.0724356    0.25086    -0.18168     -0.0131323     0.00324636   0.0134689  
 -0.0213466    0.0365816   -0.142734     0.0509998    0.0548329   -0.0411879    0.00457039   0.0552326   0.0187556     0.0167299     0.223617    -0.152179     0.113137    -0.000261312   0.0834899   -0.00808098   0.0896381   -0.0941627   -0.0236564   -0.048314     0.0272027   -0.0514466  -0.0308746    0.0998301    -0.0148756   -0.0695759  
  0.130597    -0.0132105    0.166446     0.0568364   -0.0492702   -0.119764    -0.146879     0.0736244  -0.0172298     0.0300925    -0.100483    -0.0718494   -0.0163492    0.140333     -0.0519674   -0.221998     0.0787675    0.0130239   -0.0036677    0.176121     0.0393969   -0.106915   -0.113376     0.104958      0.0577083    0.111734   
 -0.148007     0.0230987   -0.122644    -0.129387     0.104222     0.0173435    0.161882     0.0268488  -0.136576     -0.039065      0.0810036    0.1143       0.0653852    0.156232     -0.124798    -0.0927187    0.105711    -0.0754982    0.129698    -0.0229199   -0.0927362   -0.132447   -0.007074     0.193662     -0.113897     0.0486352  ┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      4
│      6
│     12
│     18
│     20
│     24
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 1, average log likelihood -1.027863
┌ Warning: Variances had to be floored 
│   ind =
│    12-element Array{Int64,1}:
│      4
│      6
│     12
│     13
│      ⋮
│     25
│     26
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 2, average log likelihood -0.987100
┌ Warning: Variances had to be floored 
│   ind =
│    11-element Array{Int64,1}:
│      2
│      3
│      4
│      6
│      ⋮
│     24
│     27
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 3, average log likelihood -0.998243
┌ Warning: Variances had to be floored 
│   ind =
│    13-element Array{Int64,1}:
│      4
│      5
│      6
│     12
│      ⋮
│     25
│     26
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 4, average log likelihood -1.000573
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      3
│      4
│      6
│     12
│     18
│     20
│     24
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 5, average log likelihood -1.012490
┌ Warning: Variances had to be floored 
│   ind =
│    14-element Array{Int64,1}:
│      4
│      6
│      8
│     12
│      ⋮
│     26
│     27
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 6, average log likelihood -0.977697
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      3
│      4
│      6
│     12
│     18
│     20
│     24
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 7, average log likelihood -1.023266
┌ Warning: Variances had to be floored 
│   ind =
│    13-element Array{Int64,1}:
│      2
│      4
│      6
│     12
│      ⋮
│     25
│     26
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 8, average log likelihood -0.986216
┌ Warning: Variances had to be floored 
│   ind =
│    10-element Array{Int64,1}:
│      3
│      4
│      6
│      8
│      ⋮
│     24
│     27
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 9, average log likelihood -0.999221
┌ Warning: Variances had to be floored 
│   ind =
│    12-element Array{Int64,1}:
│      4
│      6
│     12
│     13
│      ⋮
│     25
│     26
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 10, average log likelihood -1.001179
┌ Info: EM with 100000 data points 10 iterations avll -1.001179
└ 59.0 data points per parameter
32×26 Array{Float64,2}:
  0.209678    -0.0545731   -0.000319896   0.110504     0.0138713   -0.0850002    -0.0651041   -0.0164322    0.110601    -0.0182741     0.0364011    0.00561157  -0.0183733    0.0522255   -0.134664    -0.0415129  -0.000221807  -0.131201    -0.0862306    0.20625     -0.0610268   -0.157508     -0.0226085   -0.0351911   -0.164952     0.0865891 
  0.0526507   -0.145856    -0.0267895    -0.0755042   -0.0885753   -0.117337      0.0890463   -0.0375069   -0.0243894   -0.0435794    -0.0431188    0.0723025    0.0146066   -0.00811535  -0.0290006   -0.0705792  -0.137569      0.0314478   -0.00422251   0.085183    -0.0820818   -0.0169497     0.0131845    0.111887    -0.13055      0.00798426
  0.037499    -0.0220318    0.162893      0.14528     -0.0376034   -0.0516071    -0.0304448    0.0143082   -0.081657    -0.0984288    -0.0157618    0.0300741    0.0119134   -0.0898336   -0.100786     0.0632858   0.0471319     0.00193666   0.100862     0.0427752    0.135709    -0.0677095     0.0398936   -0.29734     -0.117857    -0.139432  
  0.00307454  -0.00074067   0.00483105   -0.0939275    0.0561781   -0.0513058     0.0806192    0.0932107   -0.0642495    0.106536      0.162654     0.0271431   -0.157363    -0.161594     0.163989     0.178636   -0.0216805    -0.0353155   -0.0867326   -0.0690784    0.110734    -0.0239206    -0.0689178    0.00447722  -0.0302265    0.0828685 
  0.0165064   -0.0408345    0.105779     -0.0839554    0.228714     0.0748976    -0.00825025   0.0241892    0.03907     -0.0430697     0.0360336   -0.0923223    0.0351056   -0.156351     0.00410326   0.133516    0.174004     -0.00332529   0.053811    -0.0159498   -0.066956    -0.133552     -0.0411563    0.0284753   -0.0629121    0.00985431
  0.164951    -0.0665714   -0.117254      0.134206     0.148978    -0.0470349    -0.0392042   -0.0931385   -0.0617343    0.0727039    -0.0146584   -0.0137036   -0.00768581  -0.0899674   -0.00481824  -0.0473879   0.108297      0.127576    -0.0324389   -0.0355298    0.0687723    0.207054      0.0108658    0.0311843    0.168739     0.0309002 
 -0.0126022   -0.0378699    0.0454149    -0.258617    -0.0694322    0.0340576    -0.147372    -0.0734028   -0.00711799   0.0944888    -0.00729181   0.0626828    0.022977     0.208666    -0.00894429   0.0184426   0.123146     -0.0529524   -0.141685    -0.116219    -0.0764145    0.0175664     0.0176209   -0.0888328   -0.198793     0.0672099 
 -0.154553     0.0755372   -0.126545     -0.144341     0.0257012    0.112911      0.00627816  -0.0548983   -0.0850748   -0.061627     -0.0382247   -0.110073     0.0397628    0.0313877   -0.0773321    0.145615   -0.0911375    -0.0180095   -0.0149154   -0.0667599   -0.0715072    0.0121277    -0.163041     0.12052     -0.00615718   0.131442  
  0.0445318   -0.097462    -0.135304     -0.00733478  -0.0165017    0.125781     -0.0374124    0.0354596   -0.0559994   -0.0241097     0.0469291   -0.0506559   -0.00940095   0.0316995    0.0355756   -0.147197   -0.0856644     0.0810289    0.0376047   -0.0303058   -0.00280623   0.0197423    -0.0999781   -0.134011     0.147883     0.0551433 
 -0.0986334   -0.0488055   -0.200198      0.011902    -0.0401388   -0.0750252     0.138632    -0.102134     0.28662      0.0583967     0.0552161    0.118618     0.0391598    0.192265    -0.124448     0.141054    0.0242036    -0.115181    -0.0852635   -0.0571457    0.00291556  -0.184021     -0.00299222  -0.0205806    0.0122362    0.0195586 
 -0.00649143  -0.198002     0.0773407    -0.0834429   -0.213282     0.11973       0.139766     0.0334443   -0.0839959    0.057947     -0.227154     0.0114475   -0.0192866    0.12498     -0.0197154    0.214044   -0.08845      -0.084224     0.0385049   -0.0769725   -0.031919    -0.0487032    -0.0485575   -0.0185504    0.0559942    0.00266529
  0.130326    -0.0953313   -0.0389234    -0.00742779   0.162313    -0.208208      0.0886128    0.00945659   0.00200168  -0.0480998    -0.099078     0.0746256    0.0418729    0.0610264   -0.0367098   -0.0457002  -0.159723      0.0866116   -0.130132    -0.0168715    0.0119263    0.183757     -0.056624     0.0472153   -0.00556023   0.108742  
  0.0120189    0.06241     -0.0114067    -0.0335915    0.0881611    0.011152     -0.0847447   -0.12792     -0.107774    -0.0103936     0.05546      0.118491    -0.126035     0.149087    -0.00815503  -0.0503275  -0.0129303     0.137723    -0.0168219    0.0913737    0.0736159   -0.198159      0.0241556   -0.00353441   0.117273     0.0128882 
 -0.0422183   -0.145734     0.0272227     0.0654659   -0.145925    -0.075739      0.133346    -0.0325911   -0.0921896    0.185893     -0.0642698    0.162813     0.0415616    0.0813033   -0.0277347   -0.138031   -0.0817954     0.00507082   0.217926    -0.0452376    0.211957     0.0600393    -0.0850878    0.0864792   -0.0305355    0.00184423
  0.0320079   -0.14764      0.000790825   0.0423751    0.154122     0.0333138    -0.0706751   -0.071843     0.108688    -0.00862615    0.0120706   -0.103446     0.0129744    0.00539649  -0.0593254    0.0695251   0.200202      0.242655    -0.0228444    0.0801001    0.0301646    0.0808808    -0.0468159    0.273913    -0.0237354    0.036547  
  0.00837129  -0.165623     0.262176     -0.0979141    0.122751     0.0462842     0.0356897   -0.119725    -0.0426072   -0.0826731    -0.0346323    0.015422    -0.0738279   -0.054631     0.0809503   -0.191594    0.0235689     0.0857909    0.130274     0.0147746    0.0851485    0.0224288     0.0876128   -0.0137741    0.223783     0.0791644 
  0.015208    -0.187736     0.00859099    0.0727602    0.0882804   -0.0164847    -0.0306674   -0.0305684   -0.0397527    0.000620664  -0.106756    -0.033572     0.00796579   0.0475883    0.0114737    0.07508     0.0272972    -0.144155     0.0426282    0.0609471    0.0502642    0.000641025   0.0678209    0.00830356   0.0624673   -0.0194971 
 -0.0555168   -0.00275363  -0.139441     -0.0445834   -0.0189011    0.13896       0.0418537   -0.0236121   -0.149387    -0.0253345     0.0650085   -0.0118559    0.0207157   -0.0595879    0.107302     0.119036   -0.151116     -0.0717905    0.0721003    0.0395616    0.17968     -0.00784023    0.124297     0.00914128  -0.0504097    0.0120414 
 -0.108626    -0.158623     0.00836934    0.0141148    0.0571788    0.0695667     0.182341     0.0428234    0.0472952   -0.0491038     0.0444316    0.0459473    0.0499671   -0.181971    -0.0308493    0.130175    0.0503016    -0.0725915    0.0358128   -0.192401     0.109865    -0.000332941  -0.0447646   -0.0196967   -0.131047    -0.245363  
 -0.0748251    0.0971944    0.0408991    -0.0200683   -0.0768267   -0.0475951     0.0990949    0.032851     0.145427     0.11831       0.10004      0.04596     -0.036421     0.028176    -0.166779     0.0708517  -0.007231      0.0265217   -0.037485     0.00325264  -0.0637922   -0.101353      0.0083866    0.100505    -0.181138     0.0364213 
 -0.0442685    0.118453     0.0443923    -0.0173979   -0.0226064   -0.0347027     0.223847    -0.022607     0.0757947    0.126718      0.0454095    0.195174     0.00364517   0.0580455    0.109469    -0.0869899  -0.00781491   -0.0641892    0.0077452    0.106135     0.144834    -0.0300282    -0.00409372   0.0512658    0.0356856   -0.0463611 
 -0.0428202    0.158659    -0.142703     -0.040598    -0.115321    -0.222504      0.103163    -0.0186699   -0.196629     0.114002     -0.0908785    0.00089613   0.224955    -0.112526     0.00655961   0.149791    0.0841151    -0.0205435   -0.100723     0.0690535   -0.0806664   -0.0344989     0.125673    -0.0467548   -0.134018    -0.0423222 
 -0.0154132    0.0385145    0.0924337    -0.0199236   -0.246517    -0.0826984    -0.0426498    0.0831615   -0.156658     0.07455       0.0976336    0.11468     -0.0458012    0.0570978    0.0842281    0.188194    0.00931253    0.0624535   -0.15752     -0.0290948    0.0261838    0.0707141    -0.0869896    0.0248698    0.0675102   -0.268473  
  0.0218907   -0.00306996   0.114035     -0.0466957    0.0297814    0.103772      0.192303    -0.0841733   -0.0642545    0.259156      0.0432769    0.098122    -0.0136165   -0.0645437    0.0493051   -0.0183759   0.0608149    -0.0579276    0.0554762    0.0397105   -0.132938    -0.0681189    -0.165646    -0.0910666   -0.0312363    0.0951367 
 -0.00834928  -0.0358461    0.153118      0.0310496   -0.0960837    0.0288221    -0.0838102   -0.192952    -0.0817396    0.0682465     0.145256     0.00323871  -0.16768      0.0348623   -0.0286162   -0.147308   -0.0435376    -0.0503402    0.0261115   -0.117388    -0.0941006    0.311632     -0.043479     0.107895     0.195655    -0.014461  
  0.0162177    0.0277488   -0.036504      0.0510168    0.00578926   0.000596747   0.0188173    0.0313353    0.0513742   -0.0912886     0.0355644    0.133632     0.00369971   0.0109349   -0.0543475    0.0722305  -0.16249      -0.14239     -0.0381015    0.0965558   -0.0857827    0.0512135    -0.0831678   -0.124281    -0.0531176    0.0460197 
  0.128792     0.080911     0.047725      0.00575393   0.03177     -0.0460538    -0.0396922    0.0371241    0.245085    -0.0234857     0.0487923    0.0338059    0.0388754   -0.0159316    0.224609     0.0158397   0.119349     -0.120499     0.0461494   -0.0593339    0.0309796   -0.0660635     0.286341     0.0732855    0.0689218    0.0770357 
  0.0138421    0.0482554   -0.150786     -0.0459683    0.0700059   -0.0704576    -0.193788    -0.155213     0.0620207   -0.0547064    -0.0568805   -0.178496    -0.179069    -0.00843688   0.0670564    0.0608498   0.0467707     0.214069     0.0700991   -0.0220974    0.024113    -0.129443     -0.0812054    0.108318     0.0399655    0.194751  
  0.00672173   0.021606     0.0904543     0.00352832   0.0685363    0.0785668    -0.137214    -0.00416929  -0.100569    -0.129104     -0.0229836    0.0396967   -0.118023     0.0580947    0.0355442    0.192389   -0.0279971     0.0146065   -0.0211645   -0.0425302    0.0989234    0.0467593     0.132786     0.131239     0.0170803   -0.0337159 
  0.0446578    0.0220324   -0.0458127     0.00870958  -0.134198     0.00820116    0.0625873    0.0895297    0.00589335  -0.0931386    -0.183312     0.046299     0.0966876    0.0317986    0.206535     0.124969   -0.150716     -0.0217565    0.168259     0.136577     0.186839    -0.0273221    -0.0986783   -0.0747485   -0.170871    -0.0045905 
 -0.0124884    0.170105     0.110932     -0.0588447    0.20702      0.00627761   -0.0369719    0.0372842   -0.0496479    0.106907     -0.0614       0.0408732    0.0853447   -0.0288317    0.0954134    0.100918    0.0321353    -0.134679    -0.039023     0.130967     0.0956103    0.0622686    -0.126795     0.041867     0.0421358   -0.131233  
 -0.073551    -0.0569801    0.00547017    0.0357665    0.0375612    0.0198996    -0.135605     0.108217    -0.0956151    0.023066      0.0166595   -0.00319232  -0.0868454   -0.0930655    0.0729028   -0.0119418  -0.0737476     0.0721829   -0.0759122   -0.0967343   -0.174902    -0.148496     -0.0267228    0.0690881    0.0331182   -0.218654  kind full, method split
┌ Info: 0: avll = 
└   tll[1] = -1.4256710048106496
[ Info: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.425692
[ Info: iteration 2, average log likelihood -1.425606
[ Info: iteration 3, average log likelihood -1.425526
[ Info: iteration 4, average log likelihood -1.425419
[ Info: iteration 5, average log likelihood -1.425270
[ Info: iteration 6, average log likelihood -1.425068
[ Info: iteration 7, average log likelihood -1.424792
[ Info: iteration 8, average log likelihood -1.424400
[ Info: iteration 9, average log likelihood -1.423833
[ Info: iteration 10, average log likelihood -1.423061
[ Info: iteration 11, average log likelihood -1.422184
[ Info: iteration 12, average log likelihood -1.421421
[ Info: iteration 13, average log likelihood -1.420912
[ Info: iteration 14, average log likelihood -1.420632
[ Info: iteration 15, average log likelihood -1.420493
[ Info: iteration 16, average log likelihood -1.420428
[ Info: iteration 17, average log likelihood -1.420397
[ Info: iteration 18, average log likelihood -1.420382
[ Info: iteration 19, average log likelihood -1.420375
[ Info: iteration 20, average log likelihood -1.420372
[ Info: iteration 21, average log likelihood -1.420370
[ Info: iteration 22, average log likelihood -1.420369
[ Info: iteration 23, average log likelihood -1.420369
[ Info: iteration 24, average log likelihood -1.420369
[ Info: iteration 25, average log likelihood -1.420369
[ Info: iteration 26, average log likelihood -1.420368
[ Info: iteration 27, average log likelihood -1.420368
[ Info: iteration 28, average log likelihood -1.420368
[ Info: iteration 29, average log likelihood -1.420368
[ Info: iteration 30, average log likelihood -1.420368
[ Info: iteration 31, average log likelihood -1.420368
[ Info: iteration 32, average log likelihood -1.420368
[ Info: iteration 33, average log likelihood -1.420368
[ Info: iteration 34, average log likelihood -1.420368
[ Info: iteration 35, average log likelihood -1.420368
[ Info: iteration 36, average log likelihood -1.420368
[ Info: iteration 37, average log likelihood -1.420368
[ Info: iteration 38, average log likelihood -1.420368
[ Info: iteration 39, average log likelihood -1.420368
[ Info: iteration 40, average log likelihood -1.420368
[ Info: iteration 41, average log likelihood -1.420368
[ Info: iteration 42, average log likelihood -1.420368
[ Info: iteration 43, average log likelihood -1.420368
[ Info: iteration 44, average log likelihood -1.420368
[ Info: iteration 45, average log likelihood -1.420368
[ Info: iteration 46, average log likelihood -1.420368
[ Info: iteration 47, average log likelihood -1.420368
[ Info: iteration 48, average log likelihood -1.420367
[ Info: iteration 49, average log likelihood -1.420367
[ Info: iteration 50, average log likelihood -1.420367
┌ Info: EM with 100000 data points 50 iterations avll -1.420367
└ 952.4 data points per parameter
┌ Info: 1
│   : avll =  = &quot;: avll = &quot;
│   avll =
│    50-element Array{Float64,1}:
│     -1.4256919252913496
│     -1.4256062642391214
│      ⋮                 
└     -1.4203674743448234
[ Info: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.420388
[ Info: iteration 2, average log likelihood -1.420299
[ Info: iteration 3, average log likelihood -1.420217
[ Info: iteration 4, average log likelihood -1.420111
[ Info: iteration 5, average log likelihood -1.419977
[ Info: iteration 6, average log likelihood -1.419825
[ Info: iteration 7, average log likelihood -1.419677
[ Info: iteration 8, average log likelihood -1.419551
[ Info: iteration 9, average log likelihood -1.419455
[ Info: iteration 10, average log likelihood -1.419386
[ Info: iteration 11, average log likelihood -1.419335
[ Info: iteration 12, average log likelihood -1.419298
[ Info: iteration 13, average log likelihood -1.419268
[ Info: iteration 14, average log likelihood -1.419245
[ Info: iteration 15, average log likelihood -1.419225
[ Info: iteration 16, average log likelihood -1.419208
[ Info: iteration 17, average log likelihood -1.419192
[ Info: iteration 18, average log likelihood -1.419179
[ Info: iteration 19, average log likelihood -1.419167
[ Info: iteration 20, average log likelihood -1.419155
[ Info: iteration 21, average log likelihood -1.419145
[ Info: iteration 22, average log likelihood -1.419136
[ Info: iteration 23, average log likelihood -1.419127
[ Info: iteration 24, average log likelihood -1.419119
[ Info: iteration 25, average log likelihood -1.419112
[ Info: iteration 26, average log likelihood -1.419105
[ Info: iteration 27, average log likelihood -1.419099
[ Info: iteration 28, average log likelihood -1.419093
[ Info: iteration 29, average log likelihood -1.419088
[ Info: iteration 30, average log likelihood -1.419083
[ Info: iteration 31, average log likelihood -1.419078
[ Info: iteration 32, average log likelihood -1.419073
[ Info: iteration 33, average log likelihood -1.419068
[ Info: iteration 34, average log likelihood -1.419064
[ Info: iteration 35, average log likelihood -1.419059
[ Info: iteration 36, average log likelihood -1.419054
[ Info: iteration 37, average log likelihood -1.419049
[ Info: iteration 38, average log likelihood -1.419044
[ Info: iteration 39, average log likelihood -1.419038
[ Info: iteration 40, average log likelihood -1.419033
[ Info: iteration 41, average log likelihood -1.419026
[ Info: iteration 42, average log likelihood -1.419020
[ Info: iteration 43, average log likelihood -1.419013
[ Info: iteration 44, average log likelihood -1.419006
[ Info: iteration 45, average log likelihood -1.418998
[ Info: iteration 46, average log likelihood -1.418990
[ Info: iteration 47, average log likelihood -1.418981
[ Info: iteration 48, average log likelihood -1.418973
[ Info: iteration 49, average log likelihood -1.418964
[ Info: iteration 50, average log likelihood -1.418955
┌ Info: EM with 100000 data points 50 iterations avll -1.418955
└ 473.9 data points per parameter
┌ Info: 2
│   : avll =  = &quot;: avll = &quot;
│   avll =
│    50-element Array{Float64,1}:
│     -1.4203880315575792
│     -1.4202991856269498
│      ⋮                 
└     -1.4189546196347926
[ Info: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.418957
[ Info: iteration 2, average log likelihood -1.418880
[ Info: iteration 3, average log likelihood -1.418808
[ Info: iteration 4, average log likelihood -1.418721
[ Info: iteration 5, average log likelihood -1.418610
[ Info: iteration 6, average log likelihood -1.418474
[ Info: iteration 7, average log likelihood -1.418317
[ Info: iteration 8, average log likelihood -1.418151
[ Info: iteration 9, average log likelihood -1.417994
[ Info: iteration 10, average log likelihood -1.417856
[ Info: iteration 11, average log likelihood -1.417744
[ Info: iteration 12, average log likelihood -1.417656
[ Info: iteration 13, average log likelihood -1.417589
[ Info: iteration 14, average log likelihood -1.417538
[ Info: iteration 15, average log likelihood -1.417498
[ Info: iteration 16, average log likelihood -1.417467
[ Info: iteration 17, average log likelihood -1.417441
[ Info: iteration 18, average log likelihood -1.417420
[ Info: iteration 19, average log likelihood -1.417401
[ Info: iteration 20, average log likelihood -1.417385
[ Info: iteration 21, average log likelihood -1.417371
[ Info: iteration 22, average log likelihood -1.417358
[ Info: iteration 23, average log likelihood -1.417346
[ Info: iteration 24, average log likelihood -1.417334
[ Info: iteration 25, average log likelihood -1.417324
[ Info: iteration 26, average log likelihood -1.417314
[ Info: iteration 27, average log likelihood -1.417305
[ Info: iteration 28, average log likelihood -1.417296
[ Info: iteration 29, average log likelihood -1.417288
[ Info: iteration 30, average log likelihood -1.417280
[ Info: iteration 31, average log likelihood -1.417273
[ Info: iteration 32, average log likelihood -1.417266
[ Info: iteration 33, average log likelihood -1.417260
[ Info: iteration 34, average log likelihood -1.417254
[ Info: iteration 35, average log likelihood -1.417249
[ Info: iteration 36, average log likelihood -1.417244
[ Info: iteration 37, average log likelihood -1.417240
[ Info: iteration 38, average log likelihood -1.417235
[ Info: iteration 39, average log likelihood -1.417232
[ Info: iteration 40, average log likelihood -1.417228
[ Info: iteration 41, average log likelihood -1.417225
[ Info: iteration 42, average log likelihood -1.417222
[ Info: iteration 43, average log likelihood -1.417219
[ Info: iteration 44, average log likelihood -1.417217
[ Info: iteration 45, average log likelihood -1.417214
[ Info: iteration 46, average log likelihood -1.417212
[ Info: iteration 47, average log likelihood -1.417210
[ Info: iteration 48, average log likelihood -1.417208
[ Info: iteration 49, average log likelihood -1.417207
[ Info: iteration 50, average log likelihood -1.417205
┌ Info: EM with 100000 data points 50 iterations avll -1.417205
└ 236.4 data points per parameter
┌ Info: 3
│   : avll =  = &quot;: avll = &quot;
│   avll =
│    50-element Array{Float64,1}:
│     -1.4189571138650507
│     -1.4188803183756578
│      ⋮                 
└     -1.4172049934641877
[ Info: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.417214
[ Info: iteration 2, average log likelihood -1.417161
[ Info: iteration 3, average log likelihood -1.417116
[ Info: iteration 4, average log likelihood -1.417065
[ Info: iteration 5, average log likelihood -1.417003
[ Info: iteration 6, average log likelihood -1.416929
[ Info: iteration 7, average log likelihood -1.416841
[ Info: iteration 8, average log likelihood -1.416743
[ Info: iteration 9, average log likelihood -1.416640
[ Info: iteration 10, average log likelihood -1.416536
[ Info: iteration 11, average log likelihood -1.416437
[ Info: iteration 12, average log likelihood -1.416345
[ Info: iteration 13, average log likelihood -1.416263
[ Info: iteration 14, average log likelihood -1.416191
[ Info: iteration 15, average log likelihood -1.416128
[ Info: iteration 16, average log likelihood -1.416073
[ Info: iteration 17, average log likelihood -1.416025
[ Info: iteration 18, average log likelihood -1.415983
[ Info: iteration 19, average log likelihood -1.415945
[ Info: iteration 20, average log likelihood -1.415911
[ Info: iteration 21, average log likelihood -1.415880
[ Info: iteration 22, average log likelihood -1.415851
[ Info: iteration 23, average log likelihood -1.415823
[ Info: iteration 24, average log likelihood -1.415797
[ Info: iteration 25, average log likelihood -1.415773
[ Info: iteration 26, average log likelihood -1.415749
[ Info: iteration 27, average log likelihood -1.415725
[ Info: iteration 28, average log likelihood -1.415703
[ Info: iteration 29, average log likelihood -1.415681
[ Info: iteration 30, average log likelihood -1.415659
[ Info: iteration 31, average log likelihood -1.415639
[ Info: iteration 32, average log likelihood -1.415619
[ Info: iteration 33, average log likelihood -1.415599
[ Info: iteration 34, average log likelihood -1.415581
[ Info: iteration 35, average log likelihood -1.415563
[ Info: iteration 36, average log likelihood -1.415546
[ Info: iteration 37, average log likelihood -1.415529
[ Info: iteration 38, average log likelihood -1.415514
[ Info: iteration 39, average log likelihood -1.415499
[ Info: iteration 40, average log likelihood -1.415485
[ Info: iteration 41, average log likelihood -1.415472
[ Info: iteration 42, average log likelihood -1.415459
[ Info: iteration 43, average log likelihood -1.415448
[ Info: iteration 44, average log likelihood -1.415436
[ Info: iteration 45, average log likelihood -1.415426
[ Info: iteration 46, average log likelihood -1.415416
[ Info: iteration 47, average log likelihood -1.415407
[ Info: iteration 48, average log likelihood -1.415398
[ Info: iteration 49, average log likelihood -1.415390
[ Info: iteration 50, average log likelihood -1.415381
┌ Info: EM with 100000 data points 50 iterations avll -1.415381
└ 118.1 data points per parameter
┌ Info: 4
│   : avll =  = &quot;: avll = &quot;
│   avll =
│    50-element Array{Float64,1}:
│     -1.4172135504166399
│     -1.4171607824478456
│      ⋮                 
└     -1.41538144427789  
[ Info: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.415382
[ Info: iteration 2, average log likelihood -1.415319
[ Info: iteration 3, average log likelihood -1.415257
[ Info: iteration 4, average log likelihood -1.415184
[ Info: iteration 5, average log likelihood -1.415092
[ Info: iteration 6, average log likelihood -1.414977
[ Info: iteration 7, average log likelihood -1.414841
[ Info: iteration 8, average log likelihood -1.414692
[ Info: iteration 9, average log likelihood -1.414539
[ Info: iteration 10, average log likelihood -1.414389
[ Info: iteration 11, average log likelihood -1.414248
[ Info: iteration 12, average log likelihood -1.414118
[ Info: iteration 13, average log likelihood -1.414000
[ Info: iteration 14, average log likelihood -1.413892
[ Info: iteration 15, average log likelihood -1.413795
[ Info: iteration 16, average log likelihood -1.413708
[ Info: iteration 17, average log likelihood -1.413629
[ Info: iteration 18, average log likelihood -1.413559
[ Info: iteration 19, average log likelihood -1.413496
[ Info: iteration 20, average log likelihood -1.413439
[ Info: iteration 21, average log likelihood -1.413388
[ Info: iteration 22, average log likelihood -1.413342
[ Info: iteration 23, average log likelihood -1.413299
[ Info: iteration 24, average log likelihood -1.413259
[ Info: iteration 25, average log likelihood -1.413222
[ Info: iteration 26, average log likelihood -1.413188
[ Info: iteration 27, average log likelihood -1.413155
[ Info: iteration 28, average log likelihood -1.413125
[ Info: iteration 29, average log likelihood -1.413096
[ Info: iteration 30, average log likelihood -1.413068
[ Info: iteration 31, average log likelihood -1.413041
[ Info: iteration 32, average log likelihood -1.413016
[ Info: iteration 33, average log likelihood -1.412992
[ Info: iteration 34, average log likelihood -1.412968
[ Info: iteration 35, average log likelihood -1.412946
[ Info: iteration 36, average log likelihood -1.412924
[ Info: iteration 37, average log likelihood -1.412904
[ Info: iteration 38, average log likelihood -1.412884
[ Info: iteration 39, average log likelihood -1.412865
[ Info: iteration 40, average log likelihood -1.412846
[ Info: iteration 41, average log likelihood -1.412828
[ Info: iteration 42, average log likelihood -1.412811
[ Info: iteration 43, average log likelihood -1.412795
[ Info: iteration 44, average log likelihood -1.412779
[ Info: iteration 45, average log likelihood -1.412764
[ Info: iteration 46, average log likelihood -1.412749
[ Info: iteration 47, average log likelihood -1.412735
[ Info: iteration 48, average log likelihood -1.412722
[ Info: iteration 49, average log likelihood -1.412709
[ Info: iteration 50, average log likelihood -1.412696
┌ Info: EM with 100000 data points 50 iterations avll -1.412696
└ 59.0 data points per parameter
┌ Info: 5
│   : avll =  = &quot;: avll = &quot;
│   avll =
│    50-element Array{Float64,1}:
│     -1.4153819740617095
│     -1.4153186156642519
│      ⋮                 
└     -1.4126961763200634
┌ Info: Total log likelihood: 
│   tll =
│    251-element Array{Float64,1}:
│     -1.4256710048106496
│     -1.4256919252913496
│     -1.4256062642391214
│     -1.425526304993986 
│      ⋮                 
│     -1.412721841009552 
│     -1.4127087857349723
└     -1.4126961763200634
32×26 Array{Float64,2}:
  0.317143     0.236372     0.193522    0.0938385     0.77543      0.703674    0.113696    0.180045   -0.0598572    0.404281     -0.725811    0.15396     -0.158053    -0.133286    -0.0530578   -0.418185     0.380579    -0.313348    0.030729    0.246376     -0.600781   -0.180224    -0.179051     -0.258252     0.287481   -0.43908  
  1.21931      0.105637    -0.175056    0.0582545    -0.414948     0.253713    0.180969    0.215075   -0.179254     0.181296      0.510265   -0.261236    -0.14766      0.221674    -0.402982    -0.395736     0.0190441   -0.140611    0.166229    0.563224      0.301386   -0.00630945  -0.182052     -0.100808    -0.135079    0.342943 
  0.295951     0.592269    -0.0800064  -0.225528      0.336215     0.0619342   0.439025   -0.0268631  -0.703435     0.0937503     0.0420164  -0.419092    -0.00880336  -0.386144     0.415345    -0.7796      -0.188288     0.81138     0.215025    0.175953      0.124092    0.00770405  -0.967277     -0.161276     1.05894    -0.279737 
 -0.0902294   -0.0839108    0.0732167  -0.126395     -0.154653     0.0608042   0.140797    0.236866   -0.707775     0.166569     -0.487616   -0.365994     0.282956    -0.335438     0.717713    -0.614093     0.342827     0.487543    0.613743   -0.0648818     0.433025   -0.0892821    0.512773      0.0846982    0.128917   -0.141811 
 -0.266418    -0.904566     0.279515    0.125804      0.018413     0.350439    0.194699    0.0625831  -0.0107961    0.200865     -0.729275   -0.224793    -0.0159738    0.443927     0.0323134    0.616654    -0.0355779   -0.100271   -0.0326823  -0.274336      0.741232    0.338815     0.39233       0.397709    -0.506296   -0.0189848
 -0.176794     0.00818179   0.0134242  -0.593034      0.0619184    0.248632    0.200676    0.334101   -0.130291    -0.306541     -0.716312   -0.0623861    0.086836     0.46495      0.564041     1.10029      0.43314     -0.0905624  -0.606394   -1.0706        0.0315749   0.296269    -0.23354       0.0331492   -9.2949e-5  -0.57411  
  0.0509781   -0.380762     0.529689   -0.131473      0.00161801  -0.430119    0.155481    0.194773    0.00756057  -0.138424      0.603319    0.102121    -0.211444    -0.507044    -0.597703     0.633101    -0.301425    -0.0722722  -0.689123   -0.27676       0.213025   -0.164845     0.156792     -0.335339    -0.413248    0.124736 
  0.20927     -0.235303     0.218985   -0.448941     -0.129869    -0.178074    0.206332    0.17905    -0.564679     0.828053      0.240732    0.196883    -0.204359    -0.16746     -0.127693     0.501533     0.492692     0.16518    -0.418862   -1.18858       0.260632    0.807428     0.000118142  -0.233721     0.0933203   0.374396 
 -0.465899    -0.365284    -0.0255936  -0.393051      0.0516507    0.341431   -0.472709   -0.0418057   0.537606    -0.354644     -0.601242   -0.0730905    0.0272042   -0.140338     0.820908    -0.261452    -0.275059    -0.0977583   0.794347    0.95929      -0.18881    -0.0558842   -0.402626      0.00775866  -0.568183    0.0673037
 -0.320706    -0.207397     0.144081   -0.320489      0.739358    -0.577156    0.0784282  -0.0843396   0.266795    -0.877318     -0.514393    0.108038     0.32421     -0.42227      0.739987     0.196943    -0.144411     0.471928   -0.602985    0.294516     -0.226052    0.20136     -0.0228338    -0.333591     0.208462   -0.123293 
 -0.78587      0.508845     0.043395    0.28059       0.120074     0.0691706  -0.215139   -0.776013    0.0973924   -0.237546      0.024645    0.32576      0.694686     0.462541    -0.207615     0.108783    -0.396538    -0.251806    0.0308447  -0.0937733    -0.415649   -0.11529     -0.0988867    -0.286302     0.0631695  -0.449709 
 -0.117082     0.835411    -0.364437    0.375441     -0.00468457   0.122902    0.163923   -0.488491   -0.0272763   -0.337971      0.555338    0.673283    -0.00230991  -0.102035    -0.196033    -0.0832894    0.739773     0.368702    0.0854959  -0.198651     -0.279791   -0.383065     0.0116304    -0.525882    -0.516388    0.220873 
 -0.519188    -0.109669     0.263222   -0.426339     -0.186547    -0.34274    -0.325063    0.284535    0.348916    -0.124395      0.711712    0.219187    -0.135306     0.200742     0.148314     0.514688    -0.0919506   -0.187476    0.144697   -0.189628      0.0670233  -0.800434     0.545242      0.699457    -0.661432   -0.0270794
 -0.116913    -0.132679    -0.12595     0.247709     -0.21936     -0.367668   -0.0506455  -0.246422    0.133734     0.0357813     0.230039   -0.0198591    0.100293    -0.0470932    0.0219499   -0.0740698   -0.2183       0.0219105   0.0427454   0.000918851   0.0283518   0.0159793   -0.206424      0.287071     0.3041      0.199901 
 -0.32806     -0.141588    -0.583845   -0.220053      0.303319    -0.305644   -0.225368   -0.334997    0.135252     0.0165945     0.236339   -0.128791    -0.0278887   -0.097434    -0.728716    -0.122957    -0.0295757    0.236756   -0.124839    0.646274      0.227128    0.170786     0.609907     -0.922903     0.164852   -0.142376 
  0.329876     0.0780102   -0.456162   -0.338803     -0.217792    -0.43761    -0.723008   -0.351615    0.572827     0.276286      0.430573    0.440078     0.0829116   -0.374203    -0.223749    -0.102977     0.00469474  -0.0117087  -0.468483    0.423247     -0.371036   -0.453931    -0.30811      -0.18762      0.727783    0.142634 
 -0.0024125    0.0510164   -0.374984    0.0429712     0.111506    -0.745638   -0.217714   -0.640532   -0.213905     0.407827     -0.0919901  -0.131237     0.144796    -0.133836    -0.0664112    0.0841507    0.0892139    0.101455    0.0203993   0.0998405     0.327765    0.909811    -0.19517      -0.0338282    0.314098    0.307717 
  0.176674     0.181671    -0.465195    0.000147413   0.100741     0.554424   -0.188552   -0.360356   -0.488358     0.644442     -0.886508    0.12552      0.13913      0.122022     0.0476555   -0.206872     0.0895262   -0.240192    0.524275   -0.0165933     0.198176    0.670371    -0.220831      0.0902323    0.337792    0.137061 
  0.0984406    0.241927    -0.145338    0.182435     -0.126556    -0.0765112   0.0640993  -0.229942   -0.160633     0.0349479     0.161924    0.0610884    0.0224607   -0.079427     0.027032    -0.393076     0.0775655    0.063227    0.171727    0.15374      -0.161265   -0.164284    -0.0494145    -0.0779816    0.063499    0.11208  
  0.109255     0.105698     0.0578572   0.134521     -0.0881607   -0.0062416   0.121092   -0.0433512  -0.256874     0.010688      0.0716306   0.15234     -0.103468    -0.147491    -0.338385     0.413608    -0.0700342    0.203474   -0.132792   -0.282122      0.263355    0.16728      0.000344231  -0.151       -0.032531    0.0735766
  0.699636    -0.384743     0.223098   -0.198813     -0.183418     0.160947   -0.180967    0.434823   -0.14507      0.43636      -0.10191    -0.376267    -0.741559    -0.346168    -0.0270218    0.0977457    0.372926     0.0621093   0.137342    0.112467      0.335639    0.244239    -0.173663      0.3732      -0.184333    0.378838 
  0.466249    -0.232152    -0.221605   -0.436422      0.161982     0.0783741   0.201063    0.608621    0.130636     0.000866778   0.0520998   0.244357    -0.36644     -0.332881    -0.00133984  -0.012605    -0.0239495    0.0687037  -0.0552786   0.522342      0.137586   -0.113039     0.441709     -0.0852733   -0.369667    0.178154 
 -0.377798    -0.303297     0.333732   -0.390263      0.310787     0.159276   -0.297813   -0.0731013  -0.503737     0.337723     -0.335393   -0.233563     0.275737    -0.4312      -0.288017    -0.0605898   -0.146005     0.28482     0.309504    0.0964512     0.0898859   0.0658214    0.283677     -0.54286      0.0372157  -0.370941 
 -0.310552    -0.256367     0.132834   -0.242769      0.156001    -0.0218825   0.100997    0.173876   -0.127142    -0.0002389    -0.209406   -0.0872466    0.114166     0.0263954    0.559711     0.00898011  -0.011076    -0.0612373   0.193095   -0.220303      0.254446    0.0870886    0.122059      0.260231    -0.0430858  -0.136908 
 -0.140178    -0.259009    -0.0967902  -0.119727      0.111407     0.171363   -0.239735   -0.0223259   0.570735    -0.0023686    -0.0425513   0.157477    -0.0528406    0.390307    -0.152013     0.400381    -0.0966822   -0.268489   -0.336786   -0.0471789    -0.13564     0.0609843   -0.127323      0.0930314    0.0281585  -0.149327 
 -0.0769478   -0.122456    -0.335454    0.175928      0.231758    -0.234175    0.171964    0.0462135   0.475335    -0.042702      0.734578    0.118639    -0.264113     0.725044    -0.162552     0.146225    -0.36935     -0.90165    -0.0485939   0.191478     -0.379502    0.188386     0.0147762     0.0817242   -0.100623   -0.0816828
 -0.380415     0.100446     0.24531     0.242014      0.2804      -0.486452    0.208198   -0.0377543   0.657376    -0.346615      0.467154    0.159574     0.141219     0.00368927  -0.0315508   -0.138756    -0.155356     0.327293   -0.187669    0.13251      -0.367125   -0.498494     0.0636143     0.0867775    0.189716   -0.224407 
  0.28333      0.033632     0.0889504   0.0341396     0.238033     0.28929     0.279751    0.314787   -0.217868    -0.285751     -0.0462923  -0.289438     0.285803     0.508546     0.0556114   -0.0741521    0.385653    -0.02365    -0.0999457   0.309805     -0.408806   -0.175032     0.201888     -0.60669     -0.115677   -0.364478 
 -0.00677518   0.110925     0.232935    0.143224     -0.392009     0.141906   -0.35273    -0.205921   -0.00581223  -0.145085     -0.263907   -0.12909      0.0154547   -0.1145       0.0981106   -0.183339     8.17767e-5  -0.0130683   0.20607    -0.0357342    -0.562248   -0.461533    -0.997436      0.380392    -0.0312525   0.302712 
 -0.245094     0.101526    -0.0211822   0.0954948    -0.41528     -0.340561   -0.163398   -0.465218    0.11255     -0.0515575     0.1464      0.40301      0.125578    -0.125039     0.246558     0.132353    -0.498741    -0.0885094   0.198051   -0.0316929     0.55387    -0.157485    -0.0161342     0.797445    -0.0758466   0.334116 
  0.214778     0.105255     0.248178    0.526867     -0.54347      0.144847    0.366923    0.307629   -0.230545    -0.0265183     0.116385   -0.00605453  -0.231527     0.282916     0.267362    -0.209907    -0.00835614  -0.283191    0.0140078  -0.862719      0.0402105  -0.306132    -0.0996126     0.783028     0.0603392   0.0778359
  0.581474     0.382097     0.149576    0.575281     -0.158365     0.373891    0.693666    0.0943571   0.317472    -0.315484     -0.220816    0.35776     -0.455381     0.344414    -0.324999     1.08261     -0.440328    -0.105116   -0.810592   -0.280409      0.129621    0.104375    -0.191835      0.515092    -0.103764   -0.0533184[ Info: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.412684
[ Info: iteration 2, average log likelihood -1.412672
[ Info: iteration 3, average log likelihood -1.412661
[ Info: iteration 4, average log likelihood -1.412650
[ Info: iteration 5, average log likelihood -1.412639
[ Info: iteration 6, average log likelihood -1.412628
[ Info: iteration 7, average log likelihood -1.412618
[ Info: iteration 8, average log likelihood -1.412608
[ Info: iteration 9, average log likelihood -1.412598
[ Info: iteration 10, average log likelihood -1.412589
┌ Info: EM with 100000 data points 10 iterations avll -1.412589
└ 59.0 data points per parameter
kind full, method kmeans
[ Info: Initializing GMM, 32 Gaussians LinearAlgebra.diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       9.323728e+05
      1       7.038412e+05      -2.285316e+05 |       32
      2       6.918067e+05      -1.203450e+04 |       32
      3       6.866373e+05      -5.169443e+03 |       32
      4       6.840220e+05      -2.615301e+03 |       32
      5       6.823147e+05      -1.707237e+03 |       32
      6       6.810641e+05      -1.250600e+03 |       32
      7       6.800629e+05      -1.001285e+03 |       32
      8       6.792587e+05      -8.041484e+02 |       32
      9       6.785310e+05      -7.277103e+02 |       32
     10       6.779226e+05      -6.084150e+02 |       32
     11       6.774366e+05      -4.859679e+02 |       32
     12       6.770307e+05      -4.058670e+02 |       32
     13       6.766901e+05      -3.406611e+02 |       32
     14       6.763886e+05      -3.015284e+02 |       32
     15       6.761105e+05      -2.780137e+02 |       32
     16       6.758602e+05      -2.503046e+02 |       32
     17       6.756369e+05      -2.233398e+02 |       32
     18       6.754463e+05      -1.906273e+02 |       32
     19       6.752500e+05      -1.962597e+02 |       32
     20       6.750687e+05      -1.812641e+02 |       32
     21       6.749043e+05      -1.644360e+02 |       32
     22       6.747582e+05      -1.460852e+02 |       32
     23       6.746168e+05      -1.414657e+02 |       32
     24       6.744863e+05      -1.304982e+02 |       32
     25       6.743743e+05      -1.119408e+02 |       32
     26       6.742679e+05      -1.064435e+02 |       32
     27       6.741618e+05      -1.060603e+02 |       32
     28       6.740635e+05      -9.829144e+01 |       32
     29       6.739741e+05      -8.937847e+01 |       32
     30       6.738890e+05      -8.514890e+01 |       32
     31       6.738167e+05      -7.225374e+01 |       32
     32       6.737452e+05      -7.158719e+01 |       32
     33       6.736696e+05      -7.557579e+01 |       32
     34       6.735878e+05      -8.181884e+01 |       32
     35       6.735154e+05      -7.234111e+01 |       32
     36       6.734559e+05      -5.947722e+01 |       32
     37       6.733966e+05      -5.936856e+01 |       32
     38       6.733511e+05      -4.549677e+01 |       32
     39       6.733073e+05      -4.373150e+01 |       32
     40       6.732610e+05      -4.637677e+01 |       32
     41       6.732138e+05      -4.721015e+01 |       32
     42       6.731680e+05      -4.575222e+01 |       32
     43       6.731265e+05      -4.149356e+01 |       32
     44       6.730960e+05      -3.056365e+01 |       32
     45       6.730693e+05      -2.664510e+01 |       32
     46       6.730451e+05      -2.424017e+01 |       32
     47       6.730183e+05      -2.675508e+01 |       32
     48       6.729963e+05      -2.204345e+01 |       32
     49       6.729773e+05      -1.896360e+01 |       32
     50       6.729593e+05      -1.795937e+01 |       32
K-means terminated without convergence after 50 iterations (objv = 672959.3434042213)
┌ Info: K-means with 32000 data points using 50 iterations
└ 37.0 data points per parameter
[ Info: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.423994
[ Info: iteration 2, average log likelihood -1.419120
[ Info: iteration 3, average log likelihood -1.417866
[ Info: iteration 4, average log likelihood -1.416969
[ Info: iteration 5, average log likelihood -1.416014
[ Info: iteration 6, average log likelihood -1.415069
[ Info: iteration 7, average log likelihood -1.414347
[ Info: iteration 8, average log likelihood -1.413909
[ Info: iteration 9, average log likelihood -1.413662
[ Info: iteration 10, average log likelihood -1.413509
[ Info: iteration 11, average log likelihood -1.413401
[ Info: iteration 12, average log likelihood -1.413317
[ Info: iteration 13, average log likelihood -1.413246
[ Info: iteration 14, average log likelihood -1.413186
[ Info: iteration 15, average log likelihood -1.413132
[ Info: iteration 16, average log likelihood -1.413084
[ Info: iteration 17, average log likelihood -1.413041
[ Info: iteration 18, average log likelihood -1.413002
[ Info: iteration 19, average log likelihood -1.412966
[ Info: iteration 20, average log likelihood -1.412933
[ Info: iteration 21, average log likelihood -1.412902
[ Info: iteration 22, average log likelihood -1.412874
[ Info: iteration 23, average log likelihood -1.412847
[ Info: iteration 24, average log likelihood -1.412822
[ Info: iteration 25, average log likelihood -1.412798
[ Info: iteration 26, average log likelihood -1.412775
[ Info: iteration 27, average log likelihood -1.412754
[ Info: iteration 28, average log likelihood -1.412733
[ Info: iteration 29, average log likelihood -1.412714
[ Info: iteration 30, average log likelihood -1.412695
[ Info: iteration 31, average log likelihood -1.412677
[ Info: iteration 32, average log likelihood -1.412660
[ Info: iteration 33, average log likelihood -1.412643
[ Info: iteration 34, average log likelihood -1.412627
[ Info: iteration 35, average log likelihood -1.412611
[ Info: iteration 36, average log likelihood -1.412596
[ Info: iteration 37, average log likelihood -1.412581
[ Info: iteration 38, average log likelihood -1.412566
[ Info: iteration 39, average log likelihood -1.412553
[ Info: iteration 40, average log likelihood -1.412539
[ Info: iteration 41, average log likelihood -1.412526
[ Info: iteration 42, average log likelihood -1.412513
[ Info: iteration 43, average log likelihood -1.412501
[ Info: iteration 44, average log likelihood -1.412489
[ Info: iteration 45, average log likelihood -1.412478
[ Info: iteration 46, average log likelihood -1.412467
[ Info: iteration 47, average log likelihood -1.412456
[ Info: iteration 48, average log likelihood -1.412445
[ Info: iteration 49, average log likelihood -1.412435
[ Info: iteration 50, average log likelihood -1.412425
┌ Info: EM with 100000 data points 50 iterations avll -1.412425
└ 59.0 data points per parameter
32×26 Array{Float64,2}:
  0.569044    0.51024     -0.468428    0.135322   -0.651339    -0.145863      0.206458    -0.302231   -0.0266036  -0.189152    0.549856     0.326664   -0.0218841  -0.285613     0.105175    -0.18507    -0.0139159   0.101961    -0.0835431    0.306626    0.0832496   -0.441919   -0.0986674   -0.0621831  -0.171274    0.49825    
  0.701649   -0.631085     0.1648     -0.331817    0.134615    -0.0168713     0.0101732    0.673584    0.336634    0.10886     0.100702    -0.245521   -0.740926   -0.405197    -0.00339705   0.0141105   0.293239    0.171018    -0.00289223   0.267866    0.345417     0.0860003   0.127189     0.426105   -0.255414    0.291005   
 -0.545352    0.232302    -0.506747   -0.127131   -0.063652    -0.0175081    -0.269544    -0.173411   -0.308158   -0.130702   -0.547273     0.0498907   0.383927    0.146632     0.904821    -0.6626      1.10676     0.123365     0.797374    -0.205765    0.08731     -0.0270758   0.141656    -0.236141    0.23128     0.220385   
  0.635598    0.168516    -0.406069   -0.260743   -0.48506      0.230218     -0.144118     0.039921   -0.313198    0.744015    0.378648    -0.304705   -0.0530783   0.653376    -0.751944    -0.0634205  -0.0187883  -0.359722     0.688566    -0.0596467   0.402569     0.375604    0.0656379    0.0565636  -0.0159185   0.615967   
 -0.118135   -0.00925683   0.0884115  -0.391576    0.193982     0.135361      0.253261     0.211471   -0.121579   -0.169958   -0.626404     0.105316    0.114639    0.258363     0.421682     1.14882     0.316565    0.00384707  -0.69682     -0.943586   -0.00507811   0.490173   -0.285458    -0.014457    0.037337   -0.318466   
  0.145186   -0.0296716   -0.279617   -0.42636    -0.0159442   -0.380342     -0.720513    -0.173524    0.566355    0.200148    0.420365     0.190597   -0.0774554  -0.415952    -0.411335    -0.139961    0.0263417   0.0824224   -0.480341     0.597112   -0.305435    -0.592423   -0.0811157   -0.321188    0.546288    0.134493   
  0.0423344   0.107105     0.186369    0.0898028  -0.211983     0.056498      0.0885427    0.0418438  -0.241761   -0.141935   -0.0108357   -0.163595    0.0356725  -0.059073     0.231104    -0.214779    0.126497    0.252336     0.0838086   -0.167507   -0.255219    -0.249605   -0.333872    -0.0297411   0.0781033  -0.0549575  
 -0.113905    0.645434     0.181999    0.344095    0.490742     0.000565355  -0.138062    -0.518225    0.295619   -0.0687163  -0.0742024    0.200023    0.385487    0.369575    -0.193857    -0.286682    0.199133    0.211727     0.11514      0.218754   -0.596316    -0.174728   -0.504353     0.039151    0.387138   -0.47047    
  0.0822711  -0.0198935   -0.0340434  -0.0986457  -0.113576    -0.913424      0.343931     0.0348263  -0.0818483  -0.325701    0.943535    -0.367715    0.170197    0.20499      0.0204145    0.352274   -0.0824539   0.375203    -0.444562    -0.240833    0.568964     0.292389    0.450718    -0.0935627   0.0981048   0.169089   
 -0.256324    0.55086      0.18254     0.122096    0.262146    -0.00305331    0.0504152   -0.167018    0.14902    -0.403624    0.622902     0.670107   -0.178598   -0.235114    -0.58332      0.297433    0.412594    0.2033      -0.220511    -0.184931   -0.367903    -0.384317    0.102953    -0.744492   -0.880395    0.0162059  
  0.565607    0.105956     0.327501    0.485003   -0.00998975   0.175758      0.61136      0.185722    0.404016   -0.0793044   0.376268     0.198005   -0.533072    0.205677    -0.667884     0.773224   -0.760631   -0.305089    -0.698851    -0.248757    0.0522524    0.0603686  -0.144298     0.50817    -0.135525   -0.27248    
 -0.564032    0.0658081   -0.611933    0.447867    0.124704    -0.216473      0.388295    -0.511885    0.44614    -0.331245    0.197169     0.348972    0.44882     0.441674     0.0207545   -0.16308    -0.272016   -0.177888    -0.182559    -0.0913846  -0.310674    -0.189713    0.481477    -0.227785    0.558301   -0.114306   
 -0.62965     0.162109    -0.258578    0.165541   -0.233914    -0.0383861    -0.606419    -0.664964    0.0321238   0.0863179   0.128762     0.302995    0.103778    0.303303    -0.366903     0.496028   -0.353007   -0.420052    -0.04372     -0.308646   -0.0819749    0.307098   -0.267157    -0.0102895   0.0914491  -0.200721   
  0.220647    0.139775     0.144234   -0.100808    0.866026     0.236144      0.426528     0.453083   -0.0528022  -0.0431495   0.0441386   -0.284823    0.0616092   0.359374    -0.00436224  -0.0225376   0.251682   -0.149338    -0.030881     0.283893   -0.746758    -0.28418     0.349656    -0.574618   -0.0050155  -0.459298   
  0.518605   -0.235283    -0.0680682   0.645433   -0.0758413    0.011458     -0.0424001    0.018472   -0.345564   -0.234072   -0.101486    -0.185423    0.0590982   0.695214    -0.533023    -0.121632    0.41131    -0.0769613   -0.683662     0.899742    0.221716     0.228003    0.0267101   -1.05349    -0.32093    -0.149362   
  0.445538    0.413226    -0.12789     0.149535    0.241479     0.508022      0.371464    -0.0103352  -0.487097    0.292138   -0.362332    -0.0372163   0.0188817  -0.154705     0.136491    -0.70977     0.0868154   0.167182     0.296964     0.11772    -0.03045      0.102117   -0.42915     -0.145796    0.538402   -0.228807   
  0.443031   -0.392325    -0.0149897  -0.23037    -0.485667     0.12242       0.0695579    0.176395   -0.614028    0.516153    0.156958     0.173884   -0.427225   -0.780801    -0.447209     0.242584    0.360568    0.155168    -0.290419    -0.73666     0.159502     0.315755   -0.089391    -0.326838    0.0404825   0.597174   
 -0.460761   -0.44458      0.267515    0.443526   -0.0921226    0.248005     -0.400498    -0.130815    0.114935    0.0585554  -0.708249    -1.11945     0.0419189  -0.0683719   -0.288408    -0.0377315  -0.157826    0.335601    -0.350171    -0.482965   -0.0931053   -0.118102   -0.634673     1.03509     0.406056    0.412361   
 -0.305953   -0.252291     0.0572674  -0.142499    0.567078    -0.355707     -0.170333    -0.203964    0.377783   -0.238286   -0.129166     0.0836983   0.180731   -0.160428     0.120274     0.182696   -0.233928   -0.0442556   -0.25793      0.428259   -0.153193     0.116881    0.00679372  -0.196043    0.0660984  -0.168911   
 -0.512407   -0.368214     0.0530872  -0.0668924  -0.739156    -0.0979102    -0.243737    -0.269182    0.253725    0.419577    0.00895278   0.44624    -0.0744468   0.222537    -0.028342     0.750256   -0.142092   -0.165715     0.107682    -0.457893    0.518613    -0.141004    0.378198     0.609825   -0.514563    0.000517629
 -0.350731   -0.660024     0.389264   -0.0827779   0.454454     0.409864      0.377925     0.303157   -0.274868    0.0234661  -0.565134    -0.367441    0.0524339   0.535157     0.150718     0.436681    0.129739   -0.160423     0.0884165   -0.300306    0.574384     0.238534    0.620764     0.158369   -0.513689   -0.252953   
 -0.0218278  -0.206875    -0.0945292  -0.17995     0.132072    -0.0448048    -0.0288948    0.0932641   0.470487   -0.0630814   0.171252     0.146086   -0.0958404   0.180133    -0.0152467    0.176071   -0.111943   -0.245724    -0.13721      0.183253   -0.0718884   -0.0347735   0.00135636   0.0508439  -0.126115    0.00877369 
  0.398931    0.104857     0.0611507   0.550318   -0.668758     0.277915      0.196945     0.176623    0.0445207  -0.16149     0.0970417    0.165516   -0.239251    0.69842      0.392381    -0.060572    0.148077   -0.486113    -0.0151276   -0.569205   -0.0419845   -0.279596   -0.349888     0.876766   -0.16946     0.354834   
  0.0652874   0.00915167   0.106534    0.147857   -0.0737552    0.0608425     0.222659     0.182748   -0.302032    0.0568513  -0.0204606    0.152463   -0.0664924   0.017686    -0.11686      0.110435   -0.0847009   0.0434476    0.00586365  -0.331025    0.338069    -0.0147694   0.355074     0.141974   -0.0670117  -0.0184348  
  0.22304    -0.0299789   -0.372578   -0.174677    0.203805    -0.062929     -0.0735711   -0.211572   -0.327087    0.473311   -0.277616    -0.140372   -0.0655363  -0.196969    -0.106698     0.0274039   0.195764    0.120625     0.189679     0.243746    0.254958     0.685848   -0.0672326   -0.270577    0.263053    0.0912796  
 -0.072578   -0.248866    -0.22999    -0.462014   -0.216083     0.678495     -0.387233     0.234319    0.253652   -0.238628   -0.549388    -0.384301    0.0692186  -0.00396959   0.808374    -0.385062   -0.175902   -0.0850489    0.726939     1.02675    -0.159741    -0.155792    0.0244116    0.0859247  -0.313414   -0.166161   
 -0.350614    0.186657    -0.0566299   0.428352   -0.0624703   -0.618182     -0.0672819   -0.255975    0.219985    0.0873603   0.729869    -0.0361925   0.103166   -0.168287    -0.335811    -0.802135   -0.636495   -0.0370394    0.800311     0.980538   -0.397842    -0.470421   -0.274178     0.241296   -0.245345    0.399246   
 -0.538974   -0.0264891    0.435675   -0.260159   -0.118102    -0.389129      0.00935844   0.21682     0.286466   -0.57888     0.334099     0.338266    0.0887336  -0.0852606    0.668072     0.224152   -0.45835    -0.0480342    0.00970974  -0.217661   -0.175642    -0.690314   -0.0966161    0.601544   -0.185398   -0.122972   
  0.272389   -0.221517     0.419635   -0.131422   -0.454823    -0.00393329    0.0888737    0.4351     -1.03749     0.490485   -0.331703    -0.797783   -0.136913   -0.380566     0.516627    -0.274311   -0.0973544   0.0305758    0.427611    -0.199115    0.368322     0.0115062  -0.0236833    0.251775    0.0743916  -0.173412   
 -0.352722    0.0834159    0.220021    0.500907    0.0598096   -0.517376      0.0616193   -0.502487   -0.286816    0.280693   -0.229875     0.335161    0.123373   -0.646358     0.174671    -0.174353   -0.163582    0.449065     0.0564535   -0.18106     0.563854     0.184752    0.0404959    0.566353    0.311687    0.337005   
  0.315315    0.00557811   0.338962   -0.06163     0.0560968    0.343834     -0.365046    -0.30035    -0.0681938   0.322292   -0.831999     0.452273   -0.0452163  -0.242088     0.217641    -0.109797   -0.132836   -0.411807     0.496387     0.110881   -0.115245     0.228011   -0.707541     0.388176   -0.334213    0.236135   
 -0.522623   -0.310209     0.212284   -0.626824    0.398384     0.0432235    -0.24275     -0.196782   -0.263758    0.0260943  -0.225035    -0.141567    0.491693   -0.475672    -0.108216    -0.0687905  -0.228204    0.371981     0.174687     0.244186    0.223601     0.0494361   0.328686    -0.618137   -0.0352778  -0.390264   [ Info: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.412416
[ Info: iteration 2, average log likelihood -1.412406
[ Info: iteration 3, average log likelihood -1.412397
[ Info: iteration 4, average log likelihood -1.412388
[ Info: iteration 5, average log likelihood -1.412380
[ Info: iteration 6, average log likelihood -1.412371
[ Info: iteration 7, average log likelihood -1.412363
[ Info: iteration 8, average log likelihood -1.412355
[ Info: iteration 9, average log likelihood -1.412347
[ Info: iteration 10, average log likelihood -1.412339
┌ Info: EM with 100000 data points 10 iterations avll -1.412339
└ 59.0 data points per parameter
[ Info: Initializing GMM, 2 Gaussians LinearAlgebra.diag covariance 2 dimensions using 900 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.678561e+05
      1       2.230230e+04      -1.455538e+05 |        2
      2       7.823675e+03      -1.447862e+04 |        0
      3       7.823675e+03       0.000000e+00 |        0
K-means converged with 3 iterations (objv = 7823.67549422947)
┌ Info: K-means with 900 data points using 3 iterations
└ 150.0 data points per parameter
[ Info: Running 10 iterations EM on full cov GMM with 2 Gaussians in 2 dimensions
[ Info: iteration 1, average log likelihood -2.043155
[ Info: iteration 2, average log likelihood -2.043154
[ Info: iteration 3, average log likelihood -2.043154
[ Info: iteration 4, average log likelihood -2.043154
[ Info: iteration 5, average log likelihood -2.043154
[ Info: iteration 6, average log likelihood -2.043154
[ Info: iteration 7, average log likelihood -2.043154
[ Info: iteration 8, average log likelihood -2.043154
[ Info: iteration 9, average log likelihood -2.043154
[ Info: iteration 10, average log likelihood -2.043154
┌ Info: EM with 900 data points 10 iterations avll -2.043154
└ 81.8 data points per parameter
   Testing GaussianMixtures tests passed 
</pre>
      </div>

    <h3>Results with Julia v1.2.0</h3>

    <p>
      Testing was <strong>successful</strong>.
      Last evaluation was 1 day, 11 hours ago and took 8 minutes, 48 seconds.
    </p>

    <p>
      Click <a href="/home/tim/Julia/pkg/NewPkgEval/site/build/logs/GaussianMixtures/1.2.0.log">here</a> to download the log file.
      
    </p>

      <button class="collapsible">Click here to show the log contents.</button>
      <div class="content">
      <pre> Resolving package versions...
 Installed DataAPI ──────────── v1.1.0
 Installed GaussianMixtures ─── v0.3.0
 Installed Missings ─────────── v0.4.3
 Installed PDMats ───────────── v0.9.10
 Installed FileIO ───────────── v1.0.7
 Installed NearestNeighbors ─── v0.4.3
 Installed BinaryProvider ───── v0.5.8
 Installed StatsBase ────────── v0.32.0
 Installed URIParser ────────── v0.4.0
 Installed Blosc ────────────── v0.5.1
 Installed Rmath ────────────── v0.5.1
 Installed JLD ──────────────── v0.9.1
 Installed ScikitLearnBase ──── v0.5.0
 Installed StatsFuns ────────── v0.9.0
 Installed HDF5 ─────────────── v0.12.5
 Installed Compat ───────────── v2.2.0
 Installed OrderedCollections ─ v1.1.0
 Installed CMake ────────────── v1.1.2
 Installed DataStructures ───── v0.17.6
 Installed QuadGK ───────────── v2.1.1
 Installed Parameters ───────── v0.12.0
 Installed StaticArrays ─────── v0.12.1
 Installed SortingAlgorithms ── v0.3.1
 Installed CMakeWrapper ─────── v0.2.3
 Installed Distances ────────── v0.8.2
 Installed Distributions ────── v0.21.8
 Installed SpecialFunctions ─── v0.8.0
 Installed BinDeps ──────────── v0.8.10
 Installed LegacyStrings ────── v0.4.1
 Installed Clustering ───────── v0.13.3
 Installed Arpack ───────────── v0.3.1
  Updating `~&#x2F;.julia&#x2F;environments&#x2F;v1.2&#x2F;Project.toml`
  [cc18c42c] + GaussianMixtures v0.3.0
  Updating `~&#x2F;.julia&#x2F;environments&#x2F;v1.2&#x2F;Manifest.toml`
  [7d9fca2a] + Arpack v0.3.1
  [9e28174c] + BinDeps v0.8.10
  [b99e7846] + BinaryProvider v0.5.8
  [a74b3585] + Blosc v0.5.1
  [631607c0] + CMake v1.1.2
  [d5fb7624] + CMakeWrapper v0.2.3
  [aaaa29a8] + Clustering v0.13.3
  [34da2185] + Compat v2.2.0
  [9a962f9c] + DataAPI v1.1.0
  [864edb3b] + DataStructures v0.17.6
  [b4f34e82] + Distances v0.8.2
  [31c24e10] + Distributions v0.21.8
  [5789e2e9] + FileIO v1.0.7
  [cc18c42c] + GaussianMixtures v0.3.0
  [f67ccb44] + HDF5 v0.12.5
  [4138dd39] + JLD v0.9.1
  [1b4a561d] + LegacyStrings v0.4.1
  [e1d29d7a] + Missings v0.4.3
  [b8a86587] + NearestNeighbors v0.4.3
  [bac558e1] + OrderedCollections v1.1.0
  [90014a1f] + PDMats v0.9.10
  [d96e819e] + Parameters v0.12.0
  [1fd47b50] + QuadGK v2.1.1
  [79098fc4] + Rmath v0.5.1
  [6e75b9c4] + ScikitLearnBase v0.5.0
  [a2af1166] + SortingAlgorithms v0.3.1
  [276daf66] + SpecialFunctions v0.8.0
  [90137ffa] + StaticArrays v0.12.1
  [2913bbd2] + StatsBase v0.32.0
  [4c63d2b9] + StatsFuns v0.9.0
  [30578b45] + URIParser v0.4.0
  [2a0f44e3] + Base64 
  [ade2ca70] + Dates 
  [8bb1440f] + DelimitedFiles 
  [8ba89e20] + Distributed 
  [b77e0a4c] + InteractiveUtils 
  [76f85450] + LibGit2 
  [8f399da3] + Libdl 
  [37e2e46d] + LinearAlgebra 
  [56ddb016] + Logging 
  [d6f4376e] + Markdown 
  [a63ad114] + Mmap 
  [44cfe95a] + Pkg 
  [de0858da] + Printf 
  [9abbd945] + Profile 
  [3fa0cd96] + REPL 
  [9a3f8284] + Random 
  [ea8e919c] + SHA 
  [9e88b42a] + Serialization 
  [1a1011a3] + SharedArrays 
  [6462fe0b] + Sockets 
  [2f01184e] + SparseArrays 
  [10745b16] + Statistics 
  [4607b0f0] + SuiteSparse 
  [8dfed614] + Test 
  [cf7118a7] + UUIDs 
  [4ec0a83e] + Unicode 
  Building CMake ───────────→ `~&#x2F;.julia&#x2F;packages&#x2F;CMake&#x2F;nSK2r&#x2F;deps&#x2F;build.log`
  Building Blosc ───────────→ `~&#x2F;.julia&#x2F;packages&#x2F;Blosc&#x2F;lzFr0&#x2F;deps&#x2F;build.log`
  Building HDF5 ────────────→ `~&#x2F;.julia&#x2F;packages&#x2F;HDF5&#x2F;Zh9on&#x2F;deps&#x2F;build.log`
  Building Rmath ───────────→ `~&#x2F;.julia&#x2F;packages&#x2F;Rmath&#x2F;4wt82&#x2F;deps&#x2F;build.log`
  Building SpecialFunctions → `~&#x2F;.julia&#x2F;packages&#x2F;SpecialFunctions&#x2F;ne2iw&#x2F;deps&#x2F;build.log`
  Building Arpack ──────────→ `~&#x2F;.julia&#x2F;packages&#x2F;Arpack&#x2F;cu5By&#x2F;deps&#x2F;build.log`
   Testing GaussianMixtures
    Status `&#x2F;tmp&#x2F;jl_zt2RbN&#x2F;Manifest.toml`
  [7d9fca2a] Arpack v0.3.1
  [9e28174c] BinDeps v0.8.10
  [b99e7846] BinaryProvider v0.5.8
  [a74b3585] Blosc v0.5.1
  [631607c0] CMake v1.1.2
  [d5fb7624] CMakeWrapper v0.2.3
  [aaaa29a8] Clustering v0.13.3
  [34da2185] Compat v2.2.0
  [9a962f9c] DataAPI v1.1.0
  [864edb3b] DataStructures v0.17.6
  [b4f34e82] Distances v0.8.2
  [31c24e10] Distributions v0.21.8
  [5789e2e9] FileIO v1.0.7
  [cc18c42c] GaussianMixtures v0.3.0
  [f67ccb44] HDF5 v0.12.5
  [4138dd39] JLD v0.9.1
  [1b4a561d] LegacyStrings v0.4.1
  [e1d29d7a] Missings v0.4.3
  [b8a86587] NearestNeighbors v0.4.3
  [bac558e1] OrderedCollections v1.1.0
  [90014a1f] PDMats v0.9.10
  [d96e819e] Parameters v0.12.0
  [1fd47b50] QuadGK v2.1.1
  [79098fc4] Rmath v0.5.1
  [6e75b9c4] ScikitLearnBase v0.5.0
  [a2af1166] SortingAlgorithms v0.3.1
  [276daf66] SpecialFunctions v0.8.0
  [90137ffa] StaticArrays v0.12.1
  [2913bbd2] StatsBase v0.32.0
  [4c63d2b9] StatsFuns v0.9.0
  [30578b45] URIParser v0.4.0
  [2a0f44e3] Base64  [`@stdlib&#x2F;Base64`]
  [ade2ca70] Dates  [`@stdlib&#x2F;Dates`]
  [8bb1440f] DelimitedFiles  [`@stdlib&#x2F;DelimitedFiles`]
  [8ba89e20] Distributed  [`@stdlib&#x2F;Distributed`]
  [b77e0a4c] InteractiveUtils  [`@stdlib&#x2F;InteractiveUtils`]
  [76f85450] LibGit2  [`@stdlib&#x2F;LibGit2`]
  [8f399da3] Libdl  [`@stdlib&#x2F;Libdl`]
  [37e2e46d] LinearAlgebra  [`@stdlib&#x2F;LinearAlgebra`]
  [56ddb016] Logging  [`@stdlib&#x2F;Logging`]
  [d6f4376e] Markdown  [`@stdlib&#x2F;Markdown`]
  [a63ad114] Mmap  [`@stdlib&#x2F;Mmap`]
  [44cfe95a] Pkg  [`@stdlib&#x2F;Pkg`]
  [de0858da] Printf  [`@stdlib&#x2F;Printf`]
  [9abbd945] Profile  [`@stdlib&#x2F;Profile`]
  [3fa0cd96] REPL  [`@stdlib&#x2F;REPL`]
  [9a3f8284] Random  [`@stdlib&#x2F;Random`]
  [ea8e919c] SHA  [`@stdlib&#x2F;SHA`]
  [9e88b42a] Serialization  [`@stdlib&#x2F;Serialization`]
  [1a1011a3] SharedArrays  [`@stdlib&#x2F;SharedArrays`]
  [6462fe0b] Sockets  [`@stdlib&#x2F;Sockets`]
  [2f01184e] SparseArrays  [`@stdlib&#x2F;SparseArrays`]
  [10745b16] Statistics  [`@stdlib&#x2F;Statistics`]
  [4607b0f0] SuiteSparse  [`@stdlib&#x2F;SuiteSparse`]
  [8dfed614] Test  [`@stdlib&#x2F;Test`]
  [cf7118a7] UUIDs  [`@stdlib&#x2F;UUIDs`]
  [4ec0a83e] Unicode  [`@stdlib&#x2F;Unicode`]
[ Info: Testing Data
(100000, -761191.2106813154, [68121.27765917926, 31878.72234082074], [-6966.296999439288 -932.0013104066975 -5076.340796616561; 6943.434189374497 1359.6672974372925 5124.586088157055], Array{Float64,2}[[70598.46211462152 6244.474314735143 6650.908225904578; 6244.474314735145 91360.66447072447 6226.180444579147; 6650.908225904577 6226.180444579147 51811.09983495447], [29365.02828500796 -6154.675734336827 -7279.370037253707; -6154.675734336827 8351.720303487433 -6502.1879243508; -7279.370037253707 -6502.1879243508 48077.46905875519]])
┌ Warning: rmprocs: process 1 not removed
└ @ Distributed &#x2F;buildworker&#x2F;worker&#x2F;package_linux64&#x2F;build&#x2F;usr&#x2F;share&#x2F;julia&#x2F;stdlib&#x2F;v1.2&#x2F;Distributed&#x2F;src&#x2F;cluster.jl:1005
[ Info: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       2.675080e+03
      1       1.555168e+03      -1.119912e+03 |        8
      2       1.377874e+03      -1.772939e+02 |        8
      3       1.233341e+03      -1.445329e+02 |        6
      4       1.098628e+03      -1.347128e+02 |        6
      5       9.966686e+02      -1.019596e+02 |        5
      6       9.021666e+02      -9.450197e+01 |        6
      7       8.594707e+02      -4.269588e+01 |        2
      8       8.525982e+02      -6.872492e+00 |        3
      9       8.441928e+02      -8.405457e+00 |        0
     10       8.441928e+02       0.000000e+00 |        0
K-means converged with 10 iterations (objv = 844.1927848174673)
┌ Info: K-means with 272 data points using 10 iterations
└ 11.3 data points per parameter
[ Info: Running 0 iterations EM on full cov GMM with 8 Gaussians in 2 dimensions
┌ Info: EM with 272 data points 0 iterations avll -2.058483
└ 5.8 data points per parameter
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = lowerbound(::VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at bayes.jl:221
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;bayes.jl:221
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = lowerbound(::VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at bayes.jl:221
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;bayes.jl:221
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = _broadcast_getindex_evalf at broadcast.jl:625 [inlined]
└ @ Core .&#x2F;broadcast.jl:625
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = lowerbound(::VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at bayes.jl:230
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;bayes.jl:230
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = _broadcast_getindex_evalf at broadcast.jl:625 [inlined]
└ @ Core .&#x2F;broadcast.jl:625
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = _broadcast_getindex_evalf at broadcast.jl:625 [inlined]
└ @ Core .&#x2F;broadcast.jl:625
[ Info: iteration 1, lowerbound -3.841633
[ Info: iteration 2, lowerbound -3.760648
[ Info: iteration 3, lowerbound -3.673402
[ Info: iteration 4, lowerbound -3.560586
[ Info: iteration 5, lowerbound -3.425188
[ Info: iteration 6, lowerbound -3.278137
[ Info: dropping number of Gaussions to 7
[ Info: iteration 7, lowerbound -3.121268
[ Info: iteration 8, lowerbound -2.956804
[ Info: dropping number of Gaussions to 6
[ Info: iteration 9, lowerbound -2.793405
[ Info: iteration 10, lowerbound -2.646529
[ Info: dropping number of Gaussions to 5
[ Info: iteration 11, lowerbound -2.535225
[ Info: dropping number of Gaussions to 4
[ Info: iteration 12, lowerbound -2.452804
[ Info: iteration 13, lowerbound -2.397300
[ Info: dropping number of Gaussions to 3
[ Info: iteration 14, lowerbound -2.360408
[ Info: iteration 15, lowerbound -2.329711
[ Info: iteration 16, lowerbound -2.311973
[ Info: iteration 17, lowerbound -2.307667
[ Info: dropping number of Gaussions to 2
[ Info: iteration 18, lowerbound -2.302919
[ Info: iteration 19, lowerbound -2.299260
[ Info: iteration 20, lowerbound -2.299256
[ Info: iteration 21, lowerbound -2.299254
[ Info: iteration 22, lowerbound -2.299254
[ Info: iteration 23, lowerbound -2.299253
[ Info: iteration 24, lowerbound -2.299253
[ Info: iteration 25, lowerbound -2.299253
[ Info: iteration 26, lowerbound -2.299253
[ Info: iteration 27, lowerbound -2.299253
[ Info: iteration 28, lowerbound -2.299253
[ Info: iteration 29, lowerbound -2.299253
[ Info: iteration 30, lowerbound -2.299253
[ Info: iteration 31, lowerbound -2.299253
[ Info: iteration 32, lowerbound -2.299253
[ Info: iteration 33, lowerbound -2.299253
[ Info: iteration 34, lowerbound -2.299253
[ Info: iteration 35, lowerbound -2.299253
[ Info: iteration 36, lowerbound -2.299253
[ Info: iteration 37, lowerbound -2.299253
[ Info: iteration 38, lowerbound -2.299253
[ Info: iteration 39, lowerbound -2.299253
[ Info: iteration 40, lowerbound -2.299253
[ Info: iteration 41, lowerbound -2.299253
[ Info: iteration 42, lowerbound -2.299253
[ Info: iteration 43, lowerbound -2.299253
[ Info: iteration 44, lowerbound -2.299253
[ Info: iteration 45, lowerbound -2.299253
[ Info: iteration 46, lowerbound -2.299253
[ Info: 47 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
History[Sat Nov 23 23:32:28 2019: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
, Sat Nov 23 23:32:35 2019: K-means with 272 data points using 10 iterations
11.3 data points per parameter
, Sat Nov 23 23:32:36 2019: EM with 272 data points 0 iterations avll -2.058483
5.8 data points per parameter
, Sat Nov 23 23:32:38 2019: GMM converted to Variational GMM
, Sat Nov 23 23:32:45 2019: iteration 1, lowerbound -3.841633
, Sat Nov 23 23:32:45 2019: iteration 2, lowerbound -3.760648
, Sat Nov 23 23:32:45 2019: iteration 3, lowerbound -3.673402
, Sat Nov 23 23:32:45 2019: iteration 4, lowerbound -3.560586
, Sat Nov 23 23:32:45 2019: iteration 5, lowerbound -3.425188
, Sat Nov 23 23:32:45 2019: iteration 6, lowerbound -3.278137
, Sat Nov 23 23:32:46 2019: dropping number of Gaussions to 7
, Sat Nov 23 23:32:46 2019: iteration 7, lowerbound -3.121268
, Sat Nov 23 23:32:46 2019: iteration 8, lowerbound -2.956804
, Sat Nov 23 23:32:46 2019: dropping number of Gaussions to 6
, Sat Nov 23 23:32:46 2019: iteration 9, lowerbound -2.793405
, Sat Nov 23 23:32:46 2019: iteration 10, lowerbound -2.646529
, Sat Nov 23 23:32:46 2019: dropping number of Gaussions to 5
, Sat Nov 23 23:32:46 2019: iteration 11, lowerbound -2.535225
, Sat Nov 23 23:32:46 2019: dropping number of Gaussions to 4
, Sat Nov 23 23:32:46 2019: iteration 12, lowerbound -2.452804
, Sat Nov 23 23:32:46 2019: iteration 13, lowerbound -2.397300
, Sat Nov 23 23:32:46 2019: dropping number of Gaussions to 3
, Sat Nov 23 23:32:46 2019: iteration 14, lowerbound -2.360408
, Sat Nov 23 23:32:46 2019: iteration 15, lowerbound -2.329711
, Sat Nov 23 23:32:46 2019: iteration 16, lowerbound -2.311973
, Sat Nov 23 23:32:46 2019: iteration 17, lowerbound -2.307667
, Sat Nov 23 23:32:46 2019: dropping number of Gaussions to 2
, Sat Nov 23 23:32:46 2019: iteration 18, lowerbound -2.302919
, Sat Nov 23 23:32:46 2019: iteration 19, lowerbound -2.299260
, Sat Nov 23 23:32:46 2019: iteration 20, lowerbound -2.299256
, Sat Nov 23 23:32:46 2019: iteration 21, lowerbound -2.299254
, Sat Nov 23 23:32:46 2019: iteration 22, lowerbound -2.299254
, Sat Nov 23 23:32:46 2019: iteration 23, lowerbound -2.299253
, Sat Nov 23 23:32:46 2019: iteration 24, lowerbound -2.299253
, Sat Nov 23 23:32:46 2019: iteration 25, lowerbound -2.299253
, Sat Nov 23 23:32:46 2019: iteration 26, lowerbound -2.299253
, Sat Nov 23 23:32:46 2019: iteration 27, lowerbound -2.299253
, Sat Nov 23 23:32:46 2019: iteration 28, lowerbound -2.299253
, Sat Nov 23 23:32:46 2019: iteration 29, lowerbound -2.299253
, Sat Nov 23 23:32:46 2019: iteration 30, lowerbound -2.299253
, Sat Nov 23 23:32:46 2019: iteration 31, lowerbound -2.299253
, Sat Nov 23 23:32:46 2019: iteration 32, lowerbound -2.299253
, Sat Nov 23 23:32:46 2019: iteration 33, lowerbound -2.299253
, Sat Nov 23 23:32:46 2019: iteration 34, lowerbound -2.299253
, Sat Nov 23 23:32:46 2019: iteration 35, lowerbound -2.299253
, Sat Nov 23 23:32:46 2019: iteration 36, lowerbound -2.299253
, Sat Nov 23 23:32:46 2019: iteration 37, lowerbound -2.299253
, Sat Nov 23 23:32:46 2019: iteration 38, lowerbound -2.299253
, Sat Nov 23 23:32:46 2019: iteration 39, lowerbound -2.299253
, Sat Nov 23 23:32:46 2019: iteration 40, lowerbound -2.299253
, Sat Nov 23 23:32:46 2019: iteration 41, lowerbound -2.299253
, Sat Nov 23 23:32:46 2019: iteration 42, lowerbound -2.299253
, Sat Nov 23 23:32:46 2019: iteration 43, lowerbound -2.299253
, Sat Nov 23 23:32:46 2019: iteration 44, lowerbound -2.299253
, Sat Nov 23 23:32:46 2019: iteration 45, lowerbound -2.299253
, Sat Nov 23 23:32:46 2019: iteration 46, lowerbound -2.299253
, Sat Nov 23 23:32:46 2019: 47 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
]
α = [178.04509222601484, 95.95490777398514]
β = [178.04509222601484, 95.95490777398514]
m = [4.250300733269902 79.28686694436172; 2.0002292577753624 53.85198717246126]
ν = [180.04509222601484, 97.95490777398514]
W = LinearAlgebra.UpperTriangular{Float64,Array{Float64,2}}[[0.18404155547484366 -0.007644049042327276; 0.0 0.0085817051663333], [0.37587636119485557 -0.008953123827346317; 0.0 0.012748664777409421]]
Kind: diag, size256
nx: 100000 sum(zeroth order stats): 99999.99999999997
avll from stats: -0.9980627749383432
avll from llpg:  -0.9980627749383432
avll direct:     -0.9980627749383432
sum posterior: 100000.0
Kind: full, size16
nx: 100000 sum(zeroth order stats): 99999.99999999999
avll from stats: -0.9893148332049884
avll from llpg:  -0.9893148332049886
avll direct:     -0.9893148332049886
sum posterior: 100000.0
32×26 Array{Float64,2}:
  0.0235688   -0.0256469    -0.0455175   -0.0182103   -0.0948557   -0.098791    -0.143551     0.0799085    0.134243     0.102363     0.0309949  -0.0584881    -0.166794    -0.137099     0.0118686    0.0338892    0.0191656   -0.127904     0.0266754   -0.0666716  -0.0244798    -0.0443465    -0.157726     -0.24667       0.0682665   -0.0841335 
 -0.0674088   -0.0185183     0.0118633    0.0956325   -0.048251     0.147345    -0.102389     0.136895     0.00522832   0.00681588  -0.0735841  -0.000241667   0.086444     0.162336    -0.0288725    0.120484     0.179097     0.058144     0.0348909    0.0612976  -0.228321      0.107533      0.000840425   0.0866745     0.066187     0.111726  
 -0.00519648  -0.0213552    -0.0531506   -0.103912    -0.08836     -0.0574778   -0.062651     0.166978    -0.108125     0.0385002    0.0459689  -0.0607134     0.253536     0.0510401   -0.084415    -0.218616     0.0102631    0.243826     0.147823    -0.0838657   0.0941844     0.0314573     0.0979065    -0.181281      0.110992    -0.0405736 
 -0.0454156   -0.111753     -0.147989    -0.135404     0.0745043    0.120307    -0.199454    -0.0673945    0.12837     -0.0939244   -0.0770876  -0.127812      0.0229035   -0.0811894   -0.0417186   -0.0742991   -0.158164     0.0475473    0.0854611    0.0602254   0.0649731    -0.103163      0.208347     -0.0835505    -0.263371    -0.0807607 
  0.113548     0.130325      0.114871    -0.357397    -0.275174     0.0284571   -0.0183119   -0.0629823    0.157196     0.0778921   -0.0160409   0.0573306     0.0935446   -0.0550239    0.0422749   -0.0919301   -0.101671    -0.0797128   -0.159071     0.0655894   0.163747     -0.0262373     0.12988       0.113177     -0.0242597    0.0119892 
  0.0676198    0.119272     -0.129756     0.0993321    0.317584    -0.049056    -0.0312645    0.0291841    0.168895    -0.14038     -0.0293521   0.0692377     0.0542404   -0.0612478    0.0547999    0.102637     0.170404     0.0215168    0.0622559   -0.194198   -0.126137     -0.100629      0.0264697     0.00552855   -0.0342075   -0.0253469 
  0.0601645    0.136355     -0.0190875   -0.0356097    0.111108     0.0272768   -0.037957    -0.0288429    0.145102     0.0360211    0.0361063  -0.136927      0.0204495    0.0102265   -0.203016     0.174762     0.148205    -0.124579     0.0869285    0.0390606   0.212289     -0.0390688     0.0092199    -0.127916     -0.0161119   -0.235731  
 -0.143917    -0.0980103     0.0516229    0.139809    -0.135106     0.0324716   -0.169494    -0.0120378   -0.0302354    0.0541413    0.0157317  -0.00887549    0.00186103  -0.0277043    0.0146024    0.00813514  -0.0541364   -0.0702982    0.158799    -0.0928544   0.0518536    -0.0705185    -0.0540564    -0.155317      0.186439    -0.0998943 
 -0.0030257   -0.0498243     0.078866     0.193968     0.0609747   -0.157073     0.138272     0.165219     0.0858688   -0.186846    -0.0545382  -0.0547361     0.218158    -0.0213654    0.125431     0.0869657   -0.0347289    0.0898184    0.0450279   -0.0669148   0.0880038     0.152722     -0.0231001     0.0461937     0.141126    -0.0299667 
 -0.0533594   -0.0949422    -0.153419    -0.0562844    0.0205664    0.133493     0.0488868    0.00592651  -0.1102       0.0663055   -0.0983591   0.11387      -0.0486384   -0.0431786    0.107965     0.0272521    0.128308    -0.0801825    0.0704297    0.0789917  -0.088458      0.0649549    -0.0694659     0.049305     -0.0671368   -0.0642345 
 -0.140677    -0.115353      0.0347327   -0.044348     0.0381082   -0.0208833   -0.110319    -0.0807306   -0.0645928   -0.206023     0.156566    0.00793638   -0.072339    -0.00877898   0.0840613   -0.0142805    0.137129    -0.095687     0.193016     0.0054352  -0.0537503     0.0873925     0.0299351    -0.100679     -0.265907     0.00291316
  0.00278603   0.0169879    -0.068382    -0.0481968    0.197937     0.0279662    0.0186362   -0.0776152   -0.0332606    0.062405    -0.0592706   0.150327     -0.174018     0.0159949   -0.00895887  -0.0181393    0.060366    -0.0160359   -0.114151     0.0346554  -0.0787205     0.0295839    -0.0750403    -0.233605     -0.0138945   -0.0122107 
  0.0268031   -0.148511     -0.0696949    0.146907    -0.0131184    0.0423407    0.0593839    0.0445105   -0.0304022    0.129964    -0.0126298  -0.0722217     0.0809064   -0.0872924   -0.218593    -0.0333521    0.00328937  -0.148229     0.135547     0.0912321  -0.170084     -0.000874343   0.0414839     0.059884      0.00951917   0.0156423 
 -0.197374    -0.0484647    -0.105798     0.0269741    0.0414173    0.0389226    0.0677375    0.12328     -0.0338085    0.017921    -0.0244316  -0.0616321     0.177673     0.0131861    0.0361841    0.0787898   -0.291532    -0.0168942   -0.00308317   0.175949    0.0694271    -0.0632568     0.113714     -0.056614      0.104545     0.0831736 
  0.297468     0.0152368     0.0500819    0.217325     0.00111856  -0.00100173   0.140696     0.0657785   -0.0667598   -0.0792745    0.0660379  -0.0531906    -0.0879264    0.0481777   -0.198701    -0.090632    -0.071343    -0.0702027   -0.037597     0.0230766  -0.0553907    -0.0194891    -0.162122     -0.00897958   -0.133706     0.123226  
 -0.163797     0.132354      0.104889     0.0615905   -0.0858792   -0.106296     0.102126     0.0572971   -0.142041    -0.167455     0.102071    0.065864     -0.0374452   -0.0248764   -0.0568922    0.160718     0.0228557    0.133564     0.0584707   -0.0132594  -0.0317117    -0.211554     -0.0629677     0.0763162     0.00228906  -0.149049  
  0.0644445    0.000543414  -0.115717     0.0236265    0.0234438   -0.118617     0.0190789   -0.0401376   -0.0674528    0.0721136   -0.0215733  -0.196178     -0.10544     -0.156263    -0.104189     0.0370047    0.179775     0.0491328   -0.0334537    0.0478113  -0.110355     -0.072118      0.158075      0.0837365     0.101593     0.021682  
  0.209787     0.0261887    -0.145045     0.0631446    0.0940614    0.0151831    0.0349279    0.0502925    0.0935513    0.0381804   -0.046172    0.0125254    -0.0606708    0.0214865    0.115246     0.0227317    0.146618     0.0874752   -0.0748534    0.0629238  -0.0222538    -0.0741414    -0.000117126  -0.00877778    0.0102995    0.058481  
  0.0601966   -0.0555634    -0.0171946   -0.154744    -0.10674      0.133665     0.0902761   -0.275602     0.0231253    0.0104158   -0.0184971  -0.0318928     0.0492214   -0.0908302   -0.136201    -0.0877398    0.0405684    0.050082     0.117836     0.0866247  -0.124577      0.188005      0.175633      0.000587316   0.0169115    0.0658204 
  0.10921      0.00269652    0.153659     0.0626202   -0.0599662    0.085386    -0.0141325    0.0417524   -0.182804    -0.0643141    0.025322    0.0536005     0.0372678    0.044126     0.055679    -0.166813    -0.0901528    0.00388394   0.217369     0.0153876   0.06678      -0.0305645    -0.0992469    -0.0860442    -0.055156     0.105276  
 -0.0526294   -0.011716      0.316251    -0.0243948   -0.0714762   -0.0632752   -0.0603593   -0.0838782    0.066909     0.0431288   -0.0619527  -0.0178226     0.034552    -0.0315699   -0.0781419   -0.109289     0.0966375   -0.154609    -0.163382    -0.0175592  -0.0963427     0.0100125     0.128664     -0.0746034     0.0308185    0.0175122 
 -0.0763818   -0.0160177     0.0904689   -0.16025     -0.136633     0.0336373    0.113149    -0.0446195   -0.0248292    0.0423863   -0.0993227   0.0260701     0.0399833    0.0456696   -0.0518036    0.0768352   -0.0260599   -0.080546     0.186506     0.0957797   0.0606159     0.171642     -0.20084      -0.000616772  -0.0554358   -0.122704  
  0.144049     0.141954      0.165145    -0.240156     0.0438724    0.171825     0.0725875    0.00319155   0.00790141   0.0117521   -0.120186   -0.00639608   -0.0820033    0.161382     0.0277084    0.081775    -0.0279941   -0.146392     0.0914683    0.0644174   0.0595228     0.152898     -0.023833     -0.142192      0.0451446    0.0862852 
  0.0434876    0.132766      0.071393    -0.115309     0.0413481   -0.090109     0.0854936    0.122857    -0.0880442   -0.0456718    0.0923882  -0.00731053    0.0547159   -0.0174301    0.102368    -0.187425     0.0226402    0.00220783   0.145532     0.0743587  -0.0163711    -0.043929     -0.0515685     0.0606598     0.0850423    0.0971858 
  0.105798    -0.00758147   -0.205313     0.229169    -0.0859559    0.0735988   -0.0977121   -0.0579871   -0.00758113   0.0431611    0.0869716  -0.0617652    -0.00720582  -0.115361     0.075766    -0.0131288   -0.0956567    0.0449035    0.064394    -0.266618   -0.000878798   0.0935293    -0.0779666     0.0808788    -0.0990994   -0.0736669 
 -0.0305629    0.0349301     0.0540029    0.220038     0.141856     0.0459145    0.169403    -0.0346358    0.120648    -0.124347    -0.0132939  -0.0241563     0.0492153    0.0683827    0.0314707   -0.0479985    0.0762874    0.124955    -0.152533    -0.0255777  -0.117915      0.0733039     0.139691     -0.234074      0.0590569    0.0924595 
  0.0628029   -0.134636     -0.180032     0.00871067   0.0806622    0.0305139    0.0804988   -0.0235221   -0.0152776   -0.079191     0.0765374  -0.042106     -0.110801    -0.240448     0.019451    -0.0344805    0.0180909   -0.113907     0.23226      0.162472    0.0519311    -0.154289      0.0495919    -0.00802884   -0.00600465  -0.171474  
  0.0422512    0.047034     -0.0354454   -0.0879739   -0.0978514    0.0228408   -0.00921579   0.0654797    0.0722727   -0.169577    -0.0853079   0.0116422     0.0431304   -0.0548436   -0.161411    -0.197577    -0.0788801    0.0373479    0.127813    -0.0230627   0.0784198    -0.016924     -0.0308811     0.016656     -0.198543    -0.0777614 
 -0.145316    -0.145237     -0.225369     0.126379    -0.140236     0.0771037   -0.154165     0.0472984   -0.263469    -0.0351767    0.20405     0.16766      -0.123158     0.0302833   -0.253421     0.0291434    0.249906    -0.164336    -0.207127     0.128497   -0.0123792     0.00534475   -0.150977      0.0925073     0.0229139    0.0556164 
  0.0416559    0.143904      0.134439    -0.0631975   -0.101119    -0.0871525    0.00683609  -0.083262     0.044604    -0.1485       0.0682425   0.0153123     0.113401     0.0443352    0.0953611    0.0224807    0.0319184    0.124503    -0.00550009  -0.150574    0.0854277    -0.0149248     0.00471941   -0.0233101    -0.0984939    0.300459  
  0.0160694   -0.109722      0.134656     0.171188     0.150132    -0.0490491    0.039713     0.0964675   -0.0637319    0.0455473   -0.0564982   0.0583498    -0.0837023   -0.0286552    0.0985823   -0.127376    -0.0126356    0.187542    -0.0527463   -0.0667992  -0.0790089    -0.0850311     0.034583      0.174701      0.0553589    0.0482681 
  0.00512931   0.0627274    -0.00993414   0.0577851   -0.0956751    0.0644333   -0.117151    -0.0802863    0.063162     0.132219    -0.0876208  -0.0407659     0.0223948    0.118164     0.0881104   -0.0481737   -0.1005       0.0261556   -0.018139     0.105061   -0.0291672    -0.0350811     0.0580266     0.012304      0.0374738   -0.237456  kind diag, method split
┌ Info: 0: avll = 
└   tll[1] = -1.433980473475034
[ Info: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.434107
[ Info: iteration 2, average log likelihood -1.433958
[ Info: iteration 3, average log likelihood -1.431938
[ Info: iteration 4, average log likelihood -1.416357
[ Info: iteration 5, average log likelihood -1.395353
[ Info: iteration 6, average log likelihood -1.391107
[ Info: iteration 7, average log likelihood -1.390490
[ Info: iteration 8, average log likelihood -1.390268
[ Info: iteration 9, average log likelihood -1.390151
[ Info: iteration 10, average log likelihood -1.390073
[ Info: iteration 11, average log likelihood -1.390012
[ Info: iteration 12, average log likelihood -1.389959
[ Info: iteration 13, average log likelihood -1.389908
[ Info: iteration 14, average log likelihood -1.389857
[ Info: iteration 15, average log likelihood -1.389803
[ Info: iteration 16, average log likelihood -1.389745
[ Info: iteration 17, average log likelihood -1.389683
[ Info: iteration 18, average log likelihood -1.389623
[ Info: iteration 19, average log likelihood -1.389571
[ Info: iteration 20, average log likelihood -1.389531
[ Info: iteration 21, average log likelihood -1.389500
[ Info: iteration 22, average log likelihood -1.389475
[ Info: iteration 23, average log likelihood -1.389455
[ Info: iteration 24, average log likelihood -1.389438
[ Info: iteration 25, average log likelihood -1.389424
[ Info: iteration 26, average log likelihood -1.389411
[ Info: iteration 27, average log likelihood -1.389400
[ Info: iteration 28, average log likelihood -1.389390
[ Info: iteration 29, average log likelihood -1.389382
[ Info: iteration 30, average log likelihood -1.389375
[ Info: iteration 31, average log likelihood -1.389369
[ Info: iteration 32, average log likelihood -1.389364
[ Info: iteration 33, average log likelihood -1.389359
[ Info: iteration 34, average log likelihood -1.389355
[ Info: iteration 35, average log likelihood -1.389352
[ Info: iteration 36, average log likelihood -1.389348
[ Info: iteration 37, average log likelihood -1.389345
[ Info: iteration 38, average log likelihood -1.389342
[ Info: iteration 39, average log likelihood -1.389340
[ Info: iteration 40, average log likelihood -1.389337
[ Info: iteration 41, average log likelihood -1.389334
[ Info: iteration 42, average log likelihood -1.389332
[ Info: iteration 43, average log likelihood -1.389329
[ Info: iteration 44, average log likelihood -1.389327
[ Info: iteration 45, average log likelihood -1.389324
[ Info: iteration 46, average log likelihood -1.389321
[ Info: iteration 47, average log likelihood -1.389317
[ Info: iteration 48, average log likelihood -1.389313
[ Info: iteration 49, average log likelihood -1.389309
[ Info: iteration 50, average log likelihood -1.389304
┌ Info: EM with 100000 data points 50 iterations avll -1.389304
└ 952.4 data points per parameter
┌ Info: 1
│   : avll =  = &quot;: avll = &quot;
│   avll =
│    50-element Array{Float64,1}:
│     -1.4341073023217072
│     -1.4339578408306393
│      ⋮                 
└     -1.389303558561133 
[ Info: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.389497
[ Info: iteration 2, average log likelihood -1.389304
[ Info: iteration 3, average log likelihood -1.388482
[ Info: iteration 4, average log likelihood -1.380074
[ Info: iteration 5, average log likelihood -1.359174
[ Info: iteration 6, average log likelihood -1.347293
[ Info: iteration 7, average log likelihood -1.342355
[ Info: iteration 8, average log likelihood -1.339054
[ Info: iteration 9, average log likelihood -1.337054
[ Info: iteration 10, average log likelihood -1.335903
[ Info: iteration 11, average log likelihood -1.335273
[ Info: iteration 12, average log likelihood -1.334886
[ Info: iteration 13, average log likelihood -1.334612
[ Info: iteration 14, average log likelihood -1.334394
[ Info: iteration 15, average log likelihood -1.334207
[ Info: iteration 16, average log likelihood -1.334029
[ Info: iteration 17, average log likelihood -1.333850
[ Info: iteration 18, average log likelihood -1.333665
[ Info: iteration 19, average log likelihood -1.333471
[ Info: iteration 20, average log likelihood -1.333267
[ Info: iteration 21, average log likelihood -1.333064
[ Info: iteration 22, average log likelihood -1.332890
[ Info: iteration 23, average log likelihood -1.332764
[ Info: iteration 24, average log likelihood -1.332683
[ Info: iteration 25, average log likelihood -1.332631
[ Info: iteration 26, average log likelihood -1.332596
[ Info: iteration 27, average log likelihood -1.332573
[ Info: iteration 28, average log likelihood -1.332557
[ Info: iteration 29, average log likelihood -1.332545
[ Info: iteration 30, average log likelihood -1.332537
[ Info: iteration 31, average log likelihood -1.332530
[ Info: iteration 32, average log likelihood -1.332525
[ Info: iteration 33, average log likelihood -1.332520
[ Info: iteration 34, average log likelihood -1.332516
[ Info: iteration 35, average log likelihood -1.332513
[ Info: iteration 36, average log likelihood -1.332510
[ Info: iteration 37, average log likelihood -1.332508
[ Info: iteration 38, average log likelihood -1.332506
[ Info: iteration 39, average log likelihood -1.332504
[ Info: iteration 40, average log likelihood -1.332502
[ Info: iteration 41, average log likelihood -1.332501
[ Info: iteration 42, average log likelihood -1.332500
[ Info: iteration 43, average log likelihood -1.332499
[ Info: iteration 44, average log likelihood -1.332498
[ Info: iteration 45, average log likelihood -1.332497
[ Info: iteration 46, average log likelihood -1.332497
[ Info: iteration 47, average log likelihood -1.332496
[ Info: iteration 48, average log likelihood -1.332496
[ Info: iteration 49, average log likelihood -1.332496
[ Info: iteration 50, average log likelihood -1.332495
┌ Info: EM with 100000 data points 50 iterations avll -1.332495
└ 473.9 data points per parameter
┌ Info: 2
│   : avll =  = &quot;: avll = &quot;
│   avll =
│    50-element Array{Float64,1}:
│     -1.3894972181685892
│     -1.3893041100853112
│      ⋮                 
└     -1.3324952886075097
[ Info: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.332738
[ Info: iteration 2, average log likelihood -1.332502
[ Info: iteration 3, average log likelihood -1.331617
[ Info: iteration 4, average log likelihood -1.323626
[ Info: iteration 5, average log likelihood -1.303776
[ Info: iteration 6, average log likelihood -1.284787
[ Info: iteration 7, average log likelihood -1.276917
[ Info: iteration 8, average log likelihood -1.274319
[ Info: iteration 9, average log likelihood -1.273321
[ Info: iteration 10, average log likelihood -1.272743
[ Info: iteration 11, average log likelihood -1.272256
[ Info: iteration 12, average log likelihood -1.271729
[ Info: iteration 13, average log likelihood -1.271043
[ Info: iteration 14, average log likelihood -1.270116
[ Info: iteration 15, average log likelihood -1.268818
[ Info: iteration 16, average log likelihood -1.267287
[ Info: iteration 17, average log likelihood -1.266094
[ Info: iteration 18, average log likelihood -1.265454
[ Info: iteration 19, average log likelihood -1.265123
[ Info: iteration 20, average log likelihood -1.264916
[ Info: iteration 21, average log likelihood -1.264765
[ Info: iteration 22, average log likelihood -1.264651
[ Info: iteration 23, average log likelihood -1.264568
[ Info: iteration 24, average log likelihood -1.264508
[ Info: iteration 25, average log likelihood -1.264464
[ Info: iteration 26, average log likelihood -1.264429
[ Info: iteration 27, average log likelihood -1.264400
[ Info: iteration 28, average log likelihood -1.264374
[ Info: iteration 29, average log likelihood -1.264350
[ Info: iteration 30, average log likelihood -1.264326
[ Info: iteration 31, average log likelihood -1.264302
[ Info: iteration 32, average log likelihood -1.264275
[ Info: iteration 33, average log likelihood -1.264245
[ Info: iteration 34, average log likelihood -1.264207
[ Info: iteration 35, average log likelihood -1.264159
[ Info: iteration 36, average log likelihood -1.264095
[ Info: iteration 37, average log likelihood -1.264013
[ Info: iteration 38, average log likelihood -1.263906
[ Info: iteration 39, average log likelihood -1.263772
[ Info: iteration 40, average log likelihood -1.263609
[ Info: iteration 41, average log likelihood -1.263431
[ Info: iteration 42, average log likelihood -1.263260
[ Info: iteration 43, average log likelihood -1.263111
[ Info: iteration 44, average log likelihood -1.262996
[ Info: iteration 45, average log likelihood -1.262910
[ Info: iteration 46, average log likelihood -1.262844
[ Info: iteration 47, average log likelihood -1.262793
[ Info: iteration 48, average log likelihood -1.262752
[ Info: iteration 49, average log likelihood -1.262719
[ Info: iteration 50, average log likelihood -1.262689
┌ Info: EM with 100000 data points 50 iterations avll -1.262689
└ 236.4 data points per parameter
┌ Info: 3
│   : avll =  = &quot;: avll = &quot;
│   avll =
│    50-element Array{Float64,1}:
│     -1.3327381445520021
│     -1.332501945878243 
│      ⋮                 
└     -1.2626885403760018
[ Info: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.262924
[ Info: iteration 2, average log likelihood -1.262614
[ Info: iteration 3, average log likelihood -1.261584
[ Info: iteration 4, average log likelihood -1.250056
[ Info: iteration 5, average log likelihood -1.215543
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     4
│     6
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 6, average log likelihood -1.187672
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      2
│     16
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 7, average log likelihood -1.185260
[ Info: iteration 8, average log likelihood -1.185583
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      4
│      6
│     15
│     16
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 9, average log likelihood -1.164356
[ Info: iteration 10, average log likelihood -1.183194
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      2
│     16
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 11, average log likelihood -1.165738
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     4
│     6
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 12, average log likelihood -1.171314
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     15
│     16
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 13, average log likelihood -1.170075
[ Info: iteration 14, average log likelihood -1.165641
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      2
│      4
│      6
│      8
│     16
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 15, average log likelihood -1.153535
[ Info: iteration 16, average log likelihood -1.189584
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     15
│     16
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 17, average log likelihood -1.161951
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     2
│     6
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 18, average log likelihood -1.162863
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      4
│     16
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 19, average log likelihood -1.177100
[ Info: iteration 20, average log likelihood -1.167375
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      6
│      8
│     15
│     16
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 21, average log likelihood -1.149114
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     2
│     4
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 22, average log likelihood -1.174525
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     16
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 23, average log likelihood -1.181900
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     6
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 24, average log likelihood -1.162625
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     15
│     16
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 25, average log likelihood -1.158125
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     2
│     4
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 26, average log likelihood -1.159406
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      6
│      8
│     16
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 27, average log likelihood -1.174084
[ Info: iteration 28, average log likelihood -1.176960
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     15
│     16
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 29, average log likelihood -1.155534
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     2
│     4
│     6
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 30, average log likelihood -1.159032
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     16
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 31, average log likelihood -1.184559
[ Info: iteration 32, average log likelihood -1.163871
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      6
│      8
│     15
│     16
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 33, average log likelihood -1.148606
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     2
│     4
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 34, average log likelihood -1.173460
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     16
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 35, average log likelihood -1.181959
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     6
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 36, average log likelihood -1.162681
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     15
│     16
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 37, average log likelihood -1.158297
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     2
│     4
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 38, average log likelihood -1.159613
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      6
│      8
│     16
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 39, average log likelihood -1.174190
[ Info: iteration 40, average log likelihood -1.176959
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     15
│     16
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 41, average log likelihood -1.155630
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     2
│     4
│     6
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 42, average log likelihood -1.159172
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     16
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 43, average log likelihood -1.184598
[ Info: iteration 44, average log likelihood -1.163937
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      6
│      8
│     15
│     16
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 45, average log likelihood -1.148737
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     2
│     4
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 46, average log likelihood -1.173522
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     16
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 47, average log likelihood -1.181965
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     6
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 48, average log likelihood -1.162703
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     15
│     16
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 49, average log likelihood -1.158366
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     2
│     4
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 50, average log likelihood -1.159712
┌ Info: EM with 100000 data points 50 iterations avll -1.159712
└ 118.1 data points per parameter
┌ Info: 4
│   : avll =  = &quot;: avll = &quot;
│   avll =
│    50-element Array{Float64,1}:
│     -1.2629240104347859
│     -1.2626144300483693
│      ⋮                 
└     -1.1597115404803096
[ Info: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│     11
│     12
│     15
│     16
│     31
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 1, average log likelihood -1.174591
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│     11
│     12
│     15
│     16
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 2, average log likelihood -1.157288
┌ Warning: Variances had to be floored 
│   ind =
│    10-element Array{Int64,1}:
│      8
│     11
│     12
│     15
│      ⋮
│     30
│     31
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 3, average log likelihood -1.145710
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│      3
│      4
│     11
│     12
│      ⋮
│     21
│     31
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 4, average log likelihood -1.138875
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│      7
│     11
│     12
│     15
│      ⋮
│     24
│     31
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 5, average log likelihood -1.110732
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│     11
│     12
│     15
│     16
│      ⋮
│     30
│     31
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 6, average log likelihood -1.090703
┌ Warning: Variances had to be floored 
│   ind =
│    13-element Array{Int64,1}:
│      3
│      4
│      7
│     11
│      ⋮
│     27
│     31
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 7, average log likelihood -1.067950
┌ Warning: Variances had to be floored 
│   ind =
│    10-element Array{Int64,1}:
│      8
│     11
│     12
│     15
│      ⋮
│     30
│     31
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 8, average log likelihood -1.100945
┌ Warning: Variances had to be floored 
│   ind =
│    11-element Array{Int64,1}:
│      3
│      4
│      7
│     11
│      ⋮
│     24
│     31
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 9, average log likelihood -1.087899
┌ Warning: Variances had to be floored 
│   ind =
│    10-element Array{Int64,1}:
│     11
│     12
│     15
│     16
│      ⋮
│     30
│     31
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 10, average log likelihood -1.095855
┌ Warning: Variances had to be floored 
│   ind =
│    12-element Array{Int64,1}:
│      4
│      7
│      8
│     11
│      ⋮
│     27
│     31
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 11, average log likelihood -1.075629
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      3
│     11
│     12
│     15
│     16
│     21
│     31
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 12, average log likelihood -1.097237
┌ Warning: Variances had to be floored 
│   ind =
│    12-element Array{Int64,1}:
│      7
│     11
│     12
│     15
│      ⋮
│     30
│     31
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 13, average log likelihood -1.085357
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      4
│     11
│     12
│     15
│     16
│     21
│     31
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 14, average log likelihood -1.090584
┌ Warning: Variances had to be floored 
│   ind =
│    13-element Array{Int64,1}:
│      3
│      7
│      8
│     11
│      ⋮
│     30
│     31
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 15, average log likelihood -1.060434
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│     11
│     12
│     15
│     16
│      ⋮
│     29
│     31
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 16, average log likelihood -1.103781
┌ Warning: Variances had to be floored 
│   ind =
│    10-element Array{Int64,1}:
│      4
│      7
│     11
│     12
│      ⋮
│     24
│     31
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 17, average log likelihood -1.085288
┌ Warning: Variances had to be floored 
│   ind =
│    10-element Array{Int64,1}:
│      3
│      8
│     11
│     12
│      ⋮
│     29
│     31
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 18, average log likelihood -1.081071
┌ Warning: Variances had to be floored 
│   ind =
│    12-element Array{Int64,1}:
│      4
│      7
│     11
│     12
│      ⋮
│     27
│     31
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 19, average log likelihood -1.083586
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│     11
│     12
│     15
│     16
│     21
│     29
│     31
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 20, average log likelihood -1.099432
┌ Warning: Variances had to be floored 
│   ind =
│    13-element Array{Int64,1}:
│      3
│      4
│      7
│      8
│      ⋮
│     28
│     31
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 21, average log likelihood -1.056326
┌ Warning: Variances had to be floored 
│   ind =
│    10-element Array{Int64,1}:
│     11
│     12
│     15
│     16
│      ⋮
│     29
│     31
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 22, average log likelihood -1.095443
┌ Warning: Variances had to be floored 
│   ind =
│    11-element Array{Int64,1}:
│      3
│      4
│      7
│     11
│      ⋮
│     24
│     31
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 23, average log likelihood -1.083765
┌ Warning: Variances had to be floored 
│   ind =
│    10-element Array{Int64,1}:
│      8
│     11
│     12
│     15
│      ⋮
│     29
│     31
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 24, average log likelihood -1.082753
┌ Warning: Variances had to be floored 
│   ind =
│    13-element Array{Int64,1}:
│      3
│      4
│      7
│     11
│      ⋮
│     27
│     31
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 25, average log likelihood -1.076382
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│     11
│     12
│     15
│     16
│     21
│     29
│     31
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 26, average log likelihood -1.099883
┌ Warning: Variances had to be floored 
│   ind =
│    13-element Array{Int64,1}:
│      3
│      4
│      7
│      8
│      ⋮
│     28
│     31
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 27, average log likelihood -1.062618
┌ Warning: Variances had to be floored 
│   ind =
│    10-element Array{Int64,1}:
│     11
│     12
│     15
│     16
│      ⋮
│     29
│     31
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 28, average log likelihood -1.095424
┌ Warning: Variances had to be floored 
│   ind =
│    11-element Array{Int64,1}:
│      3
│      4
│      7
│     11
│      ⋮
│     24
│     31
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 29, average log likelihood -1.083546
┌ Warning: Variances had to be floored 
│   ind =
│    10-element Array{Int64,1}:
│      8
│     11
│     12
│     15
│      ⋮
│     29
│     31
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 30, average log likelihood -1.082672
┌ Warning: Variances had to be floored 
│   ind =
│    13-element Array{Int64,1}:
│      3
│      4
│      7
│     11
│      ⋮
│     27
│     31
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 31, average log likelihood -1.076219
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│     11
│     12
│     15
│     16
│     21
│     29
│     31
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 32, average log likelihood -1.099820
┌ Warning: Variances had to be floored 
│   ind =
│    13-element Array{Int64,1}:
│      3
│      4
│      7
│      8
│      ⋮
│     28
│     31
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 33, average log likelihood -1.062474
┌ Warning: Variances had to be floored 
│   ind =
│    10-element Array{Int64,1}:
│     11
│     12
│     15
│     16
│      ⋮
│     29
│     31
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 34, average log likelihood -1.095374
┌ Warning: Variances had to be floored 
│   ind =
│    11-element Array{Int64,1}:
│      3
│      4
│      7
│     11
│      ⋮
│     24
│     31
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 35, average log likelihood -1.083447
┌ Warning: Variances had to be floored 
│   ind =
│    10-element Array{Int64,1}:
│      8
│     11
│     12
│     15
│      ⋮
│     29
│     31
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 36, average log likelihood -1.082624
┌ Warning: Variances had to be floored 
│   ind =
│    13-element Array{Int64,1}:
│      3
│      4
│      7
│     11
│      ⋮
│     27
│     31
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 37, average log likelihood -1.076123
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│     11
│     12
│     15
│     16
│     21
│     29
│     31
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 38, average log likelihood -1.099792
┌ Warning: Variances had to be floored 
│   ind =
│    13-element Array{Int64,1}:
│      3
│      4
│      7
│      8
│      ⋮
│     28
│     31
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 39, average log likelihood -1.062373
┌ Warning: Variances had to be floored 
│   ind =
│    10-element Array{Int64,1}:
│     11
│     12
│     15
│     16
│      ⋮
│     29
│     31
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 40, average log likelihood -1.095355
┌ Warning: Variances had to be floored 
│   ind =
│    11-element Array{Int64,1}:
│      3
│      4
│      7
│     11
│      ⋮
│     24
│     31
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 41, average log likelihood -1.083368
┌ Warning: Variances had to be floored 
│   ind =
│    10-element Array{Int64,1}:
│      8
│     11
│     12
│     15
│      ⋮
│     29
│     31
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 42, average log likelihood -1.082597
┌ Warning: Variances had to be floored 
│   ind =
│    13-element Array{Int64,1}:
│      3
│      4
│      7
│     11
│      ⋮
│     27
│     31
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 43, average log likelihood -1.076052
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│     11
│     12
│     15
│     16
│     21
│     29
│     31
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 44, average log likelihood -1.099774
┌ Warning: Variances had to be floored 
│   ind =
│    13-element Array{Int64,1}:
│      3
│      4
│      7
│      8
│      ⋮
│     28
│     31
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 45, average log likelihood -1.062302
┌ Warning: Variances had to be floored 
│   ind =
│    10-element Array{Int64,1}:
│     11
│     12
│     15
│     16
│      ⋮
│     29
│     31
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 46, average log likelihood -1.095346
┌ Warning: Variances had to be floored 
│   ind =
│    11-element Array{Int64,1}:
│      3
│      4
│      7
│     11
│      ⋮
│     24
│     31
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 47, average log likelihood -1.083315
┌ Warning: Variances had to be floored 
│   ind =
│    10-element Array{Int64,1}:
│      8
│     11
│     12
│     15
│      ⋮
│     29
│     31
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 48, average log likelihood -1.082580
┌ Warning: Variances had to be floored 
│   ind =
│    13-element Array{Int64,1}:
│      3
│      4
│      7
│     11
│      ⋮
│     27
│     31
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 49, average log likelihood -1.076007
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│     11
│     12
│     15
│     16
│     21
│     29
│     31
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 50, average log likelihood -1.099763
┌ Info: EM with 100000 data points 50 iterations avll -1.099763
└ 59.0 data points per parameter
┌ Info: 5
│   : avll =  = &quot;: avll = &quot;
│   avll =
│    50-element Array{Float64,1}:
│     -1.1745914769662626
│     -1.157288395403652 
│      ⋮                 
└     -1.099762523666434 
32×26 Array{┌ Info: Total log likelihood: 
│   tll =
│    251-element Array{Float64,1}:
│     -1.433980473475034 
│     -1.4341073023217072
│     -1.4339578408306393
│     -1.4319380829521406
│      ⋮                 
│     -1.082579729554271 
│     -1.0760070467954277
└     -1.099762523666434 
Float64,2}:
 -0.146007    -0.149555    -0.642534      0.123412   -0.136432      0.0138441    -0.156766     0.0686639   -0.385788     -0.0809683    0.241247     0.1427       0.0476217    0.0585427   -0.12826     -0.00614133   0.263592    -0.244203     -0.243132    -0.140426     0.0500076   0.0656827    -0.148023     0.0912805    0.0226614   -0.0139408 
 -0.155721    -0.100075     0.265259      0.15144    -0.139401      0.130958     -0.185186     0.0670062   -0.209961     -0.00194      0.189936     0.18881     -0.40487      0.0738013   -0.391717     0.0566477    0.245757    -0.08693      -0.168885     0.355343    -0.109809    0.0205773    -0.150488     0.0823555    0.0232897    0.0859767 
 -0.0440894   -0.152905    -0.00995549   -0.0366566   0.00990193    0.110923     -0.00172582  -0.0255672   -0.0375872     0.0513918   -0.0909566    0.0979207    0.00833664  -0.0658779    0.0645547   -0.0379584    0.231817    -0.0862889     0.00227318   0.0464986   -0.102545    0.0421705    -0.753159    -0.00229888  -0.135659     0.00718471
 -0.0538697   -0.0560098    0.149457     -0.0228532  -0.0524368    -0.0500832    -0.0210887   -0.0295029    0.0138131     0.0460774   -0.0775922   -0.0199934    0.0182357   -0.028707    -0.0267109   -0.0590461    0.0965299   -0.11641      -0.122821     0.024416    -0.0765922   0.051826      0.527266    -0.0223965    0.0898998   -0.0054971 
 -0.0358538   -0.133778    -0.11779      -0.135751    0.105176      0.0929975    -0.205119    -0.0550191    0.102255     -0.0959857   -0.0607124   -0.167472     0.0084699   -0.0829406   -0.0860208   -0.342906    -0.178695     0.0316632     0.0629802    0.0286261    0.0680199  -0.102765      0.288238    -0.0413645   -0.286616    -0.0775695 
 -0.0555231   -0.110764    -0.16285      -0.141377    0.0463509     0.265491     -0.228415    -0.150958     0.282495     -0.0480754   -0.0754925   -0.0946166   -0.00693246  -0.0723424   -0.0301027    0.299204    -0.183819     0.057819      0.0352566    0.0163444    0.0657047  -0.100502      0.142306    -0.126225    -0.187763    -0.0811218 
 -0.0655192   -0.0252371    0.0199986     0.0893835  -0.0440765     0.135119     -0.0910678    0.133649    -0.0451623     0.00871878  -0.0645353   -0.00128205   0.0711957    0.139087    -0.0267379    0.112807     0.169225     0.061712      0.0450959    0.0523581   -0.198469    0.10387      -0.00132426   0.0835021    0.0631134    0.107618  
  0.205959     0.0223954   -0.144409      0.0630922   0.0797626     0.0213952     0.0317734    0.0452527    0.0849102     0.0376934   -0.0577045   -0.0104316   -0.0554417    0.035812     0.121136     0.0374726    0.155769     0.110939     -0.0851132    0.0593644   -0.0295447  -0.0820537    -0.00498931   0.00275718   0.0788684    0.0300161 
  0.0302339   -0.108008    -0.0481646    -0.10792    -0.10801      -0.0530212    -0.0654012    0.187628    -0.101209     -0.112489     0.0596267    0.00105104   0.248315     0.0449043   -0.618292    -0.203202     0.0196627    0.209455      0.163027    -0.0879594    0.0949554  -0.0632411     0.0760977   -0.186273     0.110657    -0.0114378 
 -0.0110296    0.0648239   -0.0603466    -0.0957059  -0.0983643    -0.071198     -0.0566331    0.146921    -0.111404      0.141107     0.0353483   -0.149965     0.261208     0.0752955    0.571585    -0.252779    -0.0407886    0.274726      0.140866    -0.0805127    0.094916    0.175648      0.0930983   -0.144881     0.110122    -0.0827996 
 -0.284265     0.0721538   -0.102912     -0.0321259  -0.332279     -0.1187        0.00575674  -0.0325503   -0.0718843     0.16603     -0.022012    -0.215459    -0.107741    -0.189636    -0.253537    -0.0236768    0.261743     0.0474811    -0.0378102   -0.0110611   -0.0797231  -0.199991      0.00107855   0.056391     0.133878     0.0241649 
  0.389535    -0.126844    -0.105589      0.0740442   0.408401     -0.118897      0.0363968   -0.0342698   -0.0598145    -0.0365649   -0.0246637   -0.173927    -0.104237    -0.130697     0.0441153    0.108748     0.143008     0.04672      -0.0192822    0.18176     -0.135606    0.0131848     0.342365     0.0948643    0.0868247    0.0229918 
 -0.108416     0.012273     0.0895644     0.13176    -0.0235811    -0.0909777     0.00682467   0.0807918   -0.0354926    -0.0959042    0.00173949   0.0210635    0.0524301   -0.0229754    0.0216502    0.100666    -0.0249877    0.0602081     0.0895833   -0.0972587    0.0280795  -0.0704016    -0.0674108   -0.0332303    0.11101     -0.0987034 
 -0.0502013    0.0258743    0.0307567    -0.0900896   0.029553     -0.00700558   -0.0108943    0.0356306   -0.078186     -0.130219     0.107181     0.0252741   -0.00689275  -0.020638     0.0932753   -0.0982484    0.0885171   -0.0534792     0.169197     0.0394467   -0.0324791   0.0296072     0.0201027   -0.00114156  -0.0807775    0.0382957 
  0.045089    -0.272959    -0.0354034    -0.0745444  -0.151036      0.105528     -0.16274      0.0219549    0.143753     -0.124078    -0.0876226    0.217601    -0.212139    -0.0464669   -0.202046    -0.206316    -0.0607336    0.0127499     0.125623    -0.0568173    0.256484   -0.212292     -0.142288     0.0320857   -0.317549    -0.0830481 
  0.0315295    0.5205      -0.0356763    -0.130676    0.0992771    -0.0967018     0.140744     0.0756017    0.00982266   -0.313458    -0.0843229   -0.212523     0.305339    -0.054345    -0.153667    -0.187382    -0.14532      0.0367319     0.132493     0.0173749   -0.124286    0.360411      0.0570583   -0.0635614   -0.0921869   -0.0680742 
 -0.0177571    0.0465433    0.113781     -0.11345    -0.118474     -0.0122206     0.0677599   -0.062334     0.0293311    -0.0531401   -0.0226203    0.00767706   0.0781498    0.0469144    0.0242491    0.0415935    0.00421765   0.0230338     0.0868383   -0.0332837    0.0979686   0.0820985    -0.0926569   -0.0091537   -0.0657083    0.0850337 
  0.0576069    0.0210456   -0.101381     -0.0340322   0.194204      0.0108601     0.0234547   -0.0826531   -0.0480352     0.0645135   -0.0563659    0.159778    -0.172938     0.020094    -0.00872756  -0.0129285    0.0726666   -0.026763     -0.10101      0.0427496   -0.0566332   0.0164048    -0.0793291   -0.234238    -0.0146826   -0.00976822
  0.026484    -0.00648738  -0.0559388    -0.0211232  -0.0799189    -0.0808919    -0.1353       0.0757523    0.122842      0.134581     0.0328096   -0.0240105   -0.155206    -0.113312     0.0180802    0.0343792    0.0100851   -0.135129      0.0172806   -0.0671893   -0.0183526  -0.0727133    -0.107857    -0.209355     0.0682993   -0.113413  
  0.0232708   -0.0339118   -0.102693      0.0453078  -0.015159      0.0574162    -0.0020552   -0.057597     0.0132267    -0.005324     0.0286123   -0.00919007  -0.0399694   -0.0497435    0.056842    -0.0740559   -0.0412618   -0.025032      0.114859     0.149311     0.0298565  -0.0736685     0.00327994   0.0179666    0.00412964  -0.160702  
 -0.0362329   -0.153021    -0.0666482     0.139831   -0.0346827     0.0647211     0.0996371    0.0301202   -0.0313323     0.127153    -0.00136975  -0.0778866    0.0701438   -0.0939914   -0.253087    -0.0444312    0.0039363   -0.194306      0.201206     0.091478    -0.175242   -0.000648255   0.0256623    0.0659133    0.0143824    0.0177483 
  0.111109     0.135003     0.116309     -0.36112    -0.280637      0.000254009   0.0162327   -0.0329592    0.146177      0.0765556   -0.00438996   0.0496267    0.119203    -0.0358536    0.0180141   -0.130544    -0.0981936   -0.103082     -0.162991     0.0654138    0.163346   -0.0341106     0.120171     0.112576    -0.0315534    0.0379736 
  0.00884066   0.0970523    0.0295264     0.0869728   0.128421      0.0375493     0.0573476   -0.042644     0.164051     -0.0442492    0.0019724   -0.07558      0.0357399    0.0387403   -0.0772555    0.0860695    0.116388    -0.00381373   -0.024718     0.00554516   0.0614482   0.0244905     0.0737358   -0.204782     0.0168461   -0.0805656 
  0.0930184   -0.0431951   -0.027249     -0.144008   -0.0902198     0.16829       0.10235     -0.261828    -0.000138448   0.0081593   -0.0426156   -0.0301353    0.00201014  -0.0809046   -0.141269    -0.0758162    0.0571818    0.0484902     0.136362     0.0804613   -0.0751876   0.161291      0.16513     -0.00361038   0.00560329   0.0845351 
  0.0732881    0.113818    -0.120747      0.0669289   0.28233      -0.0565603    -0.0108583    0.0312207    0.155778     -0.154715    -0.0310024    0.0689166    0.0892487   -0.0218579    0.0459962    0.107256     0.166437     0.0233126     0.0285068   -0.192576    -0.127145   -0.0940391     0.0399242   -0.0147534   -0.0327267   -0.023532  
 -0.18253     -0.0518124   -0.107241      0.018354    0.0946367    -0.0223211     0.06977      0.121633    -0.0325754     0.030615    -0.00590084  -0.0644699    0.176442     0.00524881   0.0355186    0.0658685   -0.293162    -0.000307288   0.0130399    0.1509       0.0686624  -0.0552443     0.144406    -0.0669057    0.112069     0.0452521 
  0.00598568  -0.110448     0.128387      0.154932    0.148842     -0.0238776     0.0776536    0.103179    -0.0533113     0.0663405   -0.0649809    0.0896228   -0.0542611   -0.0358967    0.0916082   -0.126262    -0.00678767   0.179803     -0.0812818   -0.095413    -0.0810348  -0.107098      0.0137741    0.14632      0.0418293    0.0203966 
  0.321353     0.0143918    0.0382948     0.234782   -0.000430487  -0.00137718    0.116763     0.0654784   -0.0592185    -0.0667913    0.0627247   -0.063584    -0.0785063    0.0330859   -0.189203    -0.0780327   -0.0631932   -0.0685755    -0.0352127    0.0414026   -0.0576595   0.0136638    -0.159589     0.0109601   -0.151216     0.120365  
  0.144141     0.141676     0.18034      -0.239009    0.0447263     0.173406      0.0665634   -0.00189872  -0.0012424     0.0107565   -0.120569     0.00269841  -0.0865486    0.156796     0.0281391    0.0816195   -0.0264024   -0.159944      0.120696     0.0765316    0.0572838   0.111884     -0.0244311   -0.146607     0.00286896   0.083461  
  0.103539    -0.00142196   0.144655      0.0602802  -0.0652223     0.053108     -0.0103263    0.0427749   -0.179177     -0.0649792    0.0295491    0.0500804    0.0701694    0.0146937    0.0493373   -0.159694    -0.0869368   -0.00783626    0.212023     0.00420531   0.0311331  -0.0255795    -0.0966788   -0.0959944   -0.0508407    0.125511  
  0.115677     0.162801    -0.458405      1.04664    -0.2709        0.541534     -0.0297044   -0.0589178   -0.0915892     0.0372996    0.0889667   -0.111862    -0.00199187  -0.131336     0.0765419   -0.0559315   -0.0931017    0.0480888     0.0585389   -0.246199    -0.103939    0.0785305    -0.0890455    0.0701614   -0.473289    -0.0215425 
  0.134924    -0.161496    -0.000611225  -0.698869    0.00925753   -0.160532     -0.0616268   -0.0460543    0.126468      0.0393603    0.0659167   -0.0169434   -0.0294382   -0.092147     0.0761919    0.0337948   -0.092601     0.0597685     0.0857356   -0.278022     0.0951459   0.156963     -0.0753724    0.0610505    0.322914    -0.0963359 [ Info: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
┌ Warning: Variances had to be floored 
│   ind =
│    13-element Array{Int64,1}:
│      3
│      4
│      7
│      8
│      ⋮
│     28
│     31
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 1, average log likelihood -1.062258
┌ Warning: Variances had to be floored 
│   ind =
│    15-element Array{Int64,1}:
│      4
│      7
│      8
│     11
│      ⋮
│     29
│     31
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 2, average log likelihood -1.050817
┌ Warning: Variances had to be floored 
│   ind =
│    13-element Array{Int64,1}:
│      3
│      4
│      7
│      8
│      ⋮
│     28
│     31
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 3, average log likelihood -1.056621
┌ Warning: Variances had to be floored 
│   ind =
│    15-element Array{Int64,1}:
│      4
│      7
│      8
│     11
│      ⋮
│     29
│     31
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 4, average log likelihood -1.050250
┌ Warning: Variances had to be floored 
│   ind =
│    13-element Array{Int64,1}:
│      3
│      4
│      7
│      8
│      ⋮
│     28
│     31
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 5, average log likelihood -1.056415
┌ Warning: Variances had to be floored 
│   ind =
│    15-element Array{Int64,1}:
│      4
│      7
│      8
│     11
│      ⋮
│     29
│     31
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 6, average log likelihood -1.050204
┌ Warning: Variances had to be floored 
│   ind =
│    13-element Array{Int64,1}:
│      3
│      4
│      7
│      8
│      ⋮
│     28
│     31
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 7, average log likelihood -1.056513
┌ Warning: Variances had to be floored 
│   ind =
│    15-element Array{Int64,1}:
│      4
│      7
│      8
│     11
│      ⋮
│     29
│     31
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 8, average log likelihood -1.050197
┌ Warning: Variances had to be floored 
│   ind =
│    13-element Array{Int64,1}:
│      3
│      4
│      7
│      8
│      ⋮
│     28
│     31
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 9, average log likelihood -1.056548
┌ Warning: Variances had to be floored 
│   ind =
│    15-element Array{Int64,1}:
│      4
│      7
│      8
│     11
│      ⋮
│     29
│     31
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 10, average log likelihood -1.050194
┌ Info: EM with 100000 data points 10 iterations avll -1.050194
└ 59.0 data points per parameter
kind diag, method kmeans
[ Info: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       8.975392e+05
      1       7.034839e+05      -1.940553e+05 |       32
      2       6.735472e+05      -2.993669e+04 |       32
      3       6.602513e+05      -1.329582e+04 |       32
      4       6.512000e+05      -9.051315e+03 |       32
      5       6.433316e+05      -7.868436e+03 |       32
      6       6.376729e+05      -5.658741e+03 |       32
      7       6.344293e+05      -3.243607e+03 |       32
      8       6.327918e+05      -1.637433e+03 |       32
      9       6.319575e+05      -8.343664e+02 |       32
     10       6.314933e+05      -4.641976e+02 |       32
     11       6.311742e+05      -3.190374e+02 |       32
     12       6.309692e+05      -2.050328e+02 |       32
     13       6.308143e+05      -1.548832e+02 |       32
     14       6.306848e+05      -1.295353e+02 |       32
     15       6.305934e+05      -9.140634e+01 |       32
     16       6.305275e+05      -6.583573e+01 |       32
     17       6.304633e+05      -6.419845e+01 |       31
     18       6.303936e+05      -6.970448e+01 |       32
     19       6.303153e+05      -7.834204e+01 |       32
     20       6.302199e+05      -9.535391e+01 |       32
     21       6.300832e+05      -1.366858e+02 |       32
     22       6.298523e+05      -2.309276e+02 |       32
     23       6.294489e+05      -4.034299e+02 |       32
     24       6.289855e+05      -4.633846e+02 |       32
     25       6.285575e+05      -4.280058e+02 |       32
     26       6.283070e+05      -2.505220e+02 |       32
     27       6.281322e+05      -1.747332e+02 |       32
     28       6.279793e+05      -1.529464e+02 |       32
     29       6.278092e+05      -1.700946e+02 |       32
     30       6.276275e+05      -1.816718e+02 |       32
     31       6.273811e+05      -2.464212e+02 |       32
     32       6.271107e+05      -2.703754e+02 |       32
     33       6.269585e+05      -1.522433e+02 |       32
     34       6.268841e+05      -7.436788e+01 |       32
     35       6.268438e+05      -4.035631e+01 |       32
     36       6.268219e+05      -2.182914e+01 |       32
     37       6.268050e+05      -1.692412e+01 |       30
     38       6.267912e+05      -1.384240e+01 |       28
     39       6.267810e+05      -1.018907e+01 |       28
     40       6.267732e+05      -7.781864e+00 |       29
     41       6.267659e+05      -7.255905e+00 |       28
     42       6.267597e+05      -6.201088e+00 |       29
     43       6.267551e+05      -4.628978e+00 |       23
     44       6.267521e+05      -3.045912e+00 |       26
     45       6.267494e+05      -2.640610e+00 |       25
     46       6.267463e+05      -3.102380e+00 |       22
     47       6.267444e+05      -1.897604e+00 |       21
     48       6.267424e+05      -2.055472e+00 |       20
     49       6.267410e+05      -1.372299e+00 |       16
     50       6.267389e+05      -2.070184e+00 |       22
K-means terminated without convergence after 50 iterations (objv = 626738.9206432493)
┌ Info: K-means with 32000 data points using 50 iterations
└ 37.0 data points per parameter
[ Info: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.322140
[ Info: iteration 2, average log likelihood -1.289841
[ Info: iteration 3, average log likelihood -1.260418
[ Info: iteration 4, average log likelihood -1.226574
[ Info: iteration 5, average log likelihood -1.184613
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      7
│     21
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 6, average log likelihood -1.123656
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      2
│     13
│     15
│     27
│     28
│     30
│     31
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 7, average log likelihood -1.075870
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      1
│      6
│     16
│     19
│     26
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 8, average log likelihood -1.089338
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      7
│     10
│     18
│     21
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 9, average log likelihood -1.104700
[ Info: iteration 10, average log likelihood -1.098306
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      5
│      6
│      8
│     16
│     26
│     27
│     28
│     31
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 11, average log likelihood -1.012183
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      2
│     13
│     15
│     19
│     21
│     30
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 12, average log likelihood -1.078898
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      1
│     10
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 13, average log likelihood -1.109706
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     18
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 14, average log likelihood -1.083971
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      6
│     26
│     28
│     31
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 15, average log likelihood -1.029853
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      2
│      5
│      8
│     13
│     15
│     16
│     21
│     27
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 16, average log likelihood -1.029135
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      1
│     10
│     19
│     30
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 17, average log likelihood -1.091645
[ Info: iteration 18, average log likelihood -1.096803
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      6
│     18
│     26
│     28
│     31
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 19, average log likelihood -1.040160
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      5
│      8
│     21
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 20, average log likelihood -1.055225
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│      1
│      2
│     10
│     13
│      ⋮
│     19
│     27
│     30
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 21, average log likelihood -1.035473
[ Info: iteration 22, average log likelihood -1.108969
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      6
│     26
│     28
│     31
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 23, average log likelihood -1.057545
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      8
│     21
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 24, average log likelihood -1.078162
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      2
│      5
│     18
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 25, average log likelihood -1.044736
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      1
│     10
│     15
│     19
│     27
│     30
│     31
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 26, average log likelihood -1.020469
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      6
│     13
│     16
│     26
│     28
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 27, average log likelihood -1.063500
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      8
│     21
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 28, average log likelihood -1.096574
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      5
│     31
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 29, average log likelihood -1.066979
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      1
│      2
│      6
│     18
│     26
│     27
│     30
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 30, average log likelihood -1.014910
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│     10
│     13
│     15
│     16
│     19
│     21
│     28
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 31, average log likelihood -1.053764
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      5
│      8
│     31
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 32, average log likelihood -1.109345
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     6
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 33, average log likelihood -1.098489
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     26
│     28
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 34, average log likelihood -1.046861
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│      1
│      2
│     10
│     13
│      ⋮
│     27
│     30
│     31
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 35, average log likelihood -1.002417
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      6
│      8
│     16
│     19
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 36, average log likelihood -1.084853
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      5
│     18
│     26
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 37, average log likelihood -1.097850
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     28
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 38, average log likelihood -1.083001
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      2
│      6
│      8
│     21
│     27
│     30
│     31
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 39, average log likelihood -1.016122
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      1
│      5
│     10
│     13
│     15
│     16
│     19
│     26
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 40, average log likelihood -1.049870
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     28
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 41, average log likelihood -1.117705
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      6
│     18
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 42, average log likelihood -1.078699
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      5
│      8
│     21
│     26
│     27
│     31
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 43, average log likelihood -1.026549
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      1
│      2
│     10
│     15
│     19
│     28
│     30
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 44, average log likelihood -1.051407
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     13
│     16
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 45, average log likelihood -1.108616
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     6
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 46, average log likelihood -1.078615
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      8
│     18
│     21
│     26
│     27
│     28
│     31
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 47, average log likelihood -1.028241
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      1
│      2
│      5
│     10
│     15
│     25
│     30
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 48, average log likelihood -1.063956
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      6
│     13
│     16
│     19
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 49, average log likelihood -1.095450
[ Info: iteration 50, average log likelihood -1.101424
┌ Info: EM with 100000 data points 50 iterations avll -1.101424
└ 59.0 data points per parameter
32×26 Array{Float64,2}:
 -0.0137614    0.14087      0.123819    -0.0564324   -0.0713437   -0.0583592     0.0302435   -0.0923553   0.0601354   -0.192024     0.137023     0.0112548    0.105956      0.0574565    0.077467     0.0142211    0.0325793    0.0949509    -0.00412744  -0.118957     0.151589   -0.0229309   -0.00766081  -0.0296606   -0.0891216    0.287907  
 -0.0330367    0.0528995    0.0456332    0.212081     0.113238     0.0477736     0.178167    -0.065703    0.160366    -0.121295    -0.00994206  -0.0313006    0.0302837     0.0402242    0.0294043   -0.035091     0.06767      0.0952553    -0.111738    -0.012179    -0.107906    0.119337     0.173286    -0.192102     0.0577398    0.0786713 
  0.00587699  -0.0565685    0.0674488    0.188576     0.0740559   -0.148592      0.119576     0.162828    0.0821684   -0.18772     -0.0778538   -0.0669409    0.20738      -0.0221973    0.123703     0.100484    -0.0347266    0.0975954     0.0567487   -0.178249     0.112804    0.142727    -0.00979441   0.0367066    0.138509    -0.033865  
  0.0723565    0.113824    -0.119794     0.06916      0.281879    -0.0562906    -0.00926488   0.0311445   0.154417    -0.154867    -0.0318086    0.0674506    0.0882238    -0.022031     0.046278     0.1071       0.165604     0.0231005     0.0271659   -0.191942    -0.127326   -0.0942406    0.0397212   -0.0150821   -0.0327758   -0.0227629 
  0.0707498   -0.0182412   -0.100289     0.0288676    0.0288332   -0.106281      0.0226012   -0.0272675  -0.0585897    0.0657414   -0.0245444   -0.184739    -0.102683     -0.148411    -0.103115     0.0352334    0.199062     0.0412381    -0.0255847    0.0872382   -0.0993212  -0.0825232    0.147395     0.0721572    0.115037     0.0290083 
 -0.0346993   -0.0536965    0.171955    -0.0483922   -0.044496    -0.0280887    -0.0344101   -0.0546439   0.0315733    0.045743    -0.0754493    0.00830125   0.0208477    -0.0207133   -0.030337    -0.0781454    0.141557    -0.110932     -0.134071     0.0241053   -0.0934392   0.00350271   0.112893    -0.0410347    0.0302055    0.0352785 
  0.133885     0.0648636   -0.0391274   -0.0322393   -0.0452488    0.185494      0.00664332  -0.0372975   0.00930905   0.0289874   -0.0060886   -0.0317158   -0.0431419     0.0128754    0.0542835    0.0322754   -0.0658119   -0.0386194     0.0965901   -0.105167     0.0244268   0.111549    -0.0536094   -0.0275277   -0.0351094    0.00678444
 -0.165761     0.176619     0.09288      0.0600324   -0.01295     -0.111535      0.080985     0.0729641  -0.129704    -0.138403     0.0874649    0.0869713   -0.0365988    -0.0254785   -0.0522195    0.168438     0.0215762    0.150007      0.0578804   -0.00986695  -0.0519924  -0.201668    -0.111981     0.0512881    0.00874041  -0.156695  
 -0.14169     -0.037633     0.0341992   -0.0749364    0.0330223   -0.0199819    -0.121928    -0.074175   -0.0529143   -0.210208     0.131836     0.0468236   -0.0720593    -0.0375747    0.045072    -0.0169935    0.173178    -0.0952358     0.203258     0.0127308   -0.0715315   0.0851893    0.0413856   -0.0711713   -0.265714     0.00789811
 -0.0569362   -0.0253182    0.0837356   -0.14303     -0.116885    -0.0494867     0.0997052   -0.0382391   2.51912e-5   0.0521247   -0.136392     0.0217623    0.0374858     0.0513693   -0.0361206    0.0595114   -0.0263691   -0.0510253     0.148488     0.0617748    0.0438227   0.180767    -0.161737    -0.00999869  -0.0503212   -0.109555  
 -0.183056    -0.0513805   -0.107401     0.0197003    0.0921779   -0.0222742     0.0700755    0.122614   -0.032661     0.029858    -0.00528894  -0.0645059    0.176778      0.00585057   0.0354725    0.0653283   -0.291038     0.000613894   0.0137396    0.15064      0.0689397  -0.0553906    0.14389     -0.0674435    0.111396     0.0477135 
  0.109315     0.133035     0.116008    -0.359999    -0.279993    -0.000539879   0.0164455   -0.0363514   0.146581     0.0775428   -0.00679399   0.0463665    0.117664     -0.0361074    0.018504    -0.129404    -0.0975707   -0.106517     -0.163177     0.0656564    0.162813   -0.034476     0.120448     0.112799    -0.0310958    0.0368941 
  0.496246     0.0131354    0.0103933    0.35103     -0.00904335  -0.000223006   0.0997234    0.0956491  -0.031722    -0.0561395    0.134745    -0.0825334   -0.0810663     0.0419451   -0.207288    -0.0941448   -0.0582475   -0.0345402    -0.0441659    0.05369     -0.0588078  -0.00166569  -0.261158     0.0159568   -0.334738     0.194022  
  0.0559088    0.0207836   -0.0984652   -0.0351686    0.193238     0.0186171     0.024956    -0.0823249  -0.0429972    0.0650592   -0.0563621    0.158714    -0.172036      0.0212369   -0.00692759  -0.0124884    0.0720785   -0.0279179    -0.0999803    0.0422902   -0.0580479   0.017355    -0.0796565   -0.234305    -0.0183473   -0.011873  
  0.075323    -0.152594    -0.237774     0.016265     0.0512885    0.0263285     0.0659421   -0.207902   -0.0266838   -0.132924     0.0938768   -0.0392956   -0.133799     -0.227188     0.00781239  -0.0233826   -0.0261661   -0.162853      0.227264     0.177024     0.0576085  -0.287147     0.178051    -0.00290957  -0.00785815  -0.153435  
  0.229048     0.02314     -0.0808498    0.135097     0.0614198    0.0122618     0.0267719    0.0521511   0.0442863    0.0270551   -0.0346569   -0.023977    -0.0789527     0.0476911    0.0722513    0.0182177    0.0977607    0.0932266    -0.101659     0.0804184   -0.0244203  -0.0586953   -0.0370832   -0.0054926    0.111102     0.0633343 
 -0.14831     -0.12611     -0.164767     0.137591    -0.137225     0.0724695    -0.170472     0.0610224  -0.285218    -0.0373398    0.215079     0.165347    -0.185371      0.0632717   -0.269576     0.0251845    0.253263    -0.165416     -0.202157     0.109677    -0.0338058   0.0417632   -0.146467     0.0824911    0.0229421    0.0421231 
  0.0640211   -0.0880848    0.122068     0.146596     0.111856    -0.0177666     0.0868253    0.112022   -0.0507872    0.0414097   -0.083773     0.0727308   -0.0367762    -0.00733029   0.070298    -0.120538    -0.018051     0.144119     -0.0737056   -0.0962082   -0.0615745  -0.0988368    0.00300849   0.132554     0.0223581    0.0574158 
 -0.0229525    0.0635893   -0.00885393   0.0508245   -0.0749463    0.0569403    -0.113244    -0.11304     0.065933     0.163147    -0.0666609    0.0641532   -0.00309609    0.15914      0.0671195   -0.1585      -0.113911     0.0259526    -0.0294969    0.130309     0.0017669  -0.0336774   -0.00868147   0.0142252    0.0352624   -0.346467  
  0.0424834    0.0835528    0.050143    -0.101666     0.034068    -0.0128899     0.0902439    0.138052   -0.0960844   -0.0614187    0.0918369   -0.0198888    0.0655744    -0.00200607   0.1093      -0.188952     0.0130833   -0.0010704     0.138063     0.0668742   -0.014262   -0.023337    -0.0220048    0.0658649    0.0883781    0.0857729 
 -0.0189484   -0.14671     -0.0601962    0.134248    -0.0371147    0.0572155     0.0975672    0.0671184  -0.0353758    0.128798    -0.00157632  -0.0721136    0.0655764    -0.0906892   -0.242071    -0.0457544    0.00128874  -0.203949      0.189488     0.0903252   -0.169885   -0.0066035    0.0113215    0.0610463    0.0136886    0.0226349 
 -0.135504    -0.0971508    0.0670114    0.123228    -0.141528    -0.00532661   -0.177893     0.0103958  -0.0386412    0.0463388    0.00133763   0.00967955   0.000795651  -0.0245875    0.0139668    0.00316527  -0.0646672   -0.0746214     0.150205    -0.109307     0.0547006  -0.069735    -0.0486364   -0.179719     0.199883    -0.094521  
  0.0662084    0.12386     -0.00228524  -0.0441988    0.102402     0.0612253    -0.038641    -0.0351013   0.150462     0.0388408    0.0294634   -0.132284     0.0176784     0.0108621   -0.196432     0.186018     0.146932    -0.0963696     0.0876834    0.0390588    0.209331   -0.0394703    0.0104382   -0.162068    -0.0250605   -0.249438  
 -0.0432784   -0.123124    -0.133289    -0.136201     0.085581     0.173568     -0.210973    -0.0899216   0.176008    -0.0769535   -0.0675855   -0.141367     0.00387038   -0.0790627   -0.0622887   -0.0645263   -0.171904     0.0404069     0.0498306    0.0320626    0.0666901  -0.096964     0.223183    -0.06776     -0.256073    -0.0742478 
  0.0246547    0.00210658   0.0657121   -0.0521739   -0.0985661    0.290827      0.059548     0.0245668  -0.100867     0.0302767   -0.0737538    0.0136579    0.0516973     0.00419663  -0.0439777    0.13606      0.0486044   -0.0341767     0.0821919    0.0909331   -0.0109596   0.115018    -0.133283     0.0752777    0.0744648   -0.0114005 
 -0.0634524   -0.0276999    0.0114188    0.09043     -0.0475127    0.12592      -0.0903581    0.130924   -0.00983221   0.0147738   -0.0881202   -0.00971946   0.0686887     0.15687     -0.0229939    0.101965     0.16632      0.0712156     0.0310643    0.0522117   -0.215727    0.0977251    0.0110277    0.0691826    0.0809489    0.091116  
  0.0427186    0.0115119   -0.0310113   -0.0278274   -0.0794307   -0.0654488    -0.120021     0.0947155   0.113018     0.139858     0.023891    -0.0232853   -0.167919     -0.107761     0.0117381    0.0395182    0.0168426   -0.140059      0.0117773   -0.0560211   -0.0156925  -0.0572551   -0.145939    -0.219066     0.0561885   -0.0879926 
  0.0458614    0.125997    -0.0393425   -0.0969265   -0.019151     0.00206045   -0.00659822   0.0522228   0.0770615   -0.210479    -0.0898241   -0.00296742   0.0565687    -0.0478726   -0.16643     -0.188961    -0.0985522    0.0237837     0.127396    -0.0186719    0.0594909   0.0744345   -0.045322    -0.0147002   -0.226451    -0.0760668 
  0.110751     0.00208188   0.133136     0.0572373   -0.0545177    0.0604043    -0.00614217   0.042151   -0.169604    -0.059953     0.0271482    0.0489102    0.0610154     0.0201407    0.0473483   -0.157       -0.080785    -0.00381237    0.209034     0.00377573   0.0285345  -0.024822    -0.0977998   -0.0900193   -0.0497577    0.122904  
 -0.0256344   -0.12051     -0.148265    -0.00788204   0.0332628    0.123485      0.054276     0.105451   -0.0812369    0.0325184   -0.0598326    0.0803449   -0.0601678    -0.0999758    0.119933     0.00729015   0.115106    -0.0619595     0.114423     0.0987851   -0.0454323   0.0817928   -0.245313     0.0276795   -0.0806932   -0.0751609 
  0.100951    -0.0529804   -0.0233592   -0.146768    -0.106459     0.154244      0.0964847   -0.283442   -0.0028205    0.0128084   -0.0658131   -0.0334367    0.0104779    -0.0778023   -0.127911    -0.0785336    0.0452046    0.0616662     0.142793     0.0820956   -0.076152    0.16387      0.152238     0.00571672  -0.00114512   0.0782106 
  0.00996609  -0.0304583   -0.0537123   -0.102483    -0.106024    -0.0599214    -0.0610397    0.167955   -0.105266     0.00428145   0.0483958   -0.0677863    0.253743      0.0590733   -0.0723179   -0.225919    -0.00803599   0.238472      0.152586    -0.0845877    0.0949727   0.0473161    0.0838712   -0.166989     0.110463    -0.0442047 [ Info: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      6
│      8
│     21
│     27
│     28
│     31
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 1, average log likelihood -1.017895
┌ Warning: Variances had to be floored 
│   ind =
│    17-element Array{Int64,1}:
│      1
│      2
│      5
│      6
│      ⋮
│     28
│     30
│     31
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 2, average log likelihood -0.963131
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│      5
│      6
│      8
│     16
│      ⋮
│     27
│     28
│     31
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 3, average log likelihood -1.006968
┌ Warning: Variances had to be floored 
│   ind =
│    17-element Array{Int64,1}:
│      1
│      2
│      5
│      6
│      ⋮
│     28
│     30
│     31
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 4, average log likelihood -0.974515
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│      5
│      6
│      8
│     16
│      ⋮
│     27
│     28
│     31
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 5, average log likelihood -1.008359
┌ Warning: Variances had to be floored 
│   ind =
│    17-element Array{Int64,1}:
│      1
│      2
│      5
│      6
│      ⋮
│     28
│     30
│     31
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 6, average log likelihood -0.974526
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│      5
│      6
│      8
│     16
│      ⋮
│     27
│     28
│     31
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 7, average log likelihood -1.008362
┌ Warning: Variances had to be floored 
│   ind =
│    17-element Array{Int64,1}:
│      1
│      2
│      5
│      6
│      ⋮
│     28
│     30
│     31
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 8, average log likelihood -0.974526
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│      5
│      6
│      8
│     16
│      ⋮
│     27
│     28
│     31
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 9, average log likelihood -1.008361
┌ Warning: Variances had to be floored 
│   ind =
│    17-element Array{Int64,1}:
│      1
│      2
│      5
│      6
│      ⋮
│     28
│     30
│     31
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 10, average log likelihood -0.974525
┌ Info: EM with 100000 data points 10 iterations avll -0.974525
└ 59.0 data points per parameter
32×26 Array{Float64,2}:
 -0.00171068  -0.091812     0.0535498   -0.0281929   -0.121096     -0.115094    -0.229189      0.0863738    0.0130991   -0.0025458   -0.019025    -0.025083    -0.0230856    0.0306098    0.066228    -0.0706154    0.0650194   -0.0286361   -0.063498     0.0310664  -0.0788755   -0.023876    -0.0529801     0.110204     0.00853459  -0.0139545 
 -0.0371733    0.115131     0.0479555   -0.0473414   -0.079692      0.190806     0.0876551     0.0939826   -0.0669915   -0.0511235   -0.054936    -0.0536193    0.0136071    0.0254475    0.055784    -0.0225608   -0.00259376  -0.0650077   -0.128788    -0.154752   -0.167554     0.0134826   -0.00450526   -0.0401043    0.0126938   -0.0692978 
 -0.056132    -0.00617181  -0.0345384    0.148626    -0.218192     -0.044583     0.0530114    -0.201048    -0.00421883  -0.120594    -0.18239      0.0420921    0.163394     0.204605     0.0365645   -0.0615542   -0.00259655  -0.26235      0.0237652   -0.0728456  -0.129629    -0.0185229   -0.0422901    -0.0739254    0.0323784   -0.164475  
 -0.0266082    0.0123868    0.213045    -0.0411526   -0.0541007    -0.0142696   -0.00780538    0.153451    -0.0275106    0.0545339    0.0262554   -0.147529    -0.0433604   -0.0441655   -0.0345398    0.0269022   -0.04345     -0.148221     0.17424      0.270657    0.0939117   -0.0341798   -0.0594371     0.0343049    0.0124116    0.100669  
  0.0807009   -0.11077      0.0308195   -0.00848393   0.116163     -0.055885    -0.00762187    0.0155739    0.214855     0.0253836    0.0829211   -0.0228276   -0.0196667    0.117678    -0.029742     0.0243527    0.0377853   -0.0124621    0.0409042   -0.0542295  -0.00131714  -0.120361     0.158683     -0.115946    -0.133201    -0.0848966 
  0.140753    -0.0282318   -0.0308696    0.153967     0.0939602     0.0319532    0.0380429     0.0737241    0.0949458   -0.045638    -0.0772338    0.0211239    0.0734317   -0.135613    -0.224255     0.106908     0.0937204   -0.137999    -0.0957941    0.209977   -0.131403    -0.158683    -0.0241183     0.121948    -0.115003     0.0324855 
  0.079169     0.129701     0.124139    -0.0690591    0.0983739    -0.0259482    0.106588     -0.0409012    0.00305483   0.0557745    0.115644    -0.0222649    0.0305438    0.17353      0.249145     0.0349643    0.0656004   -0.0302389    0.127253     0.043341    0.108402     0.00807729   0.218366      0.0410704    0.204113     0.100283  
 -0.0328372    0.052765    -0.106942     0.00512178  -0.0145094     0.0670213    0.220785      0.0721099    0.199385    -0.059816    -0.0184423    0.0291473   -0.0295188   -0.236687    -0.113613    -0.00122861   0.135365     0.0977999    0.0558379   -0.017507    0.0269142   -0.00616605  -0.164518      0.0498341   -0.0139437   -0.0661108 
 -0.0177649    0.0405549   -0.045825    -0.0658523   -0.0460121    -0.111213     0.0463343     0.106884     0.191756    -0.0212219   -0.00782971   0.0492105    0.159993    -0.103726    -0.057964    -0.0094804   -0.0977074    0.0155375   -0.0351682    0.11469     0.0092806    0.0204741    0.183998     -0.11643      0.0946372    0.116528  
  0.209805     0.173895     0.0487013   -0.234881     0.0561517    -0.167916    -0.0562663    -0.133251     0.133162     0.129288    -0.0341337   -0.0207662    0.0274482   -0.0505707   -0.0659137   -0.0250187   -0.0850299   -0.0472915    0.0595691    0.0592188   0.0395145    0.0764124   -0.143927      0.0753498    0.0263329   -0.149196  
  0.0377851   -0.0215035   -0.0266086   -0.0386074    0.0883812    -0.0258251   -0.0798462    -0.0784308    0.0230267    0.0068724    0.0287163   -0.0663377   -0.0945564    0.0224769    0.221208    -0.0255067    0.093741     0.0992539    0.0850626   -0.0403982  -0.0288012   -0.10712     -0.0420675    -0.0378513    0.0612854   -0.0236262 
  0.0727099    0.0396604    0.145096     0.0421644    0.000962871  -0.0510283   -0.0443658    -0.104415    -0.103772    -0.145768    -0.068846     0.137207    -0.0477165    0.0239463   -0.0186804    0.0920378   -0.111822    -0.0948202    0.210303     0.15785     0.0367023   -0.00835275  -0.185709     -0.096841    -0.0968052    0.0111527 
  0.115271     0.00932052   0.106619     0.0620363    0.0254737     0.0294035    0.061054      0.0593654   -0.0399655   -0.0951461   -0.122202     0.152492    -0.0807188   -0.1261      -0.0311694    0.0702461    0.0755721    0.025792     0.0542969    0.160517    0.00548801   0.0855556   -0.140031      0.115966     0.00704091   0.00119487
 -0.127799     0.0675322    0.0762229   -0.0445278   -0.000246619   0.13313     -0.111477      0.0109653    0.00244554  -0.148585    -0.0609365    0.0900301   -0.00281851  -0.0732514   -0.186304     0.085976    -0.013374    -0.333279    -0.0301363   -0.0771233  -0.112645     0.0234154    0.184586     -0.152609    -0.0641853   -0.00902402
 -0.034898    -0.0249387   -0.00248526   0.0839279    0.120826     -0.122926     0.0610819     0.0616322   -0.0639452   -0.0876115   -0.262431     0.0622617    0.0343744    0.0720053   -0.0748506    0.0238447    0.0432409   -0.148272     0.00802224   0.125671   -0.0474562    0.058152    -0.159461     -0.0304056    0.0668132   -0.19109   
 -0.0457206   -0.0333541   -0.027292     0.193663     0.138314     -0.0737431    0.008759     -0.125864     0.153335    -0.115945    -0.0850606    0.0212372    0.0273911   -0.0146865    0.0588548   -0.0806921    0.115519     0.00401029   0.0920667   -0.175822    0.0361651    0.0225111   -0.0327402    -0.116159     0.146091     0.0211898 
 -0.0308189   -0.161204    -0.00190775   0.111189    -0.181449      0.162159    -0.0455802     0.0890017   -0.0595212   -0.0278701   -0.0192777    0.0990531   -0.062646     0.143452    -0.0827737    0.154428    -0.0289762    0.151556    -0.105919     0.0394586  -0.10726      0.0591804    0.0138969    -0.0456042   -0.059936    -0.023828  
  0.125514    -0.0498674    0.102023     0.0990959    0.0128735     0.132198    -0.0301793     0.0722922   -0.0502558   -0.0458152   -0.0812361    0.00656944   0.0937502    0.0426875   -0.047345     0.114651     0.036156    -0.137684    -0.101827     0.037306    0.12279      0.0882381    0.000200798   0.0293942    0.0637882   -0.0610511 
 -0.0319779   -0.0351166    0.00425789  -0.0685014   -0.0796925     0.120975    -0.197291      0.0770208    0.113745    -0.0715252    0.00415608   0.0583469   -0.0100014    0.0902351   -0.0754391    0.159898     0.126809     0.0948399    0.121632     0.0322454  -0.114096    -0.137049     0.0307172    -0.162146    -0.0402818   -0.0155889 
  0.00854097  -0.149493     0.086528    -0.0716771    0.0988338    -0.0992427   -0.191298     -0.0940853   -0.157861    -0.00219594   0.0454634    0.00159449  -0.0668989   -0.029659     0.196357     0.0309816    0.103139    -0.13096     -0.0230612   -0.0420843  -0.123544     0.19956      0.0404708     0.287976     0.0336486   -0.0239789 
  0.00650708   0.145547     0.0842633    0.0345855   -0.299808      0.115096     0.0242346     0.0557962   -0.168677    -0.0806227   -0.0843495   -0.00129164  -0.019644     0.114863    -0.0204729    0.00615003  -0.15174     -0.0627879    0.0695361   -0.0995361   0.217238    -0.100254     0.0742473     0.0258598    0.015355    -0.0697539 
 -0.00966547   0.100129    -0.0348084    0.0055045    0.120344     -0.0972431    0.0594841     0.0879581    0.0440533   -0.152        0.165328    -0.0237105    0.269217     0.0391584   -0.170038     0.14874      0.0138115   -0.015911    -0.239583     0.127829    0.0580321    0.0376195   -0.209673      0.145997    -0.100889     0.0738258 
 -0.0434441    0.0672798    0.0926979    0.0681349   -0.219825     -0.0147808    0.0683545    -0.0791315    0.0553838    0.0753988    0.014379     0.0599395   -0.103802    -0.0901381   -0.0210989    0.0306523   -0.124461    -0.0383905   -0.173639     0.078988    0.0482358   -0.0646046   -0.117875     -0.116107    -0.102511    -0.0689671 
  0.0219079    0.0539792   -0.184297     0.0294363    0.268558      0.0963033   -0.193571      0.0556121   -0.00854019  -0.0231345    0.100837     0.0480248    0.0255286    0.0256626    0.0678494   -0.0196357   -0.102168     0.0222443   -0.118452     0.0738946   0.0759765   -0.146128    -0.0349963    -0.206855    -0.155479     0.0747826 
 -0.0557707    0.213176    -0.100415     0.137973     0.194176     -0.0699432    0.054575      0.00161587   0.0300753   -0.0987523   -0.0237185   -0.192623    -0.198091     0.00598669   0.0108428   -0.081663    -0.0780487   -0.0416131    0.127353     0.0712119   0.0144606    0.0983252   -0.0252672     0.062447    -0.113482     0.122061  
 -0.143541     0.130175     0.112373     0.0405274   -0.232872      0.0528128   -0.072223      0.0108529   -0.00915577  -0.148842     0.107229     0.0483507   -0.014234    -0.0772759   -0.0505577    0.018008     0.0250319    0.0324682    0.00108035   0.116413    0.025681     0.058284     0.0324738     0.0693645    0.0602763   -0.114849  
 -0.00764763   0.162004    -0.00227755   0.0304598    0.0966529     0.00699012   0.000590957  -0.107807     0.0442316   -0.0567703    0.00317218  -0.0563007    0.0759564   -0.0700383    0.151948     0.0378595   -0.104662     0.107671     0.00620766   0.250386    0.0748012   -0.00369899  -0.0636593    -0.0573344   -0.0247471    0.0400487 
  0.00213075  -0.163621    -0.0804019   -0.0851387   -0.0177388     0.138999     0.00900316    0.144175     0.143237    -0.0605058    0.099227     0.0577595    0.138825    -0.0459857    0.184148    -0.10652     -0.0253692    0.0786827   -0.104597     0.0179412   0.0716679   -0.030522    -0.04324      -0.051586     0.0593383   -0.224839  
  0.0504606   -0.067202    -0.0248419   -0.0848553    0.0676553     0.110279    -0.135864     -0.0629462   -0.128742     0.0186318    0.00743659  -0.140446     0.0913124   -0.0185295    0.0391066    0.117941     0.0811285    0.0695874   -0.0608747    0.169884    0.00354955  -0.0903843    0.0403365     0.0484705    0.136375     0.206092  
  0.0838836   -0.00439492   0.0559069   -0.020291    -0.0800774    -0.19333      0.0578133    -0.0401729   -0.0198306   -0.036192    -0.0930862    0.0273602   -0.00841879   0.0113995   -0.00196754   0.0651198   -0.0161638   -0.0514807    0.13296      0.0590989   0.00700913   0.0207369    0.0290295     0.0161457    0.0407687   -0.199236  
 -0.147943    -0.033325    -0.13884      0.0559861   -0.0947168    -0.0553126    0.128252      0.0161863    0.0092221   -0.107903     0.049248    -0.0452611   -0.0172498    0.00600662   0.0775206    0.0279122    0.176569    -0.184805    -0.131355     0.196926   -0.0810943    0.112276    -0.161945      0.00897875   0.011466     0.0572978 
  0.165777    -0.033998     0.0368629   -0.0171854    0.200004     -0.0679391    0.0500604     0.0783309    0.0189772   -0.149851    -0.0421635    0.0110558    0.123381     0.112886     0.267658    -0.139877     0.0290476   -0.146683    -0.207283    -0.0191605  -0.0105756   -0.0187248    0.0558914    -0.00843265   0.16754      0.0266541 kind full, method split
┌ Info: 0: avll = 
└   tll[1] = -1.4160686486012748
[ Info: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.416087
[ Info: iteration 2, average log likelihood -1.416027
[ Info: iteration 3, average log likelihood -1.415985
[ Info: iteration 4, average log likelihood -1.415939
[ Info: iteration 5, average log likelihood -1.415886
[ Info: iteration 6, average log likelihood -1.415824
[ Info: iteration 7, average log likelihood -1.415745
[ Info: iteration 8, average log likelihood -1.415626
[ Info: iteration 9, average log likelihood -1.415404
[ Info: iteration 10, average log likelihood -1.414949
[ Info: iteration 11, average log likelihood -1.414106
[ Info: iteration 12, average log likelihood -1.412917
[ Info: iteration 13, average log likelihood -1.411804
[ Info: iteration 14, average log likelihood -1.411137
[ Info: iteration 15, average log likelihood -1.410849
[ Info: iteration 16, average log likelihood -1.410741
[ Info: iteration 17, average log likelihood -1.410701
[ Info: iteration 18, average log likelihood -1.410685
[ Info: iteration 19, average log likelihood -1.410679
[ Info: iteration 20, average log likelihood -1.410676
[ Info: iteration 21, average log likelihood -1.410675
[ Info: iteration 22, average log likelihood -1.410674
[ Info: iteration 23, average log likelihood -1.410674
[ Info: iteration 24, average log likelihood -1.410673
[ Info: iteration 25, average log likelihood -1.410673
[ Info: iteration 26, average log likelihood -1.410673
[ Info: iteration 27, average log likelihood -1.410672
[ Info: iteration 28, average log likelihood -1.410672
[ Info: iteration 29, average log likelihood -1.410672
[ Info: iteration 30, average log likelihood -1.410672
[ Info: iteration 31, average log likelihood -1.410672
[ Info: iteration 32, average log likelihood -1.410672
[ Info: iteration 33, average log likelihood -1.410671
[ Info: iteration 34, average log likelihood -1.410671
[ Info: iteration 35, average log likelihood -1.410671
[ Info: iteration 36, average log likelihood -1.410671
[ Info: iteration 37, average log likelihood -1.410671
[ Info: iteration 38, average log likelihood -1.410671
[ Info: iteration 39, average log likelihood -1.410671
[ Info: iteration 40, average log likelihood -1.410671
[ Info: iteration 41, average log likelihood -1.410671
[ Info: iteration 42, average log likelihood -1.410671
[ Info: iteration 43, average log likelihood -1.410671
[ Info: iteration 44, average log likelihood -1.410671
[ Info: iteration 45, average log likelihood -1.410671
[ Info: iteration 46, average log likelihood -1.410671
[ Info: iteration 47, average log likelihood -1.410671
[ Info: iteration 48, average log likelihood -1.410670
[ Info: iteration 49, average log likelihood -1.410670
[ Info: iteration 50, average log likelihood -1.410670
┌ Info: EM with 100000 data points 50 iterations avll -1.410670
└ 952.4 data points per parameter
┌ Info: 1
│   : avll =  = &quot;: avll = &quot;
│   avll =
│    50-element Array{Float64,1}:
│     -1.4160872265880866
│     -1.4160273642422545
│      ⋮                 
└     -1.4106704442812057
[ Info: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.410685
[ Info: iteration 2, average log likelihood -1.410627
[ Info: iteration 3, average log likelihood -1.410583
[ Info: iteration 4, average log likelihood -1.410535
[ Info: iteration 5, average log likelihood -1.410478
[ Info: iteration 6, average log likelihood -1.410413
[ Info: iteration 7, average log likelihood -1.410339
[ Info: iteration 8, average log likelihood -1.410258
[ Info: iteration 9, average log likelihood -1.410175
[ Info: iteration 10, average log likelihood -1.410092
[ Info: iteration 11, average log likelihood -1.410012
[ Info: iteration 12, average log likelihood -1.409939
[ Info: iteration 13, average log likelihood -1.409875
[ Info: iteration 14, average log likelihood -1.409822
[ Info: iteration 15, average log likelihood -1.409778
[ Info: iteration 16, average log likelihood -1.409741
[ Info: iteration 17, average log likelihood -1.409711
[ Info: iteration 18, average log likelihood -1.409684
[ Info: iteration 19, average log likelihood -1.409661
[ Info: iteration 20, average log likelihood -1.409640
[ Info: iteration 21, average log likelihood -1.409621
[ Info: iteration 22, average log likelihood -1.409604
[ Info: iteration 23, average log likelihood -1.409589
[ Info: iteration 24, average log likelihood -1.409576
[ Info: iteration 25, average log likelihood -1.409565
[ Info: iteration 26, average log likelihood -1.409555
[ Info: iteration 27, average log likelihood -1.409546
[ Info: iteration 28, average log likelihood -1.409539
[ Info: iteration 29, average log likelihood -1.409532
[ Info: iteration 30, average log likelihood -1.409527
[ Info: iteration 31, average log likelihood -1.409522
[ Info: iteration 32, average log likelihood -1.409517
[ Info: iteration 33, average log likelihood -1.409513
[ Info: iteration 34, average log likelihood -1.409510
[ Info: iteration 35, average log likelihood -1.409507
[ Info: iteration 36, average log likelihood -1.409504
[ Info: iteration 37, average log likelihood -1.409501
[ Info: iteration 38, average log likelihood -1.409498
[ Info: iteration 39, average log likelihood -1.409496
[ Info: iteration 40, average log likelihood -1.409493
[ Info: iteration 41, average log likelihood -1.409491
[ Info: iteration 42, average log likelihood -1.409488
[ Info: iteration 43, average log likelihood -1.409486
[ Info: iteration 44, average log likelihood -1.409484
[ Info: iteration 45, average log likelihood -1.409482
[ Info: iteration 46, average log likelihood -1.409479
[ Info: iteration 47, average log likelihood -1.409477
[ Info: iteration 48, average log likelihood -1.409475
[ Info: iteration 49, average log likelihood -1.409472
[ Info: iteration 50, average log likelihood -1.409470
┌ Info: EM with 100000 data points 50 iterations avll -1.409470
└ 473.9 data points per parameter
┌ Info: 2
│   : avll =  = &quot;: avll = &quot;
│   avll =
│    50-element Array{Float64,1}:
│     -1.4106850268010835
│     -1.4106274623482606
│      ⋮                 
└     -1.4094701095333801
[ Info: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.409478
[ Info: iteration 2, average log likelihood -1.409419
[ Info: iteration 3, average log likelihood -1.409369
[ Info: iteration 4, average log likelihood -1.409313
[ Info: iteration 5, average log likelihood -1.409246
[ Info: iteration 6, average log likelihood -1.409167
[ Info: iteration 7, average log likelihood -1.409076
[ Info: iteration 8, average log likelihood -1.408976
[ Info: iteration 9, average log likelihood -1.408873
[ Info: iteration 10, average log likelihood -1.408771
[ Info: iteration 11, average log likelihood -1.408676
[ Info: iteration 12, average log likelihood -1.408591
[ Info: iteration 13, average log likelihood -1.408517
[ Info: iteration 14, average log likelihood -1.408454
[ Info: iteration 15, average log likelihood -1.408401
[ Info: iteration 16, average log likelihood -1.408357
[ Info: iteration 17, average log likelihood -1.408319
[ Info: iteration 18, average log likelihood -1.408286
[ Info: iteration 19, average log likelihood -1.408256
[ Info: iteration 20, average log likelihood -1.408229
[ Info: iteration 21, average log likelihood -1.408204
[ Info: iteration 22, average log likelihood -1.408179
[ Info: iteration 23, average log likelihood -1.408156
[ Info: iteration 24, average log likelihood -1.408133
[ Info: iteration 25, average log likelihood -1.408110
[ Info: iteration 26, average log likelihood -1.408088
[ Info: iteration 27, average log likelihood -1.408067
[ Info: iteration 28, average log likelihood -1.408046
[ Info: iteration 29, average log likelihood -1.408026
[ Info: iteration 30, average log likelihood -1.408007
[ Info: iteration 31, average log likelihood -1.407988
[ Info: iteration 32, average log likelihood -1.407970
[ Info: iteration 33, average log likelihood -1.407954
[ Info: iteration 34, average log likelihood -1.407938
[ Info: iteration 35, average log likelihood -1.407923
[ Info: iteration 36, average log likelihood -1.407909
[ Info: iteration 37, average log likelihood -1.407896
[ Info: iteration 38, average log likelihood -1.407883
[ Info: iteration 39, average log likelihood -1.407872
[ Info: iteration 40, average log likelihood -1.407861
[ Info: iteration 41, average log likelihood -1.407851
[ Info: iteration 42, average log likelihood -1.407841
[ Info: iteration 43, average log likelihood -1.407832
[ Info: iteration 44, average log likelihood -1.407823
[ Info: iteration 45, average log likelihood -1.407815
[ Info: iteration 46, average log likelihood -1.407808
[ Info: iteration 47, average log likelihood -1.407800
[ Info: iteration 48, average log likelihood -1.407793
[ Info: iteration 49, average log likelihood -1.407787
[ Info: iteration 50, average log likelihood -1.407780
┌ Info: EM with 100000 data points 50 iterations avll -1.407780
└ 236.4 data points per parameter
┌ Info: 3
│   : avll =  = &quot;: avll = &quot;
│   avll =
│    50-element Array{Float64,1}:
│     -1.409477913979034 
│     -1.4094188830931649
│      ⋮                 
└     -1.4077802585852508
[ Info: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.407783
[ Info: iteration 2, average log likelihood -1.407723
[ Info: iteration 3, average log likelihood -1.407667
[ Info: iteration 4, average log likelihood -1.407600
[ Info: iteration 5, average log likelihood -1.407516
[ Info: iteration 6, average log likelihood -1.407410
[ Info: iteration 7, average log likelihood -1.407281
[ Info: iteration 8, average log likelihood -1.407132
[ Info: iteration 9, average log likelihood -1.406974
[ Info: iteration 10, average log likelihood -1.406817
[ Info: iteration 11, average log likelihood -1.406670
[ Info: iteration 12, average log likelihood -1.406539
[ Info: iteration 13, average log likelihood -1.406427
[ Info: iteration 14, average log likelihood -1.406332
[ Info: iteration 15, average log likelihood -1.406254
[ Info: iteration 16, average log likelihood -1.406190
[ Info: iteration 17, average log likelihood -1.406137
[ Info: iteration 18, average log likelihood -1.406093
[ Info: iteration 19, average log likelihood -1.406055
[ Info: iteration 20, average log likelihood -1.406024
[ Info: iteration 21, average log likelihood -1.405997
[ Info: iteration 22, average log likelihood -1.405973
[ Info: iteration 23, average log likelihood -1.405952
[ Info: iteration 24, average log likelihood -1.405933
[ Info: iteration 25, average log likelihood -1.405916
[ Info: iteration 26, average log likelihood -1.405900
[ Info: iteration 27, average log likelihood -1.405885
[ Info: iteration 28, average log likelihood -1.405872
[ Info: iteration 29, average log likelihood -1.405859
[ Info: iteration 30, average log likelihood -1.405846
[ Info: iteration 31, average log likelihood -1.405834
[ Info: iteration 32, average log likelihood -1.405822
[ Info: iteration 33, average log likelihood -1.405811
[ Info: iteration 34, average log likelihood -1.405800
[ Info: iteration 35, average log likelihood -1.405789
[ Info: iteration 36, average log likelihood -1.405779
[ Info: iteration 37, average log likelihood -1.405768
[ Info: iteration 38, average log likelihood -1.405757
[ Info: iteration 39, average log likelihood -1.405747
[ Info: iteration 40, average log likelihood -1.405737
[ Info: iteration 41, average log likelihood -1.405726
[ Info: iteration 42, average log likelihood -1.405716
[ Info: iteration 43, average log likelihood -1.405705
[ Info: iteration 44, average log likelihood -1.405695
[ Info: iteration 45, average log likelihood -1.405684
[ Info: iteration 46, average log likelihood -1.405673
[ Info: iteration 47, average log likelihood -1.405662
[ Info: iteration 48, average log likelihood -1.405652
[ Info: iteration 49, average log likelihood -1.405641
[ Info: iteration 50, average log likelihood -1.405630
┌ Info: EM with 100000 data points 50 iterations avll -1.405630
└ 118.1 data points per parameter
┌ Info: 4
│   : avll =  = &quot;: avll = &quot;
│   avll =
│    50-element Array{Float64,1}:
│     -1.4077830384729233
│     -1.407722932513263 
│      ⋮                 
└     -1.405629644905048 
[ Info: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.405628
[ Info: iteration 2, average log likelihood -1.405556
[ Info: iteration 3, average log likelihood -1.405489
[ Info: iteration 4, average log likelihood -1.405409
[ Info: iteration 5, average log likelihood -1.405311
[ Info: iteration 6, average log likelihood -1.405189
[ Info: iteration 7, average log likelihood -1.405043
[ Info: iteration 8, average log likelihood -1.404877
[ Info: iteration 9, average log likelihood -1.404701
[ Info: iteration 10, average log likelihood -1.404524
[ Info: iteration 11, average log likelihood -1.404356
[ Info: iteration 12, average log likelihood -1.404201
[ Info: iteration 13, average log likelihood -1.404063
[ Info: iteration 14, average log likelihood -1.403942
[ Info: iteration 15, average log likelihood -1.403837
[ Info: iteration 16, average log likelihood -1.403747
[ Info: iteration 17, average log likelihood -1.403669
[ Info: iteration 18, average log likelihood -1.403602
[ Info: iteration 19, average log likelihood -1.403543
[ Info: iteration 20, average log likelihood -1.403491
[ Info: iteration 21, average log likelihood -1.403446
[ Info: iteration 22, average log likelihood -1.403404
[ Info: iteration 23, average log likelihood -1.403367
[ Info: iteration 24, average log likelihood -1.403333
[ Info: iteration 25, average log likelihood -1.403302
[ Info: iteration 26, average log likelihood -1.403273
[ Info: iteration 27, average log likelihood -1.403247
[ Info: iteration 28, average log likelihood -1.403221
[ Info: iteration 29, average log likelihood -1.403198
[ Info: iteration 30, average log likelihood -1.403175
[ Info: iteration 31, average log likelihood -1.403154
[ Info: iteration 32, average log likelihood -1.403134
[ Info: iteration 33, average log likelihood -1.403115
[ Info: iteration 34, average log likelihood -1.403096
[ Info: iteration 35, average log likelihood -1.403079
[ Info: iteration 36, average log likelihood -1.403062
[ Info: iteration 37, average log likelihood -1.403045
[ Info: iteration 38, average log likelihood -1.403030
[ Info: iteration 39, average log likelihood -1.403015
[ Info: iteration 40, average log likelihood -1.403001
[ Info: iteration 41, average log likelihood -1.402987
[ Info: iteration 42, average log likelihood -1.402974
[ Info: iteration 43, average log likelihood -1.402961
[ Info: iteration 44, average log likelihood -1.402949
[ Info: iteration 45, average log likelihood -1.402937
[ Info: iteration 46, average log likelihood -1.402925
[ Info: iteration 47, average log likelihood -1.402914
[ Info: iteration 48, average log likelihood -1.402904
[ Info: iteration 49, average log likelihood -1.402893
[ Info: iteration 50, average log likelihood -1.402883
┌ Info: EM with 100000 data points 50 iterations avll -1.402883
└ 59.0 data points per parameter
┌ Info: 5
│   : avll =  = &quot;: avll = &quot;
│   avll =
│    50-element Array{Float64,1}:
│     -1.4056275574860848
│     -1.4055564455827296
│      ⋮                 
└     -1.4028834145733382
┌ Info: Total log likelihood: 
│   tll =
│    251-element Array{Float64,1}:
│     -1.4160686486012748
│     -1.4160872265880866
│     -1.4160273642422545
│     -1.415985234133621 
│      ⋮                 
│     -1.402903670065055 
│     -1.402893373288528 
└     -1.4028834145733382
32×26 Array{Float64,2}:
 -0.245441   -0.329419     0.0434807  -0.371634    0.410029     0.116046    -0.134981    -0.728715   -0.433598    0.261922    0.149043    -0.573268   -0.0124265  -0.0414561   -0.00494379   0.473982     0.2706      -0.274894   -0.23257     0.0511525   0.145202   -0.316585   -0.082254     -0.58104      0.800678     0.169002  
  0.054417    0.0524025    0.0453987  -0.209325   -0.0765577   -0.165112    -0.589822     0.0824706  -0.565795    0.0605937   0.0105452   -0.480619   -0.0253173  -0.596411     0.124229     0.344553     0.56039      0.370159    0.172711   -0.117266    0.611913   -0.490952    0.109274     -0.766717     0.366204     0.111116  
  0.356925   -0.317866     0.538906   -0.143093   -0.272519     0.17382     -0.407344    -0.603907   -0.250182   -0.20995     0.431032    -0.318853    0.690418   -0.90866     -0.432633     0.275449     0.285114    -0.628665    0.0108805  -0.0557134   0.217968    0.0243157  -0.0210006     0.837337    -0.118769     0.0821218 
  0.100912   -0.168932     0.102611   -0.699431    0.244332    -0.00535217  -0.00787146  -0.0969172  -0.342738    0.109919    0.361162     0.0955796   0.409692   -0.891769     0.851744     0.205018     0.036661    -0.524037   -0.835144   -0.0360031  -0.33697    -0.0201833   0.0725451     0.743303     0.171489     0.579264  
  0.0381069  -0.193083     0.222449   -0.655723   -0.126968     0.328962    -0.0472075   -0.260117   -0.226825   -0.27888    -0.624253    -0.175646    0.549077    0.0840293    0.0772708    0.437843    -0.0558981    0.338102    0.368111   -0.149952   -0.372566   -0.0577776   0.0524395    -0.088071    -0.493647    -0.296937  
 -0.0539902   0.490461    -0.0410163  -0.32191     0.457931    -0.3246      -0.405329    -0.407112    0.301632    0.145327   -0.463016    -0.200536    0.173553   -0.49542     -0.136423     0.0652474   -0.32387     -0.203801    0.0424746  -0.045136    0.299477   -0.376109    0.168377      0.190695    -0.167697    -0.401386  
 -0.105037    0.528235     0.157675    0.101482   -0.769473     0.187659    -0.93567      0.252669   -0.227848    0.277039   -0.00658132  -0.296214    0.242393    0.386306    -0.622651     0.26038     -0.010012     0.489121   -0.430296   -0.0987585   0.151124   -0.225515    0.198713      0.276848    -0.327494    -0.16581   
  0.403532    0.384902    -0.0212469   0.538147   -0.0124066   -0.0529563    0.0465501    0.408742   -0.143402   -0.116717   -0.560962    -0.20409     0.10661     0.854267    -0.607004     0.542267    -0.323698     0.494368    0.524487    0.401599   -0.28582     0.0355593   0.0050698    -0.0528929    0.261476    -0.0284368 
  0.0246205  -0.0296863    0.0775513   0.0239248   0.00913773  -0.0519098    0.0461884   -0.101154   -0.155957   -0.0372129  -0.132398    -0.118356    0.167881    0.0468854   -0.262313     0.167773     0.0358779   -0.0731705   0.0132834   0.160148   -0.118722    0.0648     -0.105988     -0.0649286    0.114116    -0.124393  
  0.069784   -0.163716    -0.0858287  -0.156193   -0.0387437    0.191337    -0.00394952   0.202801    0.0926346  -0.0923801   0.021128     0.0597499  -0.079297    0.00894451   0.392765    -0.0717828   -0.00985228   0.0951074  -0.024995   -0.131252    0.12669    -0.0629044   0.0463184    -0.0387477   -0.108816     0.0660142 
 -0.082572   -0.269173    -0.0840428   0.337991   -0.0748707    0.38462      0.232543    -0.131996    0.556424   -0.225378    0.123476     0.260828   -0.21037     0.350448    -0.0249587   -0.00140786  -0.317513    -0.284091   -0.161077    0.578725   -0.46177     0.0761048  -0.436842      0.402129    -0.347735    -0.470659  
  0.0537569   0.432491    -0.147011    0.428786    0.109089    -0.144562     0.115586     0.146934    0.473763   -0.130798   -0.25885      0.671541   -0.0338076   0.400425     0.00993825  -0.515177     0.177326     0.11543    -0.126941   -0.0630555  -0.546899   -0.129314   -0.357051     -0.371125    -0.148805    -0.141913  
  0.304764    0.408829     0.0506683   0.5531     -0.249052    -0.0322128    0.267736     0.330714    0.0292232   0.362687    0.00974619   0.080777   -0.433037    0.152054     0.102344    -0.345506    -0.417999    -0.660313   -0.227745    0.318596    0.706698   -0.0794332  -0.205985     -0.0251354    0.503576     0.0541259 
  0.0351643  -0.0716003   -0.192842    0.316876    0.689261    -0.587343     0.136304     0.0893924   0.0504168   0.250806   -0.145168     0.393247   -0.805451    0.131155     0.568389    -0.308872    -0.273791     0.0719726   0.711333   -0.134268    0.534668    0.116792    0.0244606    -0.333397     0.027121    -0.109571  
 -0.269466   -0.342235    -0.140249    0.270765   -0.0399372   -0.534613     0.088558    -0.157072    0.174945    0.286032    0.610678    -0.0603303  -0.478807   -0.220005    -0.0433082   -0.101567     0.277652    -0.162245   -0.0826306   0.25103     0.482297    0.175306    0.163846      0.161065     0.251077     0.28118   
 -0.0636153   0.595601     0.220567    0.156442   -0.0291682   -0.40313     -0.0553144    0.120162    0.0836395  -0.368381    0.274582     0.135283    0.0666679  -0.258063    -0.158563    -0.246824     0.00404843  -0.156409   -0.0398161  -0.156582   -0.0369564   0.335617    0.330179      0.195245    -0.00669318   0.409762  
 -0.0977369  -0.165287    -0.235425   -0.42954    -0.174897     0.189944     0.161849    -0.135437   -0.292888   -0.480295   -0.401196    -0.12514    -0.200886    0.0891729    0.174552     0.0403738   -0.141283    -0.0846062  -0.327718    0.874814    0.498334   -0.892628    0.335022     -0.55693     -0.672316    -0.422529  
 -0.230992   -0.658002    -0.201344    0.223035   -0.28132      0.204597     0.136771     0.189652   -0.0158437   0.480878   -0.197171    -0.700044    0.181629   -0.19684      0.0301488    0.329684    -0.658136     0.163153    0.164933    0.574894    0.528521   -0.261025   -0.00991713    0.605879     0.29211     -0.488719  
 -0.0698804  -0.0524064    0.0860922  -0.0237393  -0.0992565    0.263191    -0.0962203    0.029667   -0.145578   -0.14589     0.180378    -0.193006   -0.0665982  -0.188528     0.229034     0.0990291    0.0827442   -0.0882413   0.0585916  -0.0513426   0.214202    0.017183   -0.0518396    -0.0717056    0.00887246  -0.0176675 
  0.130998    0.669684    -0.162826    0.0459575   0.262972    -0.823504    -0.319429    -0.302011    0.252916    0.219652   -0.188999     0.0584842   0.2619     -0.0325467   -0.405121    -0.00465856   0.0368125   -0.0633734  -0.177796    0.254258   -0.0713901  -0.367762    0.000605439  -0.0237153    0.442861     0.00988915
 -0.227592    0.00356846  -0.154174   -0.10934     0.269644     0.0580245    0.37105     -0.0977052   0.233269    0.161893   -0.42956     -0.551279    0.494715   -0.248017    -0.822965     0.345317    -0.393283    -0.484245   -0.271282   -0.692189    0.561234    1.0577     -0.153828     -0.264611     0.377864    -0.362545  
 -0.277204   -0.537818    -0.324454    0.413179   -0.328403     0.456383     0.714386     0.0486335   0.147899   -0.489661   -0.0967059   -0.367802    0.0462621  -0.108265    -0.339826     0.149778     0.224691    -0.385307   -0.707979    0.495966   -0.748077    0.338731   -0.679873     -0.00335821   0.0411569   -0.177102  
  0.325078   -0.206374     0.135876    0.230673    0.180728     0.212143     0.240865     0.148605    0.138172   -0.466023   -0.00288408   0.0580168  -0.0585657  -0.430294     0.908639     0.278241     0.150063    -0.551224    0.646947   -0.0594693  -0.301       0.0303745  -0.657318      0.401909     0.418455     0.00767843
  0.151623   -0.515593    -0.194084    0.338303    0.510234     0.126622     0.0535603   -0.12565    -0.0600779   0.275806    0.367379     0.298536    0.331284    0.65566      0.196165     0.318795     0.098116    -0.207967    0.0854025   0.421412   -0.547614   -0.255656   -0.130591      0.226047     0.709787     0.0633823 
 -0.119097    0.0680938    0.274458   -0.47114     0.151655    -0.244598     0.207351    -0.313863    0.0670264  -0.637049    0.229051     0.395203   -0.389588    0.00446097  -0.0741738   -0.301772     0.446519    -0.521495   -0.194045   -0.190495   -0.293616    0.0872056   0.131187     -0.939833     0.0645999    0.327776  
  0.156403    0.354643     0.211311   -0.29692     0.155233     0.302499    -0.511673    -0.432488   -0.241006   -0.437198    0.173352     0.432311    0.945111   -0.134177    -0.265452     0.0939555    0.370479     0.289761   -0.447753   -0.252564   -0.652067   -0.107847   -0.153118     -0.304       -0.299842     0.0836275 
 -0.139989   -0.260545     0.279811    0.151504   -0.409291     0.643975     0.410675     0.614906   -0.455563    0.0799951   0.332449     0.200666   -0.0926522   0.192232     0.237552    -0.244156     0.419525     0.331386   -0.173773   -0.301025    0.145531    0.754135   -0.0402665     0.0383253   -0.196227     0.242764  
  0.475932    0.509251    -0.144621   -0.364736    0.0331592    0.0588454    0.121472     0.894817   -0.041576   -0.19536     0.00779504  -0.147406    0.314067   -0.477972     0.168518    -0.33879     -0.344742     0.206616    0.0354676  -0.573161    0.0751351   0.311196    0.309711     -0.0349209    0.151353     0.422419  
 -0.152649    0.227772     0.0185631   0.182447   -0.13499     -0.0311532   -0.127796     0.440979    0.651158   -0.363977   -0.407727    -0.186104   -0.384353    0.625931    -0.414061    -0.358714    -0.458764    -0.0780178   0.574132    0.306386    0.25678     0.397128    0.196103      0.329888    -1.13691     -0.368214  
  0.087427   -0.0969008    0.295914    0.141391    0.225423    -0.153421     0.464447    -0.0567862   0.683618    0.239309   -0.0226684    0.868741    0.0446942   0.573603    -0.209794    -0.691169    -0.286802    -0.0344037  -0.0762226  -0.358058   -0.720797    0.653352    0.0307045     0.844907    -0.394291     0.136604  
 -0.953205   -0.255599    -0.299835    0.189592    0.106456    -0.0608086    0.197297    -0.358277    0.18662     0.295634   -0.504414     0.726946   -0.28709     0.315791     0.039061    -0.130593     0.496875    -0.0728659  -0.586227    0.118358   -0.569811   -0.163391   -0.116609     -0.373719    -0.418937    -0.478212  
  0.317155    0.0455852    0.0521069   0.117469   -0.670793    -0.0847503   -0.202824     0.311183   -0.0359908  -0.494999    0.478325     0.564585   -0.386383    0.464525     0.640508    -0.0425989    0.407346     0.400729    0.551604    0.658071   -0.43624    -0.339082    0.138641      0.312137    -0.293657     0.46531   [ Info: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.402874
[ Info: iteration 2, average log likelihood -1.402864
[ Info: iteration 3, average log likelihood -1.402855
[ Info: iteration 4, average log likelihood -1.402847
[ Info: iteration 5, average log likelihood -1.402838
[ Info: iteration 6, average log likelihood -1.402830
[ Info: iteration 7, average log likelihood -1.402822
[ Info: iteration 8, average log likelihood -1.402814
[ Info: iteration 9, average log likelihood -1.402807
[ Info: iteration 10, average log likelihood -1.402799
┌ Info: EM with 100000 data points 10 iterations avll -1.402799
└ 59.0 data points per parameter
kind full, method kmeans
[ Info: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       9.122349e+05
      1       6.961355e+05      -2.160994e+05 |       32
      2       6.812874e+05      -1.484810e+04 |       32
      3       6.762371e+05      -5.050288e+03 |       32
      4       6.735677e+05      -2.669478e+03 |       32
      5       6.718206e+05      -1.747105e+03 |       32
      6       6.706091e+05      -1.211449e+03 |       32
      7       6.697423e+05      -8.667821e+02 |       32
      8       6.691027e+05      -6.396125e+02 |       32
      9       6.685826e+05      -5.201032e+02 |       32
     10       6.681461e+05      -4.365455e+02 |       32
     11       6.678052e+05      -3.408217e+02 |       32
     12       6.675312e+05      -2.740715e+02 |       32
     13       6.672746e+05      -2.565994e+02 |       32
     14       6.670186e+05      -2.560234e+02 |       32
     15       6.667715e+05      -2.470017e+02 |       32
     16       6.665463e+05      -2.252433e+02 |       32
     17       6.663413e+05      -2.050349e+02 |       32
     18       6.661440e+05      -1.972857e+02 |       32
     19       6.659780e+05      -1.659439e+02 |       32
     20       6.658232e+05      -1.548220e+02 |       32
     21       6.656774e+05      -1.458034e+02 |       32
     22       6.655488e+05      -1.286428e+02 |       32
     23       6.654323e+05      -1.165093e+02 |       32
     24       6.653340e+05      -9.822927e+01 |       32
     25       6.652446e+05      -8.948194e+01 |       32
     26       6.651650e+05      -7.956432e+01 |       32
     27       6.650909e+05      -7.412533e+01 |       32
     28       6.650198e+05      -7.105278e+01 |       32
     29       6.649445e+05      -7.529817e+01 |       32
     30       6.648842e+05      -6.026526e+01 |       32
     31       6.648334e+05      -5.089173e+01 |       32
     32       6.647859e+05      -4.749309e+01 |       32
     33       6.647302e+05      -5.562840e+01 |       32
     34       6.646815e+05      -4.876143e+01 |       32
     35       6.646365e+05      -4.501014e+01 |       32
     36       6.646000e+05      -3.645112e+01 |       32
     37       6.645640e+05      -3.598505e+01 |       32
     38       6.645302e+05      -3.384698e+01 |       32
     39       6.644959e+05      -3.424299e+01 |       32
     40       6.644640e+05      -3.191745e+01 |       32
     41       6.644310e+05      -3.301195e+01 |       32
     42       6.643934e+05      -3.757673e+01 |       32
     43       6.643610e+05      -3.246769e+01 |       32
     44       6.643306e+05      -3.037530e+01 |       32
     45       6.642961e+05      -3.445989e+01 |       32
     46       6.642631e+05      -3.299313e+01 |       32
     47       6.642360e+05      -2.709846e+01 |       32
     48       6.642111e+05      -2.489977e+01 |       32
     49       6.641877e+05      -2.344110e+01 |       32
     50       6.641651e+05      -2.257983e+01 |       32
K-means terminated without convergence after 50 iterations (objv = 664165.1139139216)
┌ Info: K-means with 32000 data points using 50 iterations
└ 37.0 data points per parameter
[ Info: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.415146
[ Info: iteration 2, average log likelihood -1.410007
[ Info: iteration 3, average log likelihood -1.408644
[ Info: iteration 4, average log likelihood -1.407650
[ Info: iteration 5, average log likelihood -1.406598
[ Info: iteration 6, average log likelihood -1.405590
[ Info: iteration 7, average log likelihood -1.404854
[ Info: iteration 8, average log likelihood -1.404413
[ Info: iteration 9, average log likelihood -1.404154
[ Info: iteration 10, average log likelihood -1.403985
[ Info: iteration 11, average log likelihood -1.403863
[ Info: iteration 12, average log likelihood -1.403767
[ Info: iteration 13, average log likelihood -1.403688
[ Info: iteration 14, average log likelihood -1.403621
[ Info: iteration 15, average log likelihood -1.403562
[ Info: iteration 16, average log likelihood -1.403510
[ Info: iteration 17, average log likelihood -1.403464
[ Info: iteration 18, average log likelihood -1.403422
[ Info: iteration 19, average log likelihood -1.403384
[ Info: iteration 20, average log likelihood -1.403349
[ Info: iteration 21, average log likelihood -1.403317
[ Info: iteration 22, average log likelihood -1.403287
[ Info: iteration 23, average log likelihood -1.403260
[ Info: iteration 24, average log likelihood -1.403234
[ Info: iteration 25, average log likelihood -1.403211
[ Info: iteration 26, average log likelihood -1.403188
[ Info: iteration 27, average log likelihood -1.403167
[ Info: iteration 28, average log likelihood -1.403147
[ Info: iteration 29, average log likelihood -1.403128
[ Info: iteration 30, average log likelihood -1.403110
[ Info: iteration 31, average log likelihood -1.403092
[ Info: iteration 32, average log likelihood -1.403075
[ Info: iteration 33, average log likelihood -1.403059
[ Info: iteration 34, average log likelihood -1.403044
[ Info: iteration 35, average log likelihood -1.403029
[ Info: iteration 36, average log likelihood -1.403015
[ Info: iteration 37, average log likelihood -1.403001
[ Info: iteration 38, average log likelihood -1.402988
[ Info: iteration 39, average log likelihood -1.402975
[ Info: iteration 40, average log likelihood -1.402963
[ Info: iteration 41, average log likelihood -1.402951
[ Info: iteration 42, average log likelihood -1.402940
[ Info: iteration 43, average log likelihood -1.402928
[ Info: iteration 44, average log likelihood -1.402918
[ Info: iteration 45, average log likelihood -1.402907
[ Info: iteration 46, average log likelihood -1.402897
[ Info: iteration 47, average log likelihood -1.402888
[ Info: iteration 48, average log likelihood -1.402878
[ Info: iteration 49, average log likelihood -1.402869
[ Info: iteration 50, average log likelihood -1.402860
┌ Info: EM with 100000 data points 50 iterations avll -1.402860
└ 59.0 data points per parameter
32×26 Array{Float64,2}:
  0.141033    0.482318     0.0568005     0.149629    0.186454     0.136876   -0.129033     0.0867224   0.255173   -0.276497   -0.197715    0.841805    0.217164     0.330263    -0.200719   -0.332004    0.142825    0.332016   -0.324882    -0.251664    -0.566201    0.0278328  -0.0432997   -0.324611    -0.760728   -0.274941  
  0.202189    0.0682381    0.44771      -0.376907   -0.259065     0.219932   -0.624567    -0.496453   -0.190583   -0.132522    0.420229   -0.0994931   0.919635    -0.554252    -0.363532    0.370334    0.368125    0.104501   -0.4805      -0.36467     -0.264524    0.0728215   0.105126     0.382789    -0.188158    0.118751  
  0.36417     0.58391      0.261336      0.170384   -0.70836      0.266885   -0.10418      1.00258     0.247642    0.155908   -0.356933   -0.282019    0.169069     0.543099    -0.182996    0.0935639  -0.599514    0.139012    0.0866178    0.469238     0.122469    0.108839    0.110697     0.541242    -0.515806   -0.177432  
 -0.191882   -0.233164    -0.0573576     0.0285319  -0.0605203   -0.261289    0.156617     0.139       0.117867    0.0900302   0.363757   -0.220501   -0.425294    -0.3573       0.0977469  -0.178594   -0.145446   -0.298896   -0.0257278    0.0683316    0.849873    0.130191    0.272268     0.14882      0.17983     0.310255  
  0.219321   -0.398145    -0.159         0.131212    0.111087     0.200103    0.25319     -0.0127955   0.0929394   0.0872489  -0.243274   -0.110341    0.175572     0.242445     0.0833603   0.375396   -0.310598   -0.0992514   0.245655     0.339699    -0.166721   -0.0518428  -0.382321     0.264577     0.138484   -0.372728  
  0.156022   -0.8599      -0.234602     -0.64883     0.502917     0.394624   -0.385114    -0.683915   -0.563548    0.70266    -0.13802    -0.224956    0.370596    -0.164323     0.250571    0.410979   -0.112427   -0.550694   -0.446179     0.0406819    0.0538567  -0.641081    0.0626839    0.0591256    0.465425    0.19367   
  0.0524543   0.384584     0.0275663     0.459124   -0.57316     -0.355423    0.122325     0.436494    0.0901313  -0.754627    0.197267    0.277437   -0.0113765   -0.205189     0.0655633  -0.226892    0.423151    0.229258    0.43524     -0.161129    -0.341336    0.628503    0.064763     0.276001    -0.175691    0.548329  
 -0.0759635  -0.144074    -0.0882574    -0.154475    0.0940501    0.0365057  -0.0486807   -0.0202695  -0.209089    0.179299   -0.0890389  -0.593562    0.201244    -0.374959     0.0170797   0.271465   -0.130363   -0.0283824  -0.0706279    0.0861619    0.395234   -0.0926608   0.132453    -0.0641389    0.373506   -0.161846  
 -0.190399   -0.561515    -0.310772      0.433604   -0.124289     0.753917    0.555881    -0.0856105   0.52354    -0.425844    0.160619    0.191302   -0.315148     0.272297     0.0857534   0.0453115  -0.237735   -0.302279   -0.39032      0.612313    -0.650229    0.336064   -0.697166     0.359586    -0.282558   -0.410626  
  0.0217706   0.0693518    0.0277113    -0.0150465   0.00482398  -0.0160501  -0.0139114    0.0234348   0.105865   -0.140184    0.0273073   0.086236    0.0441362   -0.0231441    0.0302444  -0.0710194   0.0339386  -0.0797664  -0.0664812   -0.0405739   -0.0716428   0.0288376  -0.028524     0.00472937  -0.0276187   0.0104798 
  0.10547     0.29869     -0.038328     -0.0600796   0.566812    -0.320238   -0.545048    -0.232186   -0.211013    0.549577   -0.172183    0.183761   -0.369661    -0.47947      0.426405   -0.137709    0.105422    0.267283    0.309684    -0.484267     0.735115   -0.386035   -0.130381    -0.358194     0.117596   -0.22993   
  0.129963   -0.551711     0.255165      0.129832    0.103663     0.115742   -0.0741579   -0.101855    0.10089    -0.0384625   0.423801    0.0145887  -0.0324544   -0.332455     0.64042     0.817676    0.399765   -0.137731    0.602977    -0.0800519   -0.242854    0.600868   -0.465304     0.696535     0.722982    0.273087  
 -0.130546    0.213119    -0.062284      0.297246   -0.0391525   -0.481499    0.28645     -0.415832    0.506706    0.530771   -0.374822   -0.136238   -0.0281353    0.178771    -0.624544   -0.171332   -0.187102   -0.280553   -0.419997     0.5445       0.228168   -0.285138    0.0389708   -0.130941     0.157477   -0.622282  
  0.468086    0.442402     0.0278658     0.673092    0.154293    -0.10105    -0.215367     0.371374   -0.288556   -0.106032   -0.534208    0.0427254  -0.00668521   0.855097    -0.506441    0.540623   -0.34969     0.405361    0.508706     0.275462    -0.180993   -0.161935   -0.221932     0.0479347    0.635449    0.134409  
  0.0196587   0.0558724    0.266921     -0.0642099  -0.376562     0.637878    0.30951      0.719607   -0.314553    0.0126555   0.227816    0.131909    0.019894    -0.0072196    0.313566   -0.605777    0.124216    0.20357    -0.221613    -0.534118     0.197969    0.489574    0.071745     0.137785    -0.33975     0.319126  
 -0.190475    0.372452     0.0188538     0.0275039  -0.330959     0.189255   -0.732073    -0.346275    0.105574    0.365764   -0.492482   -0.381384    0.258254     0.126143    -0.362876    0.199254    0.0108529   0.542576   -0.281078     0.214058     0.11133    -0.579542   -0.404131     0.192029    -0.375091   -0.986973  
 -0.187116    0.00184074  -0.040328     -0.38511     0.392568    -0.336445   -0.00883577  -0.503155   -0.266844   -0.394992    0.71604     0.345451    0.0315486   -0.37899     -0.130048   -0.261262    0.505568   -0.428813   -0.237961    -0.0941501   -0.0377858  -0.056176    0.00503477  -0.617918     0.338337    0.731998  
 -0.123319    0.0206676   -0.0775821     0.031776   -0.545505     0.0553084   0.0594798    0.168847   -0.0947628  -0.386435    0.136634    0.308758   -0.343955     0.330972     0.205151   -0.166854    0.252814    0.178524   -0.0118073    0.442868    -0.0773431  -0.132927    0.200463    -0.245664    -0.546325    0.165534  
 -0.0627064  -0.191562     0.0664916     0.439898   -0.0196352    0.286338    0.386103     0.0602568  -0.460767    0.211924    0.186426    0.0602717  -0.0539654    0.285948     0.0575265   0.0278771   0.391202   -0.0937074  -0.399842     0.132242    -0.339045    0.171607   -0.175975    -0.562982     0.575007    0.274631  
  0.0366324   0.00354229  -0.0407377     0.223072   -0.0727958   -0.446953   -0.232082    -0.156643   -0.099153   -0.147247    0.304901    0.431817   -0.0536562    0.0698411    0.446859   -0.322886    0.557981    0.0503519   0.190977     0.835479    -0.594849   -0.732868   -0.137285     0.453955    -0.114565    0.0385124 
 -0.0669309  -0.0722116    0.125267     -0.179278   -0.201144    -0.0787963  -0.468509     0.103839   -0.492096    0.259512   -0.165115   -0.293146    0.0478017    0.0945339   -0.291316    0.413691    0.159222    0.53313     0.0229744   -0.0321344    0.279922    0.0788364   0.436662    -0.223637    -0.0241237   0.0872994 
  0.0512741  -0.0280434   -0.226417      0.435618    0.456569    -0.413689    0.823947     0.30317     0.172119    0.118982   -0.221346    0.294583   -0.640173     0.239925     0.651423   -0.465098   -0.396472   -0.269873    0.508949     0.131242     0.223515    0.315924   -0.00896229  -0.394907     0.200412   -0.084985  
 -0.0347809   0.835391    -0.000845443  -0.452483    0.50462     -0.717835   -0.230175    -0.151274    0.128499   -0.125156   -0.256137   -0.364013    0.394537    -0.461365    -0.173361    0.225592   -0.308393   -0.291476   -0.00261439   0.111855    -0.0669763  -0.134992    0.437881     0.145968     0.323746    0.356914  
  0.187888    0.327335     0.0357704     0.337679    0.212503    -0.324199    0.04142      0.257007    0.590999    0.474559    0.681683    0.58964    -0.187927     0.393379    -0.190866   -0.47725    -0.394438   -0.179814   -0.185089    -0.107738    -0.274336    0.44958     0.106745     0.60949      0.241033    0.53482   
 -0.299952   -0.163274     0.0132376     0.0161375  -0.354791     0.197946   -0.342282     0.0462384  -0.28364    -0.0720918   0.418781   -0.813825   -0.162186    -0.104783     0.123387    0.464228    0.0252644  -0.133347   -0.0152637    0.583634     0.506872   -0.57058     0.0776384   -0.319985     0.551303    0.00338764
 -0.198376   -0.0180451   -0.200739      0.40616     0.254554    -0.201414   -0.597907     0.0291606   0.631556   -0.621894   -0.0995089  -0.0861796  -0.806618     0.829139    -0.407929   -0.293336   -0.326521   -0.0804389   0.81824      0.035463     0.525557    0.19228     0.108603    -0.0887914   -0.651522   -0.3743    
 -0.618283   -0.240094     0.218973     -0.119425    0.418092    -0.419472    0.3325      -0.592643    0.414306   -0.0310643  -0.635       0.772246   -0.158976     0.510968    -0.0861243  -0.191186    0.382832   -0.360779   -0.17114     -0.172652    -0.844332    0.0317986  -0.2276      -0.333151    -0.328157   -0.422962  
 -0.0296055  -0.0933057    0.123744     -0.595993   -0.166677     0.454806    0.00230996  -0.313874   -0.341567   -0.681403   -0.493742   -0.228051    0.362757     0.00270003   0.0873737   0.387848    0.0165354   0.200895    0.2823      -0.0329315   -0.274999   -0.20736     0.140533    -0.29094     -0.480093   -0.124105  
  0.0869456  -0.0195177    0.455972      0.0113297  -0.179772     0.141488   -0.168538    -0.267824    0.0206084  -0.412602    0.0797617  -0.069792    0.0538259   -1.01844      0.070601   -0.111256   -0.0129862  -1.25147     0.0231867    0.00412088   0.397226   -0.225472   -0.217893     0.610107    -0.382544   -0.335835  
  0.290556   -0.448214    -0.374019     -0.266221    0.204448    -0.743993    0.290109     0.177579    0.553683    0.342026   -0.602592    0.357933    0.675954    -0.151052    -0.0916232  -0.324304   -0.396567    0.656376    0.787134     0.0415151    0.166634    0.0780323   0.645177     0.808397    -1.10033    -0.313085  
  0.665254    0.417574    -0.00965171   -0.216056    0.329262     0.101951   -0.0661666    0.38056     0.249693   -0.6451     -0.122434    0.195439   -0.142542    -0.221534     0.57184     0.0143069   0.339396   -0.302296   -0.096179    -0.315976    -0.246614   -0.797793   -0.560555    -0.506709     0.641428   -0.127784  
 -0.0697273   0.102097    -0.142318     -0.141371    0.223334     0.128354    0.307296     0.0599237   0.104814   -0.0378023  -0.43552    -0.545095    0.592848    -0.317179    -0.667504    0.25317    -0.253336   -0.337375   -0.20082     -0.7784       0.314521[ Info: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
    0.979561   -0.361041    -0.229192     0.45856    -0.236115  [ Info: iteration 1, average log likelihood -1.402852
[ Info: iteration 2, average log likelihood -1.402844
[ Info: iteration 3, average log likelihood -1.402836
[ Info: iteration 4, average log likelihood -1.402828
[ Info: iteration 5, average log likelihood -1.402821
[ Info: iteration 6, average log likelihood -1.402813
[ Info: iteration 7, average log likelihood -1.402806
[ Info: iteration 8, average log likelihood -1.402799
[ Info: iteration 9, average log likelihood -1.402793
[ Info: iteration 10, average log likelihood -1.402786
┌ Info: EM with 100000 data points 10 iterations avll -1.402786
└ 59.0 data points per parameter
[ Info: Initializing GMM, 2 Gaussians diag covariance 2 dimensions using 900 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.678561e+05
      1       2.230230e+04      -1.455538e+05 |        2
      2       7.823675e+03      -1.447862e+04 |        0
      3       7.823675e+03       0.000000e+00 |        0
K-means converged with 3 iterations (objv = 7823.67549422947)
┌ Info: K-means with 900 data points using 3 iterations
└ 150.0 data points per parameter
[ Info: Running 10 iterations EM on full cov GMM with 2 Gaussians in 2 dimensions
[ Info: iteration 1, average log likelihood -2.043155
[ Info: iteration 2, average log likelihood -2.043154
[ Info: iteration 3, average log likelihood -2.043154
[ Info: iteration 4, average log likelihood -2.043154
[ Info: iteration 5, average log likelihood -2.043154
[ Info: iteration 6, average log likelihood -2.043154
[ Info: iteration 7, average log likelihood -2.043154
[ Info: iteration 8, average log likelihood -2.043154
[ Info: iteration 9, average log likelihood -2.043154
[ Info: iteration 10, average log likelihood -2.043154
┌ Info: EM with 900 data points 10 iterations avll -2.043154
└ 81.8 data points per parameter
   Testing GaussianMixtures tests passed 
</pre>
      </div>

    <h3>Results with Julia v1.4.0-DEV-1ee8965da4</h3>

    <p>
      Testing was <strong>successful</strong>.
      Last evaluation was 1 day, 10 hours ago and took 8 minutes, 40 seconds.
    </p>

    <p>
      Click <a href="/home/tim/Julia/pkg/NewPkgEval/site/build/logs/GaussianMixtures/1.4.0-DEV-1ee8965da4.log">here</a> to download the log file.
      
    </p>

      <button class="collapsible">Click here to show the log contents.</button>
      <div class="content">
      <pre> Resolving package versions...
 Installed Missings ─────────── v0.4.3
 Installed GaussianMixtures ─── v0.3.0
 Installed Distances ────────── v0.8.2
 Installed Rmath ────────────── v0.5.1
 Installed NearestNeighbors ─── v0.4.3
 Installed BinDeps ──────────── v0.8.10
 Installed Parameters ───────── v0.12.0
 Installed Clustering ───────── v0.13.3
 Installed SortingAlgorithms ── v0.3.1
 Installed DataAPI ──────────── v1.1.0
 Installed QuadGK ───────────── v2.1.1
 Installed Arpack ───────────── v0.3.1
 Installed CMake ────────────── v1.1.2
 Installed DataStructures ───── v0.17.6
 Installed Distributions ────── v0.21.8
 Installed Blosc ────────────── v0.5.1
 Installed ScikitLearnBase ──── v0.5.0
 Installed CMakeWrapper ─────── v0.2.3
 Installed URIParser ────────── v0.4.0
 Installed StatsFuns ────────── v0.9.0
 Installed StatsBase ────────── v0.32.0
 Installed StaticArrays ─────── v0.12.1
 Installed PDMats ───────────── v0.9.10
 Installed OrderedCollections ─ v1.1.0
 Installed HDF5 ─────────────── v0.12.5
 Installed JLD ──────────────── v0.9.1
 Installed LegacyStrings ────── v0.4.1
 Installed BinaryProvider ───── v0.5.8
 Installed SpecialFunctions ─── v0.8.0
 Installed FileIO ───────────── v1.0.7
 Installed Compat ───────────── v2.2.0
  Updating `~&#x2F;.julia&#x2F;environments&#x2F;v1.4&#x2F;Project.toml`
  [cc18c42c] + GaussianMixtures v0.3.0
  Updating `~&#x2F;.julia&#x2F;environments&#x2F;v1.4&#x2F;Manifest.toml`
  [7d9fca2a] + Arpack v0.3.1
  [9e28174c] + BinDeps v0.8.10
  [b99e7846] + BinaryProvider v0.5.8
  [a74b3585] + Blosc v0.5.1
  [631607c0] + CMake v1.1.2
  [d5fb7624] + CMakeWrapper v0.2.3
  [aaaa29a8] + Clustering v0.13.3
  [34da2185] + Compat v2.2.0
  [9a962f9c] + DataAPI v1.1.0
  [864edb3b] + DataStructures v0.17.6
  [b4f34e82] + Distances v0.8.2
  [31c24e10] + Distributions v0.21.8
  [5789e2e9] + FileIO v1.0.7
  [cc18c42c] + GaussianMixtures v0.3.0
  [f67ccb44] + HDF5 v0.12.5
  [4138dd39] + JLD v0.9.1
  [1b4a561d] + LegacyStrings v0.4.1
  [e1d29d7a] + Missings v0.4.3
  [b8a86587] + NearestNeighbors v0.4.3
  [bac558e1] + OrderedCollections v1.1.0
  [90014a1f] + PDMats v0.9.10
  [d96e819e] + Parameters v0.12.0
  [1fd47b50] + QuadGK v2.1.1
  [79098fc4] + Rmath v0.5.1
  [6e75b9c4] + ScikitLearnBase v0.5.0
  [a2af1166] + SortingAlgorithms v0.3.1
  [276daf66] + SpecialFunctions v0.8.0
  [90137ffa] + StaticArrays v0.12.1
  [2913bbd2] + StatsBase v0.32.0
  [4c63d2b9] + StatsFuns v0.9.0
  [30578b45] + URIParser v0.4.0
  [2a0f44e3] + Base64 
  [ade2ca70] + Dates 
  [8bb1440f] + DelimitedFiles 
  [8ba89e20] + Distributed 
  [b77e0a4c] + InteractiveUtils 
  [76f85450] + LibGit2 
  [8f399da3] + Libdl 
  [37e2e46d] + LinearAlgebra 
  [56ddb016] + Logging 
  [d6f4376e] + Markdown 
  [a63ad114] + Mmap 
  [44cfe95a] + Pkg 
  [de0858da] + Printf 
  [9abbd945] + Profile 
  [3fa0cd96] + REPL 
  [9a3f8284] + Random 
  [ea8e919c] + SHA 
  [9e88b42a] + Serialization 
  [1a1011a3] + SharedArrays 
  [6462fe0b] + Sockets 
  [2f01184e] + SparseArrays 
  [10745b16] + Statistics 
  [4607b0f0] + SuiteSparse 
  [8dfed614] + Test 
  [cf7118a7] + UUIDs 
  [4ec0a83e] + Unicode 
  Building Rmath ───────────→ `~&#x2F;.julia&#x2F;packages&#x2F;Rmath&#x2F;4wt82&#x2F;deps&#x2F;build.log`
  Building CMake ───────────→ `~&#x2F;.julia&#x2F;packages&#x2F;CMake&#x2F;nSK2r&#x2F;deps&#x2F;build.log`
  Building Arpack ──────────→ `~&#x2F;.julia&#x2F;packages&#x2F;Arpack&#x2F;cu5By&#x2F;deps&#x2F;build.log`
  Building Blosc ───────────→ `~&#x2F;.julia&#x2F;packages&#x2F;Blosc&#x2F;lzFr0&#x2F;deps&#x2F;build.log`
  Building HDF5 ────────────→ `~&#x2F;.julia&#x2F;packages&#x2F;HDF5&#x2F;Zh9on&#x2F;deps&#x2F;build.log`
  Building SpecialFunctions → `~&#x2F;.julia&#x2F;packages&#x2F;SpecialFunctions&#x2F;ne2iw&#x2F;deps&#x2F;build.log`
   Testing GaussianMixtures
Status `&#x2F;tmp&#x2F;jl_AcjKei&#x2F;Manifest.toml`
  [7d9fca2a] Arpack v0.3.1
  [9e28174c] BinDeps v0.8.10
  [b99e7846] BinaryProvider v0.5.8
  [a74b3585] Blosc v0.5.1
  [631607c0] CMake v1.1.2
  [d5fb7624] CMakeWrapper v0.2.3
  [aaaa29a8] Clustering v0.13.3
  [34da2185] Compat v2.2.0
  [9a962f9c] DataAPI v1.1.0
  [864edb3b] DataStructures v0.17.6
  [b4f34e82] Distances v0.8.2
  [31c24e10] Distributions v0.21.8
  [5789e2e9] FileIO v1.0.7
  [cc18c42c] GaussianMixtures v0.3.0
  [f67ccb44] HDF5 v0.12.5
  [4138dd39] JLD v0.9.1
  [1b4a561d] LegacyStrings v0.4.1
  [e1d29d7a] Missings v0.4.3
  [b8a86587] NearestNeighbors v0.4.3
  [bac558e1] OrderedCollections v1.1.0
  [90014a1f] PDMats v0.9.10
  [d96e819e] Parameters v0.12.0
  [1fd47b50] QuadGK v2.1.1
  [79098fc4] Rmath v0.5.1
  [6e75b9c4] ScikitLearnBase v0.5.0
  [a2af1166] SortingAlgorithms v0.3.1
  [276daf66] SpecialFunctions v0.8.0
  [90137ffa] StaticArrays v0.12.1
  [2913bbd2] StatsBase v0.32.0
  [4c63d2b9] StatsFuns v0.9.0
  [30578b45] URIParser v0.4.0
  [2a0f44e3] Base64  [`@stdlib&#x2F;Base64`]
  [ade2ca70] Dates  [`@stdlib&#x2F;Dates`]
  [8bb1440f] DelimitedFiles  [`@stdlib&#x2F;DelimitedFiles`]
  [8ba89e20] Distributed  [`@stdlib&#x2F;Distributed`]
  [b77e0a4c] InteractiveUtils  [`@stdlib&#x2F;InteractiveUtils`]
  [76f85450] LibGit2  [`@stdlib&#x2F;LibGit2`]
  [8f399da3] Libdl  [`@stdlib&#x2F;Libdl`]
  [37e2e46d] LinearAlgebra  [`@stdlib&#x2F;LinearAlgebra`]
  [56ddb016] Logging  [`@stdlib&#x2F;Logging`]
  [d6f4376e] Markdown  [`@stdlib&#x2F;Markdown`]
  [a63ad114] Mmap  [`@stdlib&#x2F;Mmap`]
  [44cfe95a] Pkg  [`@stdlib&#x2F;Pkg`]
  [de0858da] Printf  [`@stdlib&#x2F;Printf`]
  [9abbd945] Profile  [`@stdlib&#x2F;Profile`]
  [3fa0cd96] REPL  [`@stdlib&#x2F;REPL`]
  [9a3f8284] Random  [`@stdlib&#x2F;Random`]
  [ea8e919c] SHA  [`@stdlib&#x2F;SHA`]
  [9e88b42a] Serialization  [`@stdlib&#x2F;Serialization`]
  [1a1011a3] SharedArrays  [`@stdlib&#x2F;SharedArrays`]
  [6462fe0b] Sockets  [`@stdlib&#x2F;Sockets`]
  [2f01184e] SparseArrays  [`@stdlib&#x2F;SparseArrays`]
  [10745b16] Statistics  [`@stdlib&#x2F;Statistics`]
  [4607b0f0] SuiteSparse  [`@stdlib&#x2F;SuiteSparse`]
  [8dfed614] Test  [`@stdlib&#x2F;Test`]
  [cf7118a7] UUIDs  [`@stdlib&#x2F;UUIDs`]
  [4ec0a83e] Unicode  [`@stdlib&#x2F;Unicode`]
[ Info: Testing Data
(100000, -3.7838470242774133e6, [1329.4949956709622, 98670.50500432904], [634.0274684513498 -749.4601063479147 546.7351180177433; -1218.4987165909245 1201.1253629808066 -1131.5144387116384], [[1868.7700091755735 -891.5683568370846 -311.775384609574; -891.5683568370846 2813.244091080158 1583.1051557882697; -311.775384609574 1583.1051557882697 1742.6048476078834], [97907.9075780534 597.6420767107757 120.30104139097678; 597.6420767107757 97195.88815994447 -1640.4734472712717; 120.30104139097675 -1640.4734472712717 98074.34306951116]])
┌ Warning: rmprocs: process 1 not removed
└ @ Distributed &#x2F;buildworker&#x2F;worker&#x2F;package_linux64&#x2F;build&#x2F;usr&#x2F;share&#x2F;julia&#x2F;stdlib&#x2F;v1.4&#x2F;Distributed&#x2F;src&#x2F;cluster.jl:1015
[ Info: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.222820e+03
      1       1.052175e+03      -1.706443e+02 |        4
      2       9.924635e+02      -5.971181e+01 |        2
      3       9.724609e+02      -2.000262e+01 |        0
      4       9.724609e+02       0.000000e+00 |        0
K-means converged with 4 iterations (objv = 972.4609200273935)
┌ Info: K-means with 272 data points using 4 iterations
└ 11.3 data points per parameter
[ Info: Running 0 iterations EM on full cov GMM with 8 Gaussians in 2 dimensions
┌ Info: EM with 272 data points 0 iterations avll -2.077330
└ 5.8 data points per parameter
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = lowerbound(::VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at bayes.jl:221
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;bayes.jl:221
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = lowerbound(::VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at bayes.jl:221
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;bayes.jl:221
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = _broadcast_getindex_evalf at broadcast.jl:630 [inlined]
└ @ Core .&#x2F;broadcast.jl:630
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = lowerbound(::VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at bayes.jl:230
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;bayes.jl:230
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = _broadcast_getindex_evalf at broadcast.jl:630 [inlined]
└ @ Core .&#x2F;broadcast.jl:630
┌ Warning: `lgamma(x::Real)` is deprecated, use `(logabsgamma(x))[1]` instead.
│   caller = _broadcast_getindex_evalf at broadcast.jl:630 [inlined]
└ @ Core .&#x2F;broadcast.jl:630
[ Info: iteration 1, lowerbound -3.739251
[ Info: iteration 2, lowerbound -3.625938
[ Info: iteration 3, lowerbound -3.507457
[ Info: iteration 4, lowerbound -3.361783
[ Info: iteration 5, lowerbound -3.188261
[ Info: iteration 6, lowerbound -2.999489
[ Info: dropping number of Gaussions to 7
[ Info: iteration 7, lowerbound -2.815264
[ Info: iteration 8, lowerbound -2.663356
[ Info: dropping number of Gaussions to 5
[ Info: iteration 9, lowerbound -2.548636
[ Info: dropping number of Gaussions to 4
[ Info: iteration 10, lowerbound -2.462016
[ Info: dropping number of Gaussions to 3
[ Info: iteration 11, lowerbound -2.401205
[ Info: iteration 12, lowerbound -2.357363
[ Info: iteration 13, lowerbound -2.328404
[ Info: iteration 14, lowerbound -2.311182
[ Info: iteration 15, lowerbound -2.307832
[ Info: dropping number of Gaussions to 2
[ Info: iteration 16, lowerbound -2.302918
[ Info: iteration 17, lowerbound -2.299260
[ Info: iteration 18, lowerbound -2.299256
[ Info: iteration 19, lowerbound -2.299254
[ Info: iteration 20, lowerbound -2.299254
[ Info: iteration 21, lowerbound -2.299253
[ Info: iteration 22, lowerbound -2.299253
[ Info: iteration 23, lowerbound -2.299253
[ Info: iteration 24, lowerbound -2.299253
[ Info: iteration 25, lowerbound -2.299253
[ Info: iteration 26, lowerbound -2.299253
[ Info: iteration 27, lowerbound -2.299253
[ Info: iteration 28, lowerbound -2.299253
[ Info: iteration 29, lowerbound -2.299253
[ Info: iteration 30, lowerbound -2.299253
[ Info: iteration 31, lowerbound -2.299253
[ Info: iteration 32, lowerbound -2.299253
[ Info: iteration 33, lowerbound -2.299253
[ Info: iteration 34, lowerbound -2.299253
[ Info: iteration 35, lowerbound -2.299253
[ Info: iteration 36, lowerbound -2.299253
[ Info: iteration 37, lowerbound -2.299253
[ Info: iteration 38, lowerbound -2.299253
[ Info: iteration 39, lowerbound -2.299253
[ Info: iteration 40, lowerbound -2.299253
[ Info: iteration 41, lowerbound -2.299253
[ Info: iteration 42, lowerbound -2.299253
[ Info: iteration 43, lowerbound -2.299253
[ Info: iteration 44, lowerbound -2.299253
[ Info: iteration 45, lowerbound -2.299253
[ Info: iteration 46, lowerbound -2.299253
[ Info: iteration 47, lowerbound -2.299253
[ Info: iteration 48, lowerbound -2.299253
[ Info: iteration 49, lowerbound -2.299253
[ Info: iteration 50, lowerbound -2.299253
[ Info: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
History[Sun Nov 24 00:27:00 2019: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
, Sun Nov 24 00:27:09 2019: K-means with 272 data points using 4 iterations
11.3 data points per parameter
, Sun Nov 24 00:27:11 2019: EM with 272 data points 0 iterations avll -2.077330
5.8 data points per parameter
, Sun Nov 24 00:27:13 2019: GMM converted to Variational GMM
, Sun Nov 24 00:27:22 2019: iteration 1, lowerbound -3.739251
, Sun Nov 24 00:27:22 2019: iteration 2, lowerbound -3.625938
, Sun Nov 24 00:27:22 2019: iteration 3, lowerbound -3.507457
, Sun Nov 24 00:27:22 2019: iteration 4, lowerbound -3.361783
, Sun Nov 24 00:27:22 2019: iteration 5, lowerbound -3.188261
, Sun Nov 24 00:27:22 2019: iteration 6, lowerbound -2.999489
, Sun Nov 24 00:27:22 2019: dropping number of Gaussions to 7
, Sun Nov 24 00:27:22 2019: iteration 7, lowerbound -2.815264
, Sun Nov 24 00:27:22 2019: iteration 8, lowerbound -2.663356
, Sun Nov 24 00:27:22 2019: dropping number of Gaussions to 5
, Sun Nov 24 00:27:22 2019: iteration 9, lowerbound -2.548636
, Sun Nov 24 00:27:22 2019: dropping number of Gaussions to 4
, Sun Nov 24 00:27:22 2019: iteration 10, lowerbound -2.462016
, Sun Nov 24 00:27:22 2019: dropping number of Gaussions to 3
, Sun Nov 24 00:27:22 2019: iteration 11, lowerbound -2.401205
, Sun Nov 24 00:27:22 2019: iteration 12, lowerbound -2.357363
, Sun Nov 24 00:27:22 2019: iteration 13, lowerbound -2.328404
, Sun Nov 24 00:27:22 2019: iteration 14, lowerbound -2.311182
, Sun Nov 24 00:27:22 2019: iteration 15, lowerbound -2.307832
, Sun Nov 24 00:27:22 2019: dropping number of Gaussions to 2
, Sun Nov 24 00:27:22 2019: iteration 16, lowerbound -2.302918
, Sun Nov 24 00:27:22 2019: iteration 17, lowerbound -2.299260
, Sun Nov 24 00:27:22 2019: iteration 18, lowerbound -2.299256
, Sun Nov 24 00:27:22 2019: iteration 19, lowerbound -2.299254
, Sun Nov 24 00:27:22 2019: iteration 20, lowerbound -2.299254
, Sun Nov 24 00:27:22 2019: iteration 21, lowerbound -2.299253
, Sun Nov 24 00:27:22 2019: iteration 22, lowerbound -2.299253
, Sun Nov 24 00:27:22 2019: iteration 23, lowerbound -2.299253
, Sun Nov 24 00:27:22 2019: iteration 24, lowerbound -2.299253
, Sun Nov 24 00:27:22 2019: iteration 25, lowerbound -2.299253
, Sun Nov 24 00:27:22 2019: iteration 26, lowerbound -2.299253
, Sun Nov 24 00:27:22 2019: iteration 27, lowerbound -2.299253
, Sun Nov 24 00:27:22 2019: iteration 28, lowerbound -2.299253
, Sun Nov 24 00:27:22 2019: iteration 29, lowerbound -2.299253
, Sun Nov 24 00:27:22 2019: iteration 30, lowerbound -2.299253
, Sun Nov 24 00:27:23 2019: iteration 31, lowerbound -2.299253
, Sun Nov 24 00:27:23 2019: iteration 32, lowerbound -2.299253
, Sun Nov 24 00:27:23 2019: iteration 33, lowerbound -2.299253
, Sun Nov 24 00:27:23 2019: iteration 34, lowerbound -2.299253
, Sun Nov 24 00:27:23 2019: iteration 35, lowerbound -2.299253
, Sun Nov 24 00:27:23 2019: iteration 36, lowerbound -2.299253
, Sun Nov 24 00:27:23 2019: iteration 37, lowerbound -2.299253
, Sun Nov 24 00:27:23 2019: iteration 38, lowerbound -2.299253
, Sun Nov 24 00:27:23 2019: iteration 39, lowerbound -2.299253
, Sun Nov 24 00:27:23 2019: iteration 40, lowerbound -2.299253
, Sun Nov 24 00:27:23 2019: iteration 41, lowerbound -2.299253
, Sun Nov 24 00:27:23 2019: iteration 42, lowerbound -2.299253
, Sun Nov 24 00:27:23 2019: iteration 43, lowerbound -2.299253
, Sun Nov 24 00:27:23 2019: iteration 44, lowerbound -2.299253
, Sun Nov 24 00:27:23 2019: iteration 45, lowerbound -2.299253
, Sun Nov 24 00:27:23 2019: iteration 46, lowerbound -2.299253
, Sun Nov 24 00:27:23 2019: iteration 47, lowerbound -2.299253
, Sun Nov 24 00:27:23 2019: iteration 48, lowerbound -2.299253
, Sun Nov 24 00:27:23 2019: iteration 49, lowerbound -2.299253
, Sun Nov 24 00:27:23 2019: iteration 50, lowerbound -2.299253
, Sun Nov 24 00:27:23 2019: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
]
α = [178.04509222601382, 95.95490777398619]
β = [178.04509222601382, 95.95490777398619]
m = [4.250300733269911 79.28686694436185; 2.0002292577753704 53.85198717246129]
ν = [180.04509222601382, 97.95490777398619]
W = LinearAlgebra.UpperTriangular{Float64,Array{Float64,2}}[[0.18404155547484857 -0.007644049042327489; 0.0 0.00858170516633361], [0.37587636119483825 -0.008953123827346048; 0.0 0.012748664777409427]]
Kind: diag, size256
nx: 100000 sum(zeroth order stats): 99999.99999999999
avll from stats: -0.9917334430641419
avll from llpg:  -0.9917334430641421
avll direct:     -0.991733443064142
sum posterior: 100000.0
Kind: full, size16
nx: 100000 sum(zeroth order stats): 100000.00000000001
avll from stats: -0.9668203145015637
avll from llpg:  -0.9668203145015637
avll direct:     -0.9668203145015638
sum posterior: 100000.0
32×26 Array{Float64,2}:
  0.053151     0.102472    -0.0115641   -0.0250116    0.0311347   -0.243604     0.126064    -0.118812    -0.15927      -0.12659     -0.162288     -0.0137872    0.0483703    -0.0296866   -0.0509979  -0.0763929   -0.195965     0.0881582    0.178872     0.0080594   -0.0616289    0.000356159   0.0899168   -0.0868207   -0.150284     0.239708
  0.0233471   -0.0730259    0.00396455   0.0532896    0.0362064    0.0136743   -0.0239506   -0.0192556    0.28003       0.0939404    0.0623611    -0.0542718   -0.064678     -0.0505131   -0.022082    0.0323679   -0.0342418   -0.059223     0.0724456   -0.0294492    0.082198    -0.101791      0.0272285   -0.0872019   -0.145847    -0.0890498
  0.0757333    0.00107476  -0.248153     0.0775306   -0.16373      0.129095    -0.0822964    0.0637377   -0.0489903     0.261018     0.127562     -0.133441     0.0210089     0.156924    -0.204954   -0.11603     -0.0159309   -0.00278711   0.0600559   -0.128851    -0.112024     0.0638544    -0.0476924   -0.198914    -0.055989     0.00481369
  0.0405048    0.028693    -0.0479933    0.0544796    0.108615    -0.0903664    0.0804274   -0.144061     0.0594954     0.0297974    0.109986     -0.101448     0.121913      0.0833917   -0.165252    0.0428529   -0.0604424    0.188703    -0.119065     0.0916776   -0.068284    -0.0791995    -0.141321    -0.147707    -0.00425506   0.0709233
 -0.119502    -1.88672e-5   0.0105512    0.0229354   -0.099905     0.030191    -0.272422    -0.0704764    0.0420169    -0.0135604    0.00359072    0.166337    -0.0268357     0.00233084   0.0588901  -0.0608303    0.204       -0.0778854    0.102973    -0.102801     0.289542    -0.0228858    -0.038069    -0.0722114   -0.103704    -0.130828
  0.107873     0.13156     -0.120314     0.107216    -0.0460339   -0.126606    -0.0617373    0.0693455   -0.000829044  -0.141534    -0.0648848    -0.0571202    0.143709     -0.00909412  -0.0897769   0.0401289   -0.0945355    0.0341961   -0.0886721   -0.034481    -0.168797    -0.0799071     0.0906282   -0.098049    -0.0398604    0.0988529
  0.0846979    0.116196    -0.0207402    0.1606       0.130492    -0.186843     0.0681114   -0.0502278   -0.275469      0.0682275   -0.0357694    -0.0818175   -0.0794743    -0.0299641   -0.0102755  -0.152173    -0.151134     0.0339747   -0.123813     0.0983331    0.184729    -0.0821232    -0.0728781    0.0457617   -0.0510522   -0.0799694
  0.070843    -0.043633     0.0608339   -0.0613378    0.139274    -0.0420482    0.0559946   -0.025505     0.0248976     0.163274    -0.00649103   -0.146883    -0.112854      0.164256    -0.0499719  -0.135302    -0.0801712    0.0954658   -0.253595     0.0274511    0.0649625   -0.105278     -0.21342      0.0339642    0.0247698   -0.13607
  0.0302957   -0.0289579   -0.0300124   -0.0481521   -0.106116    -0.00640742   0.109722     0.0468186    0.102155      0.206492     0.108684      0.0906675   -0.194372     -0.216632     0.0695116  -0.0581115    0.104555    -0.0291511    0.0920956    0.0387692    0.108591     0.112622     -0.0474451    0.0802767   -0.0383529    0.0473033
 -0.0445031    0.0382289   -0.0219325   -0.200767     0.0729466    0.0933016   -0.144025    -0.0254958   -0.104706      0.0765733    0.0283881     0.184674     0.0123306     0.0224969    0.041838   -0.179874    -0.0613234    0.0727403   -0.0622955   -0.0270926   -0.0185932    0.104507     -0.00267012   0.0115722    0.191896    -0.117499
 -0.107765    -0.0933755   -0.105889     0.145437    -0.0682851    0.0275568    0.077316     0.0765743   -0.211951      0.139027    -0.0933014    -0.00325555   0.070727     -0.0409544   -0.17131     0.119673    -0.0927616   -0.0583324   -0.0589859   -0.010921    -0.225003    -0.0509915     0.136956    -0.178312     0.177361     0.157522
  0.116268     0.0285935    0.0180247    0.00637617   0.0240707    0.16483      0.033376    -0.0977192   -0.0664036     0.126874    -0.0406206     0.0451504   -0.17309       0.0856099    0.02194    -0.104996    -0.00410498   0.111038    -0.0205816    0.065242    -0.0924355   -0.0545853    -0.0602242    0.168016     0.0488522   -0.0791635
  0.174128     0.0597815    0.0153647    0.0423495    0.0272404    0.0751076   -0.00575192   0.140389     0.0157362     0.051222     0.0562298     0.0317021   -0.013943      0.0242672   -0.0185818  -0.0268798   -0.0188671   -0.124907     0.021457    -0.00875269  -0.0636733    0.0344005    -0.135547    -0.058237     0.116747     0.154452
 -0.0297057   -0.0315068   -0.0822261   -0.0323581   -0.0338824    0.0356029    0.0584649    0.0190528    0.138368      0.0483946   -0.0117352    -0.186799     0.103791     -0.00433599  -0.0883947   0.04768      0.00302606  -0.0559535    0.0291354    0.138009    -0.0277432   -0.0472552     0.148157     0.0140802   -0.103355    -0.114651
 -0.0802069    0.0812633    0.114865     0.0939319   -0.25564     -0.072386    -0.122966    -0.113548     0.0800703     0.0431792    0.0442172     0.030816     0.0442527    -0.0741511   -0.106296   -0.19306     -0.0853829   -0.0636014   -0.108007     0.186272    -0.224092    -0.053815      0.169445    -0.223573    -0.0161332    0.0103435
  0.0401579    0.0909836   -0.0672817   -0.0258208   -0.11368     -0.165725    -0.163595    -0.0817298    0.0134235     0.027363    -0.0065178     0.0158399   -0.0224441     0.0789202   -0.18706    -0.117107    -0.187123     0.0184195    0.128458     0.132372     0.0380547   -0.173106      0.0797874    0.139503    -0.0705332    0.0162009
 -0.00810266  -0.0400915   -0.0768368   -0.11784     -0.0820377    0.0355318   -0.0188284    0.0019343   -0.0883106    -0.157119     0.130282      0.0585783   -0.132091     -0.0522172    0.0700097   0.169131    -0.0473463   -0.169739     0.143021    -0.0180904   -0.109168     0.0396942     0.0220374    0.00354831  -0.0283608    0.0271535
  0.0409186    0.0527208   -0.0361792   -0.0940386    0.111183     0.0926391   -0.0236173    0.0683264    0.112493     -0.128772    -0.100657     -0.05249      0.118164     -0.0608492    0.0188875  -0.00420308   0.00558425  -0.210424     0.0860799    0.0109967   -0.0652219    0.107085     -0.0649688   -0.0288476    0.0542921   -0.0743656
 -0.096319    -0.0341896   -0.0125899   -0.00351249   0.0864771    0.0901105    0.192562    -0.0265445    0.0501315    -0.00606077  -0.199527      0.054277     0.107339     -0.243644     0.0393286  -0.141409     0.0198887   -0.222743    -0.126302    -0.0339789   -0.0268694    0.0361002     0.0148308   -0.00172861   0.094772    -0.0978117
  0.12253      0.0415512    0.0730616    0.0213588    0.201806    -0.029795    -0.0347392   -0.0123693   -0.0378276    -0.0415501   -0.0447899    -0.00610378   0.230944      0.0223985   -0.17836     0.0225531   -0.0688882    0.0968654   -0.0768653    0.0104313    0.0285903    0.00709348   -0.150571    -0.156734     0.131516     0.0878431
 -0.0385624   -0.0638333    0.0129498   -0.0338887   -0.148885     0.0595281   -0.0468493    0.041778    -0.05039      -0.115852     0.0243707     0.0338688    0.0436252    -0.0239226    0.118477    0.0735811   -0.00133678   0.0601307    0.0769318    0.121584     0.0295958    0.147486     -0.0420363    0.0211042   -0.0159801   -0.0973606
 -0.0432826   -0.0486336   -0.0884243    0.0275323    0.0168493   -0.100406    -0.00627803  -0.0248329    0.0419525     0.226804    -0.01199      -0.0860406    0.0359051     0.0741413   -0.184489   -0.0522154   -0.0714091    0.0564924    0.174038     0.0497797    0.0593441    0.137878     -0.00501584  -0.0391832   -0.0522138    0.053917
 -0.0199176    0.178917    -0.00728893   0.00198314   0.0952919    0.207787     0.0380518   -0.0668858   -0.0292529     0.15254     -0.0391625     0.106492     0.0202215     0.0527359    0.175168    0.013152     0.0576651   -0.114869     0.149914    -0.178677     0.0281243    0.107413      0.00632526  -0.119757    -0.244149    -0.0934015
 -0.180639     0.030037     0.099084    -0.105558     0.0795998   -0.0584748    0.0866475   -0.119665     0.044691     -0.24745     -0.0920803    -0.0631799   -0.103373     -0.146917     0.0736177   0.00143814   0.145402     0.0353816    0.139346    -0.00281988   0.0824515   -0.0396615     0.0949347    0.137813    -0.115585    -0.199155
  0.117414     0.0527527    0.0457948    0.193914     0.140408     0.12801     -0.0435209    0.0565611   -0.218777     -0.0275197    0.0457486     0.0323382   -0.0216189    -0.132688    -0.143275   -0.0930178   -0.0273377    0.058339     0.0165507    0.0225299    0.10628      0.103733     -0.0402472    0.00567488  -0.0581039    0.0992975
  0.0840373   -0.178036    -0.0101574   -0.0810895   -0.0157668    0.21654      0.00144645   0.00161869  -0.136504     -0.10058      0.000285239  -0.0900485   -0.252814     -0.104709    -0.0641185   0.0964327   -0.0991765   -0.0818504   -0.0848801    0.00130975   0.0251975    0.00903349    0.0566084   -0.0218526    0.173036     0.111827
 -0.169434    -0.0649441    0.0338722    0.0692713    0.00359828  -0.124108    -0.110636     0.245603    -0.0845376     0.0188478   -0.159423      0.049311     0.0184463     0.118975     0.193292    0.0898972    0.105417     0.0621516   -0.018786     0.0737004   -0.0979342   -0.0395644     0.00385991   0.154321     0.00387417  -0.0488894
 -0.0833315   -0.0187053   -0.100571     0.0100937   -0.0905195   -0.0658406    0.031807    -0.0238121   -0.274869     -0.0581083   -0.060913     -0.0671016   -0.000996687  -0.0710642   -0.163733   -0.0109965    0.0134945    0.0744958    0.050839     0.0808783    0.0167148   -0.0511125    -0.0691867    0.0312227    0.0685753   -0.12418
  0.10008     -0.0534756   -0.0387579   -0.12347     -0.0409521    0.00696748  -0.0307266    0.0338188   -0.00900154   -0.0115474    0.135472     -0.0212163    0.0613988     0.0499709   -0.0512221   0.139658    -0.120053    -0.0385441    0.0166001   -0.202798    -0.00894762   0.103186     -0.00981969   0.0553196   -0.117304     0.096961
  0.00172874   0.234524     0.178443     0.0309084    0.0387281    0.154098     0.104489    -0.11595     -0.00333835    0.0746099    0.00436068   -0.0020801    0.0741834     0.0959304   -0.0612698  -0.00345772   0.0866039   -0.093648     0.00282672   0.0856095   -0.0467683    0.0197182     0.0480634    0.0488391    0.106728    -0.0755159
  0.0567014    0.0229256    0.0385111    0.032047    -0.141057     0.112402    -0.0277539    0.0361127   -0.00234855    0.0309691    0.132531      0.113271     0.100462      0.1049       0.0852182  -0.0652127    0.0354043   -0.0246788   -0.0880347    0.216603     0.00162037   0.0148376     0.0687022    0.16392      0.142848     0.0963768
  0.223387     0.129975    -0.140244    -0.206002    -0.0519374   -0.0215966   -0.0402893    0.18464     -0.0501994     0.014031     0.057398     -0.00639941   0.221148     -0.0876404    0.0486748   0.0342726   -0.0120715    0.239275     0.0152197    0.134992    -0.0837393    0.0100883    -0.103192    -0.089152    -0.0230378    0.0702545kind diag, method split
┌ Info: 0: avll = 
└   tll[1] = -1.3948859753756992
[ Info: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.394970
[ Info: iteration 2, average log likelihood -1.394876
[ Info: iteration 3, average log likelihood -1.393956
[ Info: iteration 4, average log likelihood -1.383373
[ Info: iteration 5, average log likelihood -1.362129
[ Info: iteration 6, average log likelihood -1.356025
[ Info: iteration 7, average log likelihood -1.354757
[ Info: iteration 8, average log likelihood -1.354262
[ Info: iteration 9, average log likelihood -1.354042
[ Info: iteration 10, average log likelihood -1.353930
[ Info: iteration 11, average log likelihood -1.353862
[ Info: iteration 12, average log likelihood -1.353815
[ Info: iteration 13, average log likelihood -1.353780
[ Info: iteration 14, average log likelihood -1.353752
[ Info: iteration 15, average log likelihood -1.353730
[ Info: iteration 16, average log likelihood -1.353712
[ Info: iteration 17, average log likelihood -1.353697
[ Info: iteration 18, average log likelihood -1.353685
[ Info: iteration 19, average log likelihood -1.353674
[ Info: iteration 20, average log likelihood -1.353665
[ Info: iteration 21, average log likelihood -1.353657
[ Info: iteration 22, average log likelihood -1.353651
[ Info: iteration 23, average log likelihood -1.353645
[ Info: iteration 24, average log likelihood -1.353640
[ Info: iteration 25, average log likelihood -1.353636
[ Info: iteration 26, average log likelihood -1.353632
[ Info: iteration 27, average log likelihood -1.353629
[ Info: iteration 28, average log likelihood -1.353626
[ Info: iteration 29, average log likelihood -1.353623
[ Info: iteration 30, average log likelihood -1.353620
[ Info: iteration 31, average log likelihood -1.353617
[ Info: iteration 32, average log likelihood -1.353615
[ Info: iteration 33, average log likelihood -1.353612
[ Info: iteration 34, average log likelihood -1.353610
[ Info: iteration 35, average log likelihood -1.353608
[ Info: iteration 36, average log likelihood -1.353606
[ Info: iteration 37, average log likelihood -1.353604
[ Info: iteration 38, average log likelihood -1.353602
[ Info: iteration 39, average log likelihood -1.353600
[ Info: iteration 40, average log likelihood -1.353598
[ Info: iteration 41, average log likelihood -1.353596
[ Info: iteration 42, average log likelihood -1.353594
[ Info: iteration 43, average log likelihood -1.353592
[ Info: iteration 44, average log likelihood -1.353590
[ Info: iteration 45, average log likelihood -1.353587
[ Info: iteration 46, average log likelihood -1.353585
[ Info: iteration 47, average log likelihood -1.353582
[ Info: iteration 48, average log likelihood -1.353579
[ Info: iteration 49, average log likelihood -1.353577
[ Info: iteration 50, average log likelihood -1.353574
┌ Info: EM with 100000 data points 50 iterations avll -1.353574
└ 952.4 data points per parameter
┌ Info: 1
│   : avll =  = &quot;: avll = &quot;
│   avll =
│    50-element Array{Float64,1}:
│     -1.394969880887286
│     -1.3948762192765158
│      ⋮
└     -1.3535735836551324
[ Info: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.353676
[ Info: iteration 2, average log likelihood -1.353561
[ Info: iteration 3, average log likelihood -1.352855
[ Info: iteration 4, average log likelihood -1.346029
[ Info: iteration 5, average log likelihood -1.328880
[ Info: iteration 6, average log likelihood -1.318705
[ Info: iteration 7, average log likelihood -1.315760
[ Info: iteration 8, average log likelihood -1.314511
[ Info: iteration 9, average log likelihood -1.313741
[ Info: iteration 10, average log likelihood -1.313191
[ Info: iteration 11, average log likelihood -1.312791
[ Info: iteration 12, average log likelihood -1.312502
[ Info: iteration 13, average log likelihood -1.312285
[ Info: iteration 14, average log likelihood -1.312116
[ Info: iteration 15, average log likelihood -1.311979
[ Info: iteration 16, average log likelihood -1.311862
[ Info: iteration 17, average log likelihood -1.311759
[ Info: iteration 18, average log likelihood -1.311665
[ Info: iteration 19, average log likelihood -1.311577
[ Info: iteration 20, average log likelihood -1.311493
[ Info: iteration 21, average log likelihood -1.311411
[ Info: iteration 22, average log likelihood -1.311331
[ Info: iteration 23, average log likelihood -1.311255
[ Info: iteration 24, average log likelihood -1.311182
[ Info: iteration 25, average log likelihood -1.311114
[ Info: iteration 26, average log likelihood -1.311048
[ Info: iteration 27, average log likelihood -1.310983
[ Info: iteration 28, average log likelihood -1.310917
[ Info: iteration 29, average log likelihood -1.310846
[ Info: iteration 30, average log likelihood -1.310768
[ Info: iteration 31, average log likelihood -1.310680
[ Info: iteration 32, average log likelihood -1.310580
[ Info: iteration 33, average log likelihood -1.310467
[ Info: iteration 34, average log likelihood -1.310345
[ Info: iteration 35, average log likelihood -1.310217
[ Info: iteration 36, average log likelihood -1.310085
[ Info: iteration 37, average log likelihood -1.309957
[ Info: iteration 38, average log likelihood -1.309842
[ Info: iteration 39, average log likelihood -1.309743
[ Info: iteration 40, average log likelihood -1.309655
[ Info: iteration 41, average log likelihood -1.309578
[ Info: iteration 42, average log likelihood -1.309512
[ Info: iteration 43, average log likelihood -1.309459
[ Info: iteration 44, average log likelihood -1.309415
[ Info: iteration 45, average log likelihood -1.309376
[ Info: iteration 46, average log likelihood -1.309339
[ Info: iteration 47, average log likelihood -1.309304
[ Info: iteration 48, average log likelihood -1.309268
[ Info: iteration 49, average log likelihood -1.309232
[ Info: iteration 50, average log likelihood -1.309197
┌ Info: EM with 100000 data points 50 iterations avll -1.309197
└ 473.9 data points per parameter
┌ Info: 2
│   : avll =  = &quot;: avll = &quot;
│   avll =
│    50-element Array{Float64,1}:
│     -1.3536758243603535
│     -1.3535609123974839
│      ⋮
└     -1.309196548026931
[ Info: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.309317
[ Info: iteration 2, average log likelihood -1.309119
[ Info: iteration 3, average log likelihood -1.308147
[ Info: iteration 4, average log likelihood -1.299658
[ Info: iteration 5, average log likelihood -1.281403
[ Info: iteration 6, average log likelihood -1.268055
[ Info: iteration 7, average log likelihood -1.262740
[ Info: iteration 8, average log likelihood -1.260057
[ Info: iteration 9, average log likelihood -1.258520
[ Info: iteration 10, average log likelihood -1.257650
[ Info: iteration 11, average log likelihood -1.257113
[ Info: iteration 12, average log likelihood -1.256734
[ Info: iteration 13, average log likelihood -1.256447
[ Info: iteration 14, average log likelihood -1.256222
[ Info: iteration 15, average log likelihood -1.256013
[ Info: iteration 16, average log likelihood -1.255781
[ Info: iteration 17, average log likelihood -1.255468
[ Info: iteration 18, average log likelihood -1.254947
[ Info: iteration 19, average log likelihood -1.253954
[ Info: iteration 20, average log likelihood -1.252104
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     2
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 21, average log likelihood -1.249391
[ Info: iteration 22, average log likelihood -1.267103
[ Info: iteration 23, average log likelihood -1.258505
[ Info: iteration 24, average log likelihood -1.256205
[ Info: iteration 25, average log likelihood -1.255722
[ Info: iteration 26, average log likelihood -1.255554
[ Info: iteration 27, average log likelihood -1.255437
[ Info: iteration 28, average log likelihood -1.255322
[ Info: iteration 29, average log likelihood -1.255178
[ Info: iteration 30, average log likelihood -1.254949
[ Info: iteration 31, average log likelihood -1.254533
[ Info: iteration 32, average log likelihood -1.253711
[ Info: iteration 33, average log likelihood -1.251979
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     2
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 34, average log likelihood -1.249017
[ Info: iteration 35, average log likelihood -1.266651
[ Info: iteration 36, average log likelihood -1.257952
[ Info: iteration 37, average log likelihood -1.255704
[ Info: iteration 38, average log likelihood -1.255255
[ Info: iteration 39, average log likelihood -1.255071
[ Info: iteration 40, average log likelihood -1.254892
[ Info: iteration 41, average log likelihood -1.254667
[ Info: iteration 42, average log likelihood -1.254362
[ Info: iteration 43, average log likelihood -1.253930
[ Info: iteration 44, average log likelihood -1.253291
[ Info: iteration 45, average log likelihood -1.252236
[ Info: iteration 46, average log likelihood -1.250219
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     2
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 47, average log likelihood -1.246655
[ Info: iteration 48, average log likelihood -1.262942
[ Info: iteration 49, average log likelihood -1.253601
[ Info: iteration 50, average log likelihood -1.251193
┌ Info: EM with 100000 data points 50 iterations avll -1.251193
└ 236.4 data points per parameter
┌ Info: 3
│   : avll =  = &quot;: avll = &quot;
│   avll =
│    50-element Array{Float64,1}:
│     -1.309317111523987
│     -1.3091188962187212
│      ⋮
└     -1.251192780597709
[ Info: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.250878
[ Info: iteration 2, average log likelihood -1.250327
[ Info: iteration 3, average log likelihood -1.246553
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     4
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 4, average log likelihood -1.218115
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     13
│     14
│     16
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 5, average log likelihood -1.197506
[ Info: iteration 6, average log likelihood -1.199258
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      3
│      4
│     15
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 7, average log likelihood -1.174384
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     13
│     14
│     16
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 8, average log likelihood -1.190890
[ Info: iteration 9, average log likelihood -1.196509
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      4
│     16
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 10, average log likelihood -1.165193
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      3
│     13
│     14
│     15
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 11, average log likelihood -1.160259
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      4
│     16
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 12, average log likelihood -1.184949
[ Info: iteration 13, average log likelihood -1.178366
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      3
│      4
│     13
│     14
│     15
│     16
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 14, average log likelihood -1.143571
[ Info: iteration 15, average log likelihood -1.194716
[ Info: iteration 16, average log likelihood -1.177169
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      4
│     13
│     14
│     16
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 17, average log likelihood -1.141615
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      3
│     15
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 18, average log likelihood -1.166118
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      4
│     16
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 19, average log likelihood -1.177200
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     13
│     14
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 20, average log likelihood -1.164830
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      3
│     15
│     16
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 21, average log likelihood -1.153873
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     4
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 22, average log likelihood -1.179511
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     13
│     14
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 23, average log likelihood -1.164146
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      3
│     16
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 24, average log likelihood -1.153420
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      4
│     15
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 25, average log likelihood -1.162339
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     13
│     14
│     16
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 26, average log likelihood -1.165694
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     3
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 27, average log likelihood -1.166406
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      4
│     15
│     16
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 28, average log likelihood -1.156321
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     13
│     14
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 29, average log likelihood -1.171876
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     3
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 30, average log likelihood -1.165342
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      4
│     16
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 31, average log likelihood -1.155893
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     13
│     14
│     15
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 32, average log likelihood -1.155311
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      3
│     16
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 33, average log likelihood -1.167394
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     4
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 34, average log likelihood -1.169102
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│     13
│     14
│     15
│     16
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 35, average log likelihood -1.149781
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     3
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 36, average log likelihood -1.173930
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     4
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 37, average log likelihood -1.168108
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     13
│     14
│     16
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 38, average log likelihood -1.149379
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      3
│     15
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 39, average log likelihood -1.157248
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      4
│     16
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 40, average log likelihood -1.169877
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     13
│     14
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 41, average log likelihood -1.162407
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      3
│     15
│     16
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 42, average log likelihood -1.151952
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     4
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 43, average log likelihood -1.176369
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     13
│     14
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 44, average log likelihood -1.161903
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      3
│     16
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 45, average log likelihood -1.152128
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      4
│     15
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 46, average log likelihood -1.160171
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     13
│     14
│     16
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 47, average log likelihood -1.164304
[ Info: iteration 48, average log likelihood -1.165141
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      3
│      4
│     15
│     16
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 49, average log likelihood -1.143650
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     13
│     14
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 50, average log likelihood -1.176130
┌ Info: EM with 100000 data points 50 iterations avll -1.176130
└ 118.1 data points per parameter
┌ Info: 4
│   : avll =  = &quot;: avll = &quot;
│   avll =
│    50-element Array{Float64,1}:
│     -1.2508775576921158
│     -1.2503266623406484
│      ⋮
└     -1.1761298865453165
[ Info: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.175573
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      7
│      8
│     31
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 2, average log likelihood -1.148617
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      5
│      6
│     25
│     28
│     29
│     30
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 3, average log likelihood -1.139767
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      7
│      8
│     26
│     27
│     31
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 4, average log likelihood -1.123398
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     14
│     31
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 5, average log likelihood -1.102023
┌ Warning: Variances had to be floored 
│   ind =
│    10-element Array{Int64,1}:
│      2
│      5
│      6
│      7
│      ⋮
│     30
│     31
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 6, average log likelihood -1.077603
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│     14
│     27
│     31
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 7, average log likelihood -1.090989
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      5
│      6
│      7
│      8
│     26
│     31
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 8, average log likelihood -1.060701
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│      2
│     14
│     26
│     27
│      ⋮
│     30
│     31
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 9, average log likelihood -1.051778
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      6
│      7
│      8
│     31
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 10, average log likelihood -1.078360
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      5
│     14
│     26
│     31
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 11, average log likelihood -1.057912
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│      2
│      7
│      8
│     27
│      ⋮
│     30
│     31
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 12, average log likelihood -1.056449
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      5
│     14
│     19
│     26
│     31
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 13, average log likelihood -1.066587
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      7
│      8
│     27
│     28
│     31
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 14, average log likelihood -1.064481
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│      2
│      5
│      6
│     14
│      ⋮
│     30
│     31
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 15, average log likelihood -1.040127
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      7
│      8
│     27
│     28
│     31
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 16, average log likelihood -1.079929
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│     14
│     26
│     31
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 17, average log likelihood -1.058861
┌ Warning: Variances had to be floored 
│   ind =
│    10-element Array{Int64,1}:
│      2
│      5
│      7
│      8
│      ⋮
│     30
│     31
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 18, average log likelihood -1.049212
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│     14
│     26
│     31
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 19, average log likelihood -1.075061
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│      5
│      7
│      8
│     20
│      ⋮
│     28
│     31
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 20, average log likelihood -1.055950
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      2
│     14
│     29
│     30
│     31
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 21, average log likelihood -1.064064
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      5
│      6
│      7
│      8
│     26
│     31
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 22, average log likelihood -1.063261
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      2
│     14
│     27
│     28
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 23, average log likelihood -1.063991
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      7
│      8
│     26
│     29
│     30
│     31
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 24, average log likelihood -1.051440
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      5
│     14
│     27
│     28
│     31
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 25, average log likelihood -1.061050
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      2
│      7
│      8
│     26
│     31
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 26, average log likelihood -1.056324
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      5
│     14
│     27
│     28
│     29
│     30
│     31
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 27, average log likelihood -1.056340
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      7
│      8
│     19
│     26
│     31
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 28, average log likelihood -1.061131
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      2
│      5
│     14
│     26
│     27
│     28
│     31
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 29, average log likelihood -1.052108
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      6
│      7
│      8
│     29
│     30
│     31
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 30, average log likelihood -1.059778
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      5
│     14
│     26
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 31, average log likelihood -1.066589
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      2
│      7
│      8
│     27
│     28
│     31
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 32, average log likelihood -1.055149
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      5
│     14
│     26
│     29
│     30
│     31
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 33, average log likelihood -1.052232
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      2
│      6
│      7
│      8
│     26
│     27
│     28
│     31
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 34, average log likelihood -1.058790
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      5
│     14
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 35, average log likelihood -1.065894
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      7
│      8
│     26
│     29
│     30
│     31
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 36, average log likelihood -1.057366
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      2
│      5
│     14
│     26
│     27
│     28
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 37, average log likelihood -1.052324
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      6
│      7
│      8
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 38, average log likelihood -1.062466
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      5
│     14
│     19
│     26
│     29
│     30
│     31
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 39, average log likelihood -1.050761
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      2
│      7
│      8
│     27
│     28
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 40, average log likelihood -1.068712
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      5
│     14
│     26
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 41, average log likelihood -1.057333
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│      6
│      7
│      8
│     26
│      ⋮
│     29
│     30
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 42, average log likelihood -1.046758
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      2
│      5
│     14
│     31
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 43, average log likelihood -1.067221
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      7
│      8
│     26
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 44, average log likelihood -1.067036
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      5
│     14
│     26
│     27
│     28
│     29
│     30
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 45, average log likelihood -1.047435
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      2
│      6
│      7
│      8
│     31
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 46, average log likelihood -1.059985
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      5
│     14
│     26
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 47, average log likelihood -1.064557
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      7
│      8
│     27
│     28
│     29
│     30
│     31
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 48, average log likelihood -1.052392
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      2
│      5
│     14
│     26
│     31
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 49, average log likelihood -1.055363
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      7
│      8
│     26
│     27
│     28
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 50, average log likelihood -1.054898
┌ Info: EM with 100000 data points 50 iterations avll -1.054898
└ 59.0 data points per parameter
┌ Info: 5
│   : avll =  = &quot;: avll = &quot;
│   avll =
│    50-element Array{Float64,1}:
│     -1.1755729922102722
│     -1.1486166940395324
│      ⋮
└     -1.0548977397010422
32×26 Array{Float64,2}:
┌ Info: Total log likelihood: 
│   tll =
│    251-element Array{Float64,1}:
│     -1.3948859753756992
│     -1.394969880887286
│     -1.3948762192765158
│     -1.3939559375471895
│      ⋮
│     -1.0523915473756673
│     -1.0553630556603901
└     -1.0548977397010422
 -0.0607926   -0.0194154   -0.0967277   -0.00197842  -0.157023    -0.0644199   0.0290449    -0.014054    -0.237094     -0.0696361   -0.0574766   -0.0588824   -0.0142283   -0.0852688   -0.167995    0.0115521    0.0241332    0.0737536    0.0486206   0.0832138    0.0394168    -0.0528636   -0.0697266    0.0310633     0.0957199   -0.152354
 -0.034957    -0.0257877   -0.0869964   -0.0353479   -0.0124252    0.0126694   0.0668444     0.00951634   0.158191      0.0442657   -0.0168343   -0.181964     0.0968779   -0.00826409  -0.0869977   0.0462213    0.0192819   -0.0599909    0.0303734   0.139262    -0.00745514   -0.0465418    0.146827     0.0258813    -0.0984468   -0.10331
 -0.167648    -0.0851724   -0.0365155    0.110522    -0.0446444   -0.0437273  -0.0165889     0.167767    -0.136408      0.0807845   -0.150786     0.0329573    0.0612838    0.0289649    0.0146187   0.115694     0.010024     0.00185661  -0.0406336   0.0542668   -0.181302     -0.0445529    0.0689904   -0.0173105     0.103819     0.0657762
  0.222023     0.138853    -0.139451    -0.210535    -0.0463813   -0.0232298  -0.0490679     0.184697    -0.050931      0.0146779    0.0482415   -0.00757288   0.228145    -0.0815095    0.04128     0.0407638   -0.0146958    0.236022     0.0174385   0.0929187   -0.0832854    -0.0434402   -0.104994    -0.0881288     0.00869153   0.0348561
  0.0353053    0.0793795   -0.0336186   -0.092899     0.107961     0.115511   -0.0364018     0.0617378    0.103319     -0.139154    -0.0963328   -0.120675     0.1238      -0.045923     0.0167813  -0.00257329   0.0054085   -0.209483     0.0770926   0.00464308  -0.0992162     0.0874675   -0.0618031   -0.0246746     0.0679479   -0.102693
 -0.0374849    0.0384282   -0.0268379   -0.213757     0.0386844    0.0922494  -0.130282     -0.0229576   -0.137067      0.08596      0.0405459    0.185086     0.00734183   0.0283667    0.0416351  -0.197843    -0.0664128    0.0722827   -0.0566147  -0.0319298    0.000838696   0.0943137    0.0477582    0.0212359     0.188406    -0.115233
 -0.165259     0.0293977    0.0917498   -0.0824363    0.0830881   -0.0551938   0.0871916    -0.254019     0.0456016    -0.227611    -0.0896008    0.0534618   -0.106226    -0.185465    -0.0236514   0.107501     0.269839     0.00576593   0.118542   -0.0137453   -1.48551      -0.039531     0.0163458    0.140855     -0.102057    -0.201941
 -0.175494     0.0296656    0.103403    -0.118171     0.0791274   -0.0551206   0.0867352     0.0315959    0.0458587    -0.253815    -0.0914856   -0.11777     -0.105485    -0.135581     0.14152    -0.133947     0.0659444   -0.00875627   0.154382    0.00157465   1.21609      -0.0408025    0.183201     0.135634     -0.116698    -0.197888
 -0.0965693   -0.0208808   -0.0447464   -0.00576287   0.0447933    0.119313    0.199715     -0.0249995    0.0541691    -0.00171864  -0.1785       0.0534869    0.105914    -0.243449     0.0389817  -0.166912     0.0430954   -0.207264    -0.117416   -0.0375204   -0.0392257     0.0106956   -0.00852104   0.00278143    0.0961416   -0.106799
  0.0541217    0.127997    -0.0434937   -0.0153497    0.0179985   -0.254672    0.095948     -0.12366     -0.154714     -0.124263    -0.144788    -0.0163272    0.0360582   -0.0347222   -0.0472402  -0.0754845   -0.179948     0.0863745    0.177147    0.00418626  -0.0489214     0.00336758   0.111711    -0.0850184    -0.14284      0.204433
 -0.0582529    0.0963816    0.106656     0.0258332   -0.0830616    0.0479046  -0.104876     -0.0960377    0.028048      0.0400031    0.0297179    0.070394     0.0478754    0.0019275   -0.0467823  -0.065753     0.0581816   -0.0664745   -0.0073511   0.0546045    0.00637752   -0.0104805    0.0617514   -0.0815665     0.0107952   -0.0689237
  0.138096     0.0101518   -0.0320263   -0.0354531    0.00639395   0.0359187  -0.0335468     0.0876657   -0.00298966    0.0198265    0.109653     0.0266271    0.0388558    0.0391623   -0.0397731   0.0541482   -0.0747595   -0.0712409    0.0052818  -0.092027    -0.0248163     0.0754173   -0.0631946   -0.00277086   -0.0062114    0.12799
  0.0861323    0.127452    -0.0291493    0.169583     0.133821    -0.187551    0.0676346    -0.0606863   -0.289101      0.103667    -0.0640312   -0.0845987   -0.0751347   -0.012802    -0.017121   -0.154687    -0.144545     0.0287859   -0.125128    0.112903     0.188323     -0.0659066   -0.0790685    0.0458841    -0.0711995   -0.0461578
 -0.0192609    0.177479    -0.00886503  -0.00285752   0.0886868    0.21611     0.0358557    -0.0646162   -0.00838152    0.150409    -0.0414686    0.11154      0.022809     0.0958615    0.175395    0.029248     0.0458357   -0.12927      0.145515   -0.157222     0.0299467     0.0983105    0.00825902  -0.0978868    -0.222762    -0.0875172
 -0.00445646  -0.0352446   -0.0658433   -0.132736    -0.103403     0.0339403  -0.0262265     0.0220533   -0.0603433    -0.167053     0.131359     0.060126    -0.122674    -0.0534543    0.0508164   0.143287    -0.078925    -0.189264     0.185962   -0.0198718   -0.113385      0.0362468    0.0242789   -0.00654586   -0.0434617    0.0389493
  0.0517902    0.0245863    0.0569007    0.0229632   -0.133234     0.0998628  -0.0384269     0.0340187   -0.00247916    0.0338609    0.132056     0.113343     0.101035     0.110371     0.0850745  -0.0634416    0.0906082   -0.0253737   -0.0930393   0.193463    -0.0250349     0.0270215    0.076483     0.160667      0.146865     0.116375
  0.0854411   -0.193495    -0.0108216   -0.0835307   -0.0168146    0.217351   -0.00159755    0.00607027  -0.144213     -0.0896165   -0.00958617  -0.0897648   -0.246542    -0.106459    -0.0703302   0.0998074   -0.102165    -0.112162    -0.115882   -0.00684441   0.0247118     0.00937276   0.0558282   -0.0210775     0.189707     0.118853
  0.131791     0.0814344   -0.0813736    0.070437     0.0449268   -0.105295    0.0104215    -0.0363835    0.0277166    -0.0793491    0.00337251  -0.0757325    0.143719     0.0260611   -0.119222    0.0453725   -0.0794606    0.125189    -0.0994715   0.00536662  -0.114186     -0.0806515   -0.0195101   -0.130838     -0.0312712    0.0865183
  0.113694     0.0603221    0.0485481    0.191527     0.138629     0.124176   -0.016         0.0578591   -0.209735     -0.0157292    0.00845315   0.0225625   -0.013548    -0.131086    -0.129791   -0.0886892   -0.0277703    0.065167     0.0161588   0.00509566   0.106472      0.114019    -0.0388994   -0.000342474  -0.0557731    0.0854458
  0.0875746   -0.0115436    0.0704265   -0.0276395    0.165177    -0.0445791   0.0386666    -0.0228647   -0.000253478   0.0681941   -0.0547858   -0.0759169    0.020592     0.175894    -0.109635   -0.0877819   -0.0749192    0.0988162   -0.197773    0.0374163    0.0514071    -0.0610341   -0.193593    -0.0285883     0.0504495   -0.0530422
 -0.567157     0.0246542    0.0439086    0.00684944   0.0222841    0.090393   -0.0126549    -0.108445    -0.0689987     0.375406    -0.136915     0.0536696   -0.256006     0.0888581   -0.106271   -0.0966837   -0.0371936    0.0468225   -0.0521189   0.0356528   -0.209973     -0.091705     0.0800751    0.154995      0.0275871   -0.0905791
  0.777648     0.0353538   -0.00359313   0.0131084    0.0298957    0.155064    0.115975     -0.0630319   -0.0681371    -0.157843    -0.0472406    0.0425747   -0.023641     0.0696308    0.229451   -0.109877    -0.0233135    0.131386     0.0319024   0.113237     0.0333308    -0.0349904   -0.2445       0.175608      0.0594269   -0.0609588
  0.0329441   -0.0890753    0.0209581    0.0490727   -0.00197828  -0.0864139   0.00807342   -0.0282333    0.151841      0.0842659   -0.113298    -0.0396032   -0.0645845   -0.0278478   -0.0148335   0.0611481   -0.762028    -0.092616     0.0988641  -0.0950573    0.082469     -0.0761526    0.0246201    0.00260635   -0.161797    -0.0900342
  0.0142542   -0.0519961   -0.0309252    0.0447587    0.00755344   0.0677747  -0.0822967    -0.0168315    0.424504      0.1327       0.115196    -0.0594793   -0.0426517   -0.0750637    0.0804318   0.0669454    0.732209    -0.0381582    0.0186199   0.0933222    0.0742192    -0.113888     0.0283491   -0.118025     -0.168689    -0.0886061
 -0.0259769   -0.046958    -0.0909437    0.0285791    0.0204115   -0.11196    -0.0262865    -0.0242525    0.0102402     0.218719    -0.00344513  -0.0828782    0.0666411    0.0758462   -0.153674   -0.0467024   -0.0300777    0.0570882    0.154235    0.0634772    0.0584363     0.137726     0.00894328  -0.0362065    -0.0604634    0.0555759
  0.0337742    0.0666093   -0.0738583   -0.0186871   -0.107856    -0.128167   -0.166801     -0.109458     0.0361677     0.0307337    0.0130504    0.0161145   -0.045153     0.109754    -0.21226    -0.121356    -0.182619     0.0149376    0.128563    0.131778     0.0374753    -0.166137     0.13074      0.146399     -0.0714495    0.0130976
  0.0616484   -0.093448     0.0135018    0.00795326  -0.216389     0.35325    -0.0468246     0.0571162   -0.0569203    -0.307098     0.0619771    0.028934     0.0138727   -0.0242789    0.105521    0.0869177   -0.00327705   0.4846       0.0727314   0.138866     0.029731      0.119121    -0.0787716    0.0201699    -0.0437633   -0.0798246
 -0.295913    -0.0755728    0.00800399  -0.0196093   -0.0827308   -0.275621   -0.046578      0.0503931   -0.0480104     0.0482418    0.0311181    0.0414435    0.112406    -0.0246386    0.131964    0.085119    -0.00271158  -0.488384     0.0902424   0.0951141    0.102904      0.236163    -0.00602636   0.0147838     0.0181798   -0.131976
  0.0516258    0.172524    -0.259386     0.06183     -0.157632     0.105058   -0.0815769     0.057617    -0.047839      0.259738     0.128874    -0.26975      0.0220491    0.15634     -0.215727   -0.395107    -0.0175336   -0.071361    -0.0212866  -0.131766    -0.0924109     0.362367    -0.127389    -0.490461      0.152321     0.149699
  0.0915913   -0.0161129   -0.245431     0.0744927   -0.159893     0.167922   -0.0813078     0.0631601   -0.0511694     0.263179     0.139828    -0.105783     0.021293     0.157315    -0.19835    -0.0289675    0.00364802   0.0410045    0.114357   -0.11974     -0.13551      -0.161084     0.0433116    0.0376224    -0.11524     -0.0906668
  0.0285183   -0.00554808  -0.0372806   -0.120258    -0.0815867   -0.0467974  -0.000815126   0.0450609    0.108706      0.191411     0.0967832    0.0849404   -0.213106    -0.230297     0.0615039  -0.0528022    0.106612    -0.0425088    0.0851452   0.0472075    0.180789      0.0983585   -0.0459218    0.0912316    -0.0483284    0.0393494
  0.0313203   -0.0127255   -0.0325268   -0.0803127   -0.0719988   -0.0565557   0.120702      0.0468186    0.11405       0.170564     0.0732793    0.0730578   -0.225178    -0.222645     0.0678078  -0.0677031    0.105116    -0.0341132    0.0781748   0.0208916    0.173135      0.108664    -0.0509238    0.0905312    -0.0342288    0.0459783[ Info: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      5
│      6
│     14
│     19
│     29
│     30
│     31
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 1, average log likelihood -1.039075
┌ Warning: Variances had to be floored 
│   ind =
│    12-element Array{Int64,1}:
│      2
│      5
│      6
│      7
│      ⋮
│     30
│     31
│     32
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 2, average log likelihood -1.024996
┌ Warning: Variances had to be floored 
│   ind =
│    10-element Array{Int64,1}:
│      5
│      6
│     14
│     19
│      ⋮
│     29
│     30
│     31
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 3, average log likelihood -1.031677
┌ Warning: Variances had to be floored 
│   ind =
│    10-element Array{Int64,1}:
│      2
│      5
│      6
│      7
│      ⋮
│     29
│     30
│     31
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 4, average log likelihood -1.029094
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      5
│      6
│     14
│     19
│     26
│     29
│     30
│     31
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 5, average log likelihood -1.032855
┌ Warning: Variances had to be floored 
│   ind =
│    13-element Array{Int64,1}:
│      2
│      5
│      6
│      7
│      ⋮
│     29
│     30
│     31
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 6, average log likelihood -1.019482
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      5
│      6
│     14
│     19
│     29
│     30
│     31
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 7, average log likelihood -1.028941
┌ Warning: Variances had to be floored 
│   ind =
│    11-element Array{Int64,1}:
│      2
│      5
│      6
│      7
│      ⋮
│     29
│     30
│     31
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 8, average log likelihood -1.015470
┌ Warning: Variances had to be floored 
│   ind =
│    10-element Array{Int64,1}:
│      5
│      6
│     14
│     19
│      ⋮
│     29
│     30
│     31
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 9, average log likelihood -1.022087
┌ Warning: Variances had to be floored 
│   ind =
│    10-element Array{Int64,1}:
│      2
│      5
│      6
│      7
│      ⋮
│     29
│     30
│     31
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 10, average log likelihood -1.019821
┌ Info: EM with 100000 data points 10 iterations avll -1.019821
└ 59.0 data points per parameter
kind diag, method kmeans
[ Info: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       8.185524e+05
      1       6.622082e+05      -1.563443e+05 |       32
      2       6.397091e+05      -2.249910e+04 |       32
      3       6.268530e+05      -1.285604e+04 |       32
      4       6.166230e+05      -1.022999e+04 |       32
      5       6.074342e+05      -9.188833e+03 |       32
      6       6.007371e+05      -6.697084e+03 |       32
      7       5.960765e+05      -4.660590e+03 |       32
      8       5.933210e+05      -2.755542e+03 |       32
      9       5.914654e+05      -1.855557e+03 |       32
     10       5.898619e+05      -1.603478e+03 |       32
     11       5.883528e+05      -1.509196e+03 |       32
     12       5.872194e+05      -1.133389e+03 |       32
     13       5.865628e+05      -6.565321e+02 |       32
     14       5.862318e+05      -3.310157e+02 |       32
     15       5.860477e+05      -1.840840e+02 |       32
     16       5.858702e+05      -1.775044e+02 |       32
     17       5.857100e+05      -1.602037e+02 |       32
     18       5.855347e+05      -1.753347e+02 |       32
     19       5.853247e+05      -2.100365e+02 |       32
     20       5.851195e+05      -2.051252e+02 |       32
     21       5.849408e+05      -1.787100e+02 |       32
     22       5.848228e+05      -1.179964e+02 |       31
     23       5.847396e+05      -8.318979e+01 |       32
     24       5.846789e+05      -6.074001e+01 |       32
     25       5.846203e+05      -5.855151e+01 |       32
     26       5.845592e+05      -6.115987e+01 |       32
     27       5.844996e+05      -5.955195e+01 |       32
     28       5.844518e+05      -4.785613e+01 |       32
     29       5.844094e+05      -4.242056e+01 |       32
     30       5.843738e+05      -3.551627e+01 |       31
     31       5.843439e+05      -2.990579e+01 |       31
     32       5.843129e+05      -3.099430e+01 |       31
     33       5.842864e+05      -2.649713e+01 |       31
     34       5.842635e+05      -2.295053e+01 |       32
     35       5.842411e+05      -2.237292e+01 |       27
     36       5.842202e+05      -2.086582e+01 |       32
     37       5.841924e+05      -2.785202e+01 |       31
     38       5.841354e+05      -5.702451e+01 |       32
     39       5.840280e+05      -1.073837e+02 |       32
     40       5.838598e+05      -1.682004e+02 |       32
     41       5.836994e+05      -1.603381e+02 |       32
     42       5.835899e+05      -1.095454e+02 |       32
     43       5.835258e+05      -6.413073e+01 |       32
     44       5.834724e+05      -5.341908e+01 |       32
     45       5.834072e+05      -6.513886e+01 |       32
     46       5.833357e+05      -7.154711e+01 |       31
     47       5.832774e+05      -5.830652e+01 |       32
     48       5.832267e+05      -5.070194e+01 |       31
     49       5.831834e+05      -4.325433e+01 |       32
     50       5.831377e+05      -4.570082e+01 |       32
K-means terminated without convergence after 50 iterations (objv = 583137.7048065283)
┌ Info: K-means with 32000 data points using 50 iterations
└ 37.0 data points per parameter
[ Info: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.302787
[ Info: iteration 2, average log likelihood -1.272938
[ Info: iteration 3, average log likelihood -1.240998
[ Info: iteration 4, average log likelihood -1.200901
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     13
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 5, average log likelihood -1.148910
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      6
│     25
│     29
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 6, average log likelihood -1.122894
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     24
│     27
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 7, average log likelihood -1.120570
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     12
│     17
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 8, average log likelihood -1.084173
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      3
│     13
│     22
│     29
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 9, average log likelihood -1.048087
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      2
│      6
│     27
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 10, average log likelihood -1.077490
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     12
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 11, average log likelihood -1.077622
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     20
│     25
│     29
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 12, average log likelihood -1.031784
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      3
│      6
│     13
│     17
│     22
│     24
│     27
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 13, average log likelihood -1.013766
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      2
│     12
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 14, average log likelihood -1.079533
[ Info: iteration 15, average log likelihood -1.059053
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      6
│     17
│     20
│     27
│     29
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 16, average log likelihood -1.001656
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      3
│     12
│     13
│     24
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 17, average log likelihood -1.039284
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      2
│     22
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 18, average log likelihood -1.068893
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     27
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 19, average log likelihood -1.037097
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      6
│     12
│     13
│     17
│     20
│     25
│     29
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 20, average log likelihood -0.994200
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     3
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 21, average log likelihood -1.059635
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      2
│     22
│     24
│     27
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 22, average log likelihood -1.029555
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      6
│     12
│     13
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 23, average log likelihood -1.029055
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     20
│     25
│     29
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 24, average log likelihood -1.044210
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     3
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 25, average log likelihood -1.032135
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      2
│      6
│     12
│     13
│     22
│     24
│     27
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 26, average log likelihood -0.995967
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     17
│     29
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 27, average log likelihood -1.073822
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     20
│     25
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 28, average log likelihood -1.045583
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      3
│      6
│     12
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 29, average log likelihood -1.013260
┌ Warning: Variances had to be floored 
│   ind =
│    5-element Array{Int64,1}:
│      2
│     13
│     22
│     27
│     29
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 30, average log likelihood -1.021608
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     24
│     25
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 31, average log likelihood -1.058838
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      3
│      6
│     12
│     20
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 32, average log likelihood -1.027739
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      2
│     29
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 33, average log likelihood -1.042469
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│     13
│     17
│     22
│     27
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 34, average log likelihood -1.021740
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      3
│      6
│     12
│     24
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 35, average log likelihood -1.027765
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      2
│     20
│     25
│     29
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 36, average log likelihood -1.047349
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     13
│     27
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 37, average log likelihood -1.051912
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│      3
│      6
│     22
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 38, average log likelihood -1.033131
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     12
│     17
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 39, average log likelihood -1.020346
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      2
│     13
│     20
│     24
│     25
│     27
│     29
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 40, average log likelihood -1.000373
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     6
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 41, average log likelihood -1.069042
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     12
│     17
│     22
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 42, average log likelihood -1.044684
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     13
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 43, average log likelihood -1.040494
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      2
│      6
│     20
│     24
│     25
│     27
│     29
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 44, average log likelihood -1.003129
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      3
│     12
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 45, average log likelihood -1.063601
┌ Warning: Variances had to be floored 
│   ind =
│    3-element Array{Int64,1}:
│     13
│     17
│     22
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 46, average log likelihood -1.045242
[ Info: iteration 47, average log likelihood -1.045811
┌ Warning: Variances had to be floored 
│   ind =
│    8-element Array{Int64,1}:
│      2
│      6
│     12
│     20
│     24
│     25
│     27
│     29
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 48, average log likelihood -0.988700
┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│     13
│     17
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 49, average log likelihood -1.074805
┌ Warning: Variances had to be floored 
│   ind =
│    1-element Array{Int64,1}:
│     22
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 50, average log likelihood -1.065615
┌ Info: EM with 100000 data points 50 iterations avll -1.065615
└ 59.0 data points per parameter
32×26 Array{Float64,2}:
  0.0526877    0.125504    -0.0521533    -0.0193282     0.00823655  -0.245798     0.0817827   -0.124055    -0.148856     -0.117128     -0.136909    -0.0139242    0.0352644    -0.0266834   -0.0506109  -0.0756894   -0.17769      0.0817241     0.175127     0.0104253   -0.043217     -0.00425623   0.105016    -0.0816891    -0.140788     0.192824
  0.0798499   -0.0458022    0.0597977    -0.044632      0.140405    -0.0409216    0.101029    -0.0187116   -0.00204651    0.139966     -0.0548371   -0.148802    -0.114154      0.184058    -0.0796396  -0.148575    -0.080682     0.0870767    -0.266062     0.0471688    0.0701354    -0.109612    -0.200706     0.0490306    -0.00169369  -0.146533
  0.112118     0.0472333    0.0324071     0.187719      0.145721     0.141495    -0.0364513    0.0576446   -0.213928     -0.019457      0.00438375   0.0651104   -0.0115592    -0.190739    -0.0515389  -0.0636992   -0.0400653    0.0356653     0.016508    -0.0183717    0.108616      0.0791291   -0.0261098    0.0153525    -0.0525974    0.11224
  0.0194414    0.231347     0.188215      0.0302783     0.0476181    0.176405     0.100489    -0.118563    -0.0303703     0.0709757     0.0806346    2.35061e-6   0.0885092     0.099411    -0.0912418   0.0178685    0.0849529   -0.0658094     0.00572155   0.0813154   -0.0440458     0.0164782    0.0475857    0.0454292     0.101191    -0.106219
 -0.0253323   -0.0471542   -0.0910688     0.0272323     0.0151891   -0.112828    -0.0327069   -0.023852     0.00680472    0.215754     -0.00517108  -0.080232     0.0634804     0.0763396   -0.152802   -0.0434169   -0.0375119    0.0565013     0.153943     0.0648957    0.0587273     0.135728     0.00669497  -0.0305271    -0.0605963    0.0537628
 -0.160687     0.0286441    0.097473     -0.105466      0.0802556   -0.0477107    0.0791155   -0.0902373    0.0459051    -0.24878      -0.0857748   -0.0690447   -0.0985889    -0.161052     0.084025   -0.0376788    0.148654    -0.00389724    0.140697    -0.00447527   0.138324     -0.0369231    0.117557     0.135095     -0.109294    -0.194095
 -0.0649302   -0.0187214   -0.0963625    -0.0026117    -0.155673    -0.0637404    0.026741    -0.0151199   -0.233155     -0.0676959    -0.0555835   -0.0589041   -0.0165323    -0.0849103   -0.168248    0.00937617   0.021717     0.0736826     0.0487762    0.0831388    0.0361264    -0.0538219   -0.0692582    0.0309561     0.0944018   -0.152664
  0.171186     0.0666645   -0.00396062    0.0198895     0.0355565    0.079908    -0.0212768    0.142352     0.0154957     0.0651455     0.0573571    0.0787669   -0.000989555   0.00858855  -0.018653   -0.02618     -0.0179701   -0.127873      0.0175856   -0.0138584   -0.0433679     0.0421711   -0.125044    -0.0372152     0.124799     0.147932
 -0.162552    -0.0806888   -0.0344882     0.102095     -0.0388946   -0.0436532   -0.0173179    0.167375    -0.134878      0.0778194    -0.148025     0.0372928    0.0581565     0.0299399    0.0163322   0.114627     0.0126711    0.00249887   -0.0390823    0.0505824   -0.172145     -0.0438754    0.0675403   -0.0110998     0.10197      0.057102
 -0.0939631   -0.00391906   0.0117787     0.000928774  -0.074523     0.0318768   -0.272816    -0.0647525    0.0230202     0.00838355   -0.0138261    0.160382    -0.0206425    -0.00774464   0.0646811  -0.0388654    0.184553    -0.0783836     0.0903334   -0.0925897    0.299422     -0.0185598   -0.0282653   -0.0547535    -0.0827712   -0.130751
  0.0709941    0.0284748    0.0228637     0.0109521     0.0245763    0.127156     0.0475631   -0.0837464   -0.068579      0.121056     -0.0929328    0.0475107   -0.145426      0.0785501    0.0581411  -0.101832    -0.0278991    0.0872535    -0.0106379    0.0727707   -0.0978321    -0.0612037   -0.0735367    0.166653      0.0412081   -0.0758008
 -0.0189922    0.166913    -0.0160219    -0.00737874    0.103161     0.228278     0.0339587   -0.0621418   -0.00388164    0.168624     -0.0414304    0.123116     0.0198255     0.0971506    0.174802    0.104875     0.0448314   -0.139109      0.139678    -0.169231     0.0298169     0.106763     0.00683205  -0.0926176    -0.204815    -0.0857649
  0.0802981    0.0657464   -0.281135      0.0606303    -0.163437     0.151405    -0.0821694    0.058768    -0.0503358     0.249659      0.129981    -0.167212     0.0171811     0.130866    -0.193554   -0.192131    -0.00195997   0.000382616   0.0606013   -0.124409    -0.119356      0.0437139   -0.0244057   -0.21978      -0.0244506    0.00842009
 -0.00562354  -0.0502754   -0.0582599    -0.137505     -0.11045      0.0345123   -0.032098     0.0288035   -0.0644044    -0.162249      0.128072     0.0627907   -0.12079      -0.0478961    0.0535105   0.146751    -0.0732492   -0.18415       0.191366    -0.0278889   -0.11413       0.0392756    0.0197066    0.00439148   -0.0479493    0.0362405
  0.051535     0.0245435    0.0553986     0.0223983    -0.1329       0.100336    -0.039062     0.0337699   -0.00177426    0.0327349     0.130599     0.113773     0.100505      0.110327     0.0856397  -0.0643146    0.0923659   -0.0251633    -0.0935221    0.191677    -0.0248411     0.0259119    0.0766888    0.161503      0.146165     0.114701
  0.0840054    0.123332    -0.0318586     0.168025      0.131665    -0.185914     0.0678285   -0.0578304   -0.289515      0.0961553    -0.0477207   -0.0813799   -0.0775784    -0.00968727  -0.0130341  -0.163251    -0.141958     0.0257922    -0.119783     0.110667     0.189942     -0.0611082   -0.0772215    0.051124     -0.0796189   -0.0531639
  0.0304281    0.084465    -0.0295459    -0.0957522     0.112274     0.12014     -0.0435938    0.0601862    0.100584     -0.149062     -0.0975724   -0.075434     0.114384     -0.0556414    0.0232135  -0.00740507   0.0153347   -0.207114      0.106082    -0.0113741   -0.104081      0.0987553   -0.0629086   -0.0289093     0.0639212   -0.130664
  0.220002     0.140388    -0.138883     -0.208446     -0.0477929   -0.0233935   -0.0497048    0.182664    -0.0523296     0.0146947     0.0485011   -0.00701642   0.226224     -0.0806011    0.0399378   0.0396249   -0.0163543    0.233531      0.0187531    0.0939294   -0.0833772    -0.0469277   -0.107738    -0.0840058     0.0081983    0.0333801
  0.102897     0.0450561   -0.0486731     0.0566559     0.106095    -0.0889844    0.0760193   -0.144004     0.0545965     0.0286855     0.0844119   -0.100329     0.130741      0.0583272   -0.16215     0.0412914   -0.0616556    0.216623     -0.115676     0.0517099   -0.0633372    -0.0762845   -0.139873    -0.149715     -0.00960348   0.0686429
  0.0172902    0.0636097   -0.0525909    -0.0422491    -0.0942362   -0.134018    -0.155594    -0.0928313    0.0446205     0.0382979     0.0104103    0.0265418   -0.0372222     0.0892335   -0.215422   -0.127516    -0.188983     0.0278453     0.0976997    0.108578     0.0351743    -0.182757     0.201186     0.130276     -0.0293876    0.013386
  0.117566     0.0660978    0.0760398     0.0195692     0.201418    -0.0418523   -0.042951    -0.0122634   -0.0324657    -0.0346764    -0.0434645    0.0229057    0.233443      0.115002    -0.17677     0.0206215   -0.0630166    0.0979482    -0.0717889    0.0164469    0.0293808     0.0169725   -0.145957    -0.15362       0.131458     0.123478
 -0.0260057   -0.0293995   -0.0873133    -0.0379064    -0.0107418    0.0132234    0.0708205    0.00832457   0.167632      0.045312     -0.0130682   -0.184523     0.104263     -0.00898729  -0.0869207   0.0477262    0.0214729   -0.0612206     0.0298577    0.144309    -0.000792586  -0.0463097    0.147393     0.0227075    -0.100287    -0.115117
  0.106023    -0.061062    -0.0541731    -0.101682     -0.0341308    0.0139908   -0.0602199    0.0474038   -0.0187333    -0.0146171     0.145532    -0.00831248   0.0457807     0.0843131   -0.0552544   0.139632    -0.119984    -0.0501976     0.00762499  -0.190303    -0.0109628     0.105562    -0.00126565   0.0544692    -0.144921     0.0938343
 -0.0363045    0.0471836   -0.0347512    -0.207621      0.0393474    0.122819    -0.126098    -0.0196155   -0.188482      0.0840309     0.0365651    0.252016     0.0322168     0.0290424    0.0741965  -0.416369    -0.0684007    0.0652069    -0.0522202   -0.0333142    0.0236722     0.151496     0.0466766    0.0162815     0.144667    -0.153998
  0.0871786    0.0156635    0.0274315     0.0922563     0.0547178    0.120304    -0.0250144    0.0568392   -0.149899     -0.00246417    0.0110763    0.0473775   -0.0422157    -0.062122    -0.227908   -0.0262688   -0.0107305    0.0456549     0.0288779    0.038902     0.101648      0.174891    -0.0561457    0.0155548    -0.00569626   0.0305641
  0.0235348   -0.0691502   -0.00735176    0.0489082     0.00331682  -0.00330702  -0.0389712   -0.0196964    0.296748      0.10814       0.00667908  -0.0501498   -0.0591939    -0.0540493    0.0381288   0.0593158    0.0319322   -0.06119       0.0567781    0.00696822   0.0788782    -0.0967537    0.0263504   -0.0585981    -0.165168    -0.0892384
 -0.132596    -0.0914665   -0.000858464  -0.0487236    -0.205258     0.109017    -0.063744     0.0370081   -0.0388212    -0.168375      0.0891328    0.0403945    0.054311     -0.0426249    0.226522    0.07976      0.00800781   0.0845845     0.0789583    0.11637      0.0385557     0.189918    -0.0678097    0.01421      -0.00747083  -0.0900204
 -0.0955463    0.0594977    0.0848789     0.0382719    -0.209706    -0.0457031   -0.121369    -0.102473     0.0895744     0.0305286     0.0481891   -0.0222715    0.0449413    -0.0690316   -0.090105   -0.170853    -0.084216    -0.0424293    -0.112583     0.17039     -0.209317     -0.0198934    0.152005    -0.200134      0.0211251    0.00646004
  0.0277527    0.00325571  -0.0249662    -0.109048     -0.0602705   -0.0900286    0.111275     0.0401772    0.124203      0.213088      0.0635813    0.0817103   -0.248501     -0.204407     0.0639522  -0.0672578    0.101254    -0.0329108     0.0871951    0.0272674    0.186342      0.104202    -0.0645345    0.137441     -0.041305     0.0366266
 -0.0964446   -0.0202321   -0.0434117    -0.0086935     0.0411474    0.116753     0.196858    -0.0235938    0.0535323     0.000303899  -0.177939     0.0532105    0.106293     -0.243595     0.0390387  -0.173377     0.0420513   -0.205701     -0.118377    -0.0378189   -0.0382894     0.013992    -0.00626679   0.000672101   0.0974248   -0.10766
  0.0853218   -0.193378    -0.0108564    -0.0833486    -0.0165822    0.217576  [ Info: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
  -0.00140739   0.00597777  -0.144732     -0.0901129    -0.00913127  -0.0895185   -0.247242     -0.106401    -0.070237    0.0997696   -0.102445    -0.111961     -0.115974    -0.00605463   0.0248578     0.00937991   0.0559268   -0.0212192     0.189931     0.118901
  0.148831     0.115319    -0.117005      0.0957166    -0.0237638   -0.124514    -0.0565631    0.0801483    0.000418031  -0.171078     -0.066256    -0.057623     0.151841     -0.0132545   -0.0724654   0.0441772   -0.0969012    0.039414     -0.0860605   -0.0416327   -0.166897     -0.0822879    0.0988983   -0.110115     -0.0606385    0.101324┌ Warning: Variances had to be floored 
│   ind =
│    2-element Array{Int64,1}:
│      3
│     12
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 1, average log likelihood -1.017358
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│      2
│      3
│      6
│     12
│      ⋮
│     24
│     27
│     29
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 2, average log likelihood -0.968995
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      2
│      3
│      6
│     12
│     20
│     22
│     29
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 3, average log likelihood -0.993094
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      3
│      6
│     12
│     13
│     17
│     24
│     27
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 4, average log likelihood -0.992475
┌ Warning: Variances had to be floored 
│   ind =
│    6-element Array{Int64,1}:
│      2
│      3
│      6
│     12
│     20
│     29
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 5, average log likelihood -0.996555
┌ Warning: Variances had to be floored 
│   ind =
│    10-element Array{Int64,1}:
│      2
│      3
│      6
│     12
│      ⋮
│     24
│     27
│     29
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 6, average log likelihood -0.975637
┌ Warning: Variances had to be floored 
│   ind =
│    4-element Array{Int64,1}:
│      3
│      6
│     12
│     20
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 7, average log likelihood -1.011461
┌ Warning: Variances had to be floored 
│   ind =
│    9-element Array{Int64,1}:
│      2
│      3
│      6
│     12
│      ⋮
│     24
│     27
│     29
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 8, average log likelihood -0.979885
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      2
│      3
│      6
│     12
│     20
│     22
│     29
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 9, average log likelihood -0.994818
┌ Warning: Variances had to be floored 
│   ind =
│    7-element Array{Int64,1}:
│      3
│      6
│     12
│     13
│     17
│     24
│     27
└ @ GaussianMixtures ~&#x2F;.julia&#x2F;packages&#x2F;GaussianMixtures&#x2F;RGtTJ&#x2F;src&#x2F;train.jl:255
[ Info: iteration 10, average log likelihood -0.992468
┌ Info: EM with 100000 data points 10 iterations avll -0.992468
└ 59.0 data points per parameter
32×26 Array{Float64,2}:
  0.0995648  -0.0233344   -0.0152691    0.176527      0.0571389   -0.138142     0.0969268   -0.0563023   -0.040924    0.00782896   0.126454    -0.025949    -0.0936465    0.250552    -0.00382137   0.126688    -0.167454     -0.0432961   -0.110254     0.0971323   -0.0855169   -0.0673727    0.0179046    0.0966748  -0.0297591    -0.0852012
 -0.0390779   0.0918638    0.0935853    0.0747378     0.145781     0.0959455   -0.0230803   -0.0283134    0.0542479   0.0124753    0.0999038   -0.112897    -0.0735524    0.187854     0.0707492   -0.00901749   0.0336608    -0.12179      0.0366217    0.135908    -0.00736681   0.0695746    0.0445289    0.0547545   0.0787316     0.0515729
  0.0231384  -0.11105      0.0225895    0.0726399     0.0241031   -0.0271379    0.207137     0.289561     0.0450888   0.182646     0.104292     0.0997052    0.107833    -0.149046    -0.145347    -0.0742822   -0.0656099     0.0177246   -0.145015    -0.024987    -0.199026    -0.0678201    0.141958     0.213148    0.033516      0.124658
  0.166467   -0.0549146   -0.192519    -0.0452808     0.0646509    0.0292336   -0.0990635    0.0503723   -0.0392078  -0.0167408    0.0828443   -0.0156147   -0.0955236   -0.0559208    0.0442473    0.00700761   0.0333347     0.152449     0.0341917   -0.0675528   -0.0499472   -0.135242     0.0344335    0.0422483  -0.131983      0.13661
  0.0281391  -0.0282813   -0.0410537    0.0362561    -0.144539     0.106064    -0.154482    -0.0730827   -0.0979956  -0.0940924   -0.0402421   -0.0327027   -0.0385165    0.179831    -0.219004    -0.238311    -0.0653029     0.0994277    0.0795809   -0.11865      0.0150661    0.0204573   -0.0660248   -0.116911    0.123596     -0.00599591
 -0.0722197   0.0906366   -0.13303     -0.0520107     0.0604756    0.0417016    0.106155    -0.103235     0.0666782   0.0498878    0.0774446   -0.0695749   -0.266175    -0.00408734  -0.0372447    0.0595642   -0.0943835     0.110472    -0.0208276    0.0729376   -0.016582     0.21843      0.0201492    0.159936    0.101737     -0.0430812
 -0.115858    0.0648396   -0.117291    -0.0661735    -0.0614817    0.24499     -0.0277381    0.208044     0.0900937   0.0964246   -0.0846391    0.116649     0.0731808   -0.063223    -0.0154736    0.0118884    0.000604623  -0.0635636   -0.0682513   -0.0016793    0.0307288    0.0173527   -0.0257635   -0.0874145  -0.166417     -0.0145699
  0.0509469  -0.259015     0.0187093    0.0220522    -0.100234    -0.00593038   0.0389092    0.00676919   0.229314   -0.0210098   -0.165059    -0.0330089    0.172495    -0.0392901    0.00817836   0.0356785   -0.0303441    -0.0263915    0.00184485   0.0408072    0.0884083   -0.0525813   -0.0111821   -0.0638777   0.124535     -0.139252
 -0.106064   -0.0973128    0.01833      0.00610082   -0.0710902    0.150537     0.120829    -0.00343327  -0.0720076  -0.137929    -0.107426    -0.192791     0.178578     0.100365     0.134052     0.119699    -3.98365e-5   -0.103367    -0.0390318    0.133834     0.0447985    0.0460363    0.0779414    0.035989    0.0544368    -0.0543079
 -0.0221908   0.0170123   -0.0167847    0.0897877    -0.0205516   -0.175383    -0.0752389   -0.0555025    0.125711   -0.0540204   -0.102619    -0.114886    -0.0710591    0.0153938   -0.0105698   -0.0443176    0.11797       0.0671647   -0.0716715   -0.16763      0.0637063    0.0592642   -0.126662     0.0980598   0.0212915    -0.0596383
  0.284809   -0.0552938    0.0168392   -0.0398254    -0.0897578   -0.254532     0.0401485    0.0460968   -0.0343692  -0.0668701   -0.0237035    0.128378    -0.025417    -0.126008     0.271494    -0.0778174   -0.0143751    -0.0167399   -0.194455    -0.0840456   -0.0807554    0.0306752   -0.0190569    0.0901714  -0.118876      0.0511023
  0.209742   -0.0451849    0.244969    -0.0905888     0.122994    -0.0410976    0.0739742    0.00641679   0.0917195  -0.0830351    0.0978657   -0.0198788   -0.0800066   -0.0392511   -0.0435588   -0.0131002    0.0241752     0.029671     0.0252961   -0.150942     0.200604    -0.0891671   -0.0304307    0.108536    0.0532678    -0.089663
 -0.260523    0.030484    -0.268414    -0.0921726    -0.21212      0.0378935   -0.0739155    0.0496718   -0.160136   -0.0737422    0.106486    -0.012223     0.0520719    0.170844     0.141994    -0.00994828  -0.136729      0.0587347   -0.100036     0.123009    -0.0994645   -0.0143637   -0.049184    -0.0811763   0.117137      0.0245847
 -0.0743517  -0.0192865   -0.03563      0.119831      0.0332013    0.0101936   -0.106064     0.12577     -0.0663505  -0.0520269    0.015004    -0.0413743   -0.0407581   -0.0166156    0.105369    -0.0647274    0.0323499    -0.0258658    0.0042625   -0.0891025    0.0413212   -0.0643229   -0.0676991    0.167238   -0.0743311    -0.0412542
 -0.0745686  -0.157913    -0.124153    -0.109438      0.0132165   -0.0934054   -0.118176    -0.052672    -0.129878   -0.0946351    0.0321658    0.0705178    0.0874159    0.0197758    0.145109     0.0538846   -0.0876241    -0.0933707    0.0503322   -0.0619027   -0.0640968   -0.0629296    0.0623623   -0.168135    0.151527     -0.00371977
 -0.149761    0.0317549    0.0421807    0.100766      0.0148645    0.0356136   -0.130327     0.0243886    0.163012   -0.0445433   -0.0429587   -0.0381292   -0.109184    -0.183654    -0.0313694    0.170823    -0.179278     -0.0395248   -0.111392     0.0503781   -0.0717163   -0.10772     -0.107674     0.144974    0.0763433    -0.129376
 -0.182415   -0.0622113   -0.0576952    0.0274305     0.0223018    0.143244    -0.0359772   -0.141495    -0.150676    0.0728137   -0.0751046    0.0292198   -0.0970791   -0.128354    -0.0458536    0.0184327   -0.0478382    -0.00536601   0.0992524   -0.0532274    0.17024      0.107181     0.0929672    0.0326506   0.000184794  -0.191878
 -0.207574   -0.0488293    0.0043873   -0.066607      0.00501048   0.0746908   -0.0964603    0.0485948    0.0404736   0.0422072    0.139862     0.0502726    0.0501108   -0.124536     0.03374     -0.0400296    0.0548262    -0.0305655   -0.0979556    0.0176836   -0.0373213   -0.0182309    0.0343193    0.0223368  -0.139618      0.0130963
  0.0889262  -0.0397302   -0.00146672  -0.114418     -0.0636014    0.0416022    0.0824341    0.104978    -0.0772469  -0.07536     -0.0854614   -0.143116     0.0996914    0.01588      0.224971    -0.0486464   -0.106057     -0.0449527    0.137764     0.0161463    0.13971      0.155929     0.125674     0.100765    0.0181591    -0.0325041
 -0.0858439   0.0741559   -0.0906625   -0.0284455    -0.0542217   -0.00556893   0.0179344   -0.127856     0.0146596   0.0486802   -0.103338    -0.0159072    0.0656259    0.244966     0.0113797   -0.0205837    0.0613894     0.0985682   -0.107169    -0.129836     0.0393697    0.150747    -0.0892414    0.0794688   0.0738023     0.0175246
  0.0745027   0.120109     0.0493914   -0.0668252     0.209197     0.00880225  -0.00870361  -0.00455767  -0.225002   -0.0361797   -0.0932855   -0.0321576    0.106687     0.147003     0.0341111    0.0323443    0.1594       -0.0984217    0.221913     0.122388     0.0184973    0.0933473   -0.00547729   0.0737842   0.136532     -0.114874
  0.0290891   0.0683655    0.203575     0.0378202    -0.133657     0.146883     0.0136595   -0.0105704   -0.0995618  -0.0328396    0.0616578   -0.0486314    0.169644    -0.0303643   -0.161592    -0.0850121    0.0789748     0.140363    -0.12008     -0.0289131    0.0273559    0.376596     0.094622     0.018894   -0.148616     -0.171716
 -0.107871    0.0365074   -0.0765736   -0.013705     -0.0264338   -0.0729695    0.0504141   -0.036663     0.0831247   0.1097       0.169438     0.0953783    0.013661    -0.0165004   -0.157045     0.257678     0.0129702    -0.157551    -0.00739077  -0.0572864    0.130534     0.126705    -0.0451677    0.0154196  -0.0857983    -0.0508453
 -0.0801967  -0.0168537    0.120031     0.00144246   -0.206479    -0.11611     -0.0044217    0.0928196   -0.109572    0.175471     0.0828307    0.0441847   -0.147067     0.112601    -0.0945642    0.148273    -0.0770486    -0.0484414    0.0370768    0.0407931    0.1245      -0.0147489   -0.00466051  -0.0587246  -0.0249235    -0.108038
  0.0789111  -0.00830648   0.199115     0.0667547     0.0179933   -0.0248724    0.0996374   -0.0227244    0.0820742  -0.0311004    0.019631     0.169015     0.173823    -0.0845694   -0.00861799   0.0902348    0.00305757   -0.0935808   -0.0163081   -0.170685    -0.262919     0.0083993   -0.135655     0.0541854   0.1757       -0.0123653
  0.147322    0.0246062   -0.0758847   -0.11359       0.0780223   -0.0117868   -0.0136263   -0.0561124   -0.0212898   0.21726      0.0716369    0.0734044    0.00661298   0.0150372   -0.117428     0.0611634    0.0440158     0.018293    -0.0396413   -0.0335036   -0.0015444    0.0471448   -0.145421    -0.193293    0.0492444    -0.0509866
 -0.0653191   0.00303906   0.211149     0.0495966     0.0569938   -0.0811117    0.229816     0.169031     0.0524872   0.133447     0.0454583    0.0649477    0.0788578   -0.0226653   -0.00508411   0.0383391    0.0623191    -0.12618      0.0171873   -0.0332555   -0.132904    -0.0484786    0.199861     0.0805144   0.0707572     0.139134
  0.140921    0.0777727   -0.0743698   -0.000772639   0.031942     0.059317    -0.0127028    0.109509     0.0443474  -0.164368     0.156296    -0.06669      0.183946    -0.0691288    0.0261063    0.0912595    0.0102482     0.108615     0.233949    -0.00775071  -0.00441981   0.0399021    0.0409674   -0.0743897  -0.161403      0.0530402
 -0.068464    0.108929    -0.0452558    0.0198065     0.112552    -0.06313      0.1458       0.0108682    0.15087    -0.00818508   0.0206432   -0.00919856   0.174092    -0.141976     0.059263     0.239906    -0.0144037     0.0544107   -0.0623375   -0.00984561   0.283727     0.233645     0.0871216   -0.0870632   0.156572      0.0470689
  0.0853438  -0.138826    -0.013428    -0.18027      -0.0638188    0.0594442    0.063778     0.146889     0.0340418   0.0359299   -0.182818    -0.00373852   0.06575     -0.0303746    0.135047     0.0050072   -0.109625     -0.0890233   -0.0204622   -0.0315448   -0.0229951    0.00629691   0.00447185   0.0129103  -0.0479122     0.0439085
  0.0331615   0.0516817   -0.0597817   -0.00800603    0.0524435   -0.297939     0.0766139   -0.0830299    0.145421    0.0704646    0.086668    -0.100199     0.0536219   -0.0169049   -0.103903     0.0336345    0.017893      0.177484     0.107957    -0.0941148    0.0455897    0.186139     0.0117584   -0.0405713  -0.149871     -0.259164
  0.194133   -0.0577084   -0.0306188    0.021471     -0.0558886   -0.0616566   -0.0348525    0.0670318    0.0724328  -0.104696    -0.00874252   0.234068     0.0382545   -0.0529531   -0.012882     0.0141409    0.0239279     0.0722004    0.0677148    0.0481071   -0.20778     -0.0272674    0.126233    -0.0170808   0.00983393    0.0774773kind full, method split
┌ Info: 0: avll = 
└   tll[1] = -1.4143215802565607
[ Info: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.414340
[ Info: iteration 2, average log likelihood -1.414248
[ Info: iteration 3, average log likelihood -1.414169
[ Info: iteration 4, average log likelihood -1.414075
[ Info: iteration 5, average log likelihood -1.413956
[ Info: iteration 6, average log likelihood -1.413796
[ Info: iteration 7, average log likelihood -1.413550
[ Info: iteration 8, average log likelihood -1.413139
[ Info: iteration 9, average log likelihood -1.412462
[ Info: iteration 10, average log likelihood -1.411511
[ Info: iteration 11, average log likelihood -1.410491
[ Info: iteration 12, average log likelihood -1.409704
[ Info: iteration 13, average log likelihood -1.409259
[ Info: iteration 14, average log likelihood -1.409054
[ Info: iteration 15, average log likelihood -1.408968
[ Info: iteration 16, average log likelihood -1.408933
[ Info: iteration 17, average log likelihood -1.408918
[ Info: iteration 18, average log likelihood -1.408912
[ Info: iteration 19, average log likelihood -1.408910
[ Info: iteration 20, average log likelihood -1.408908
[ Info: iteration 21, average log likelihood -1.408908
[ Info: iteration 22, average log likelihood -1.408907
[ Info: iteration 23, average log likelihood -1.408907
[ Info: iteration 24, average log likelihood -1.408907
[ Info: iteration 25, average log likelihood -1.408907
[ Info: iteration 26, average log likelihood -1.408907
[ Info: iteration 27, average log likelihood -1.408906
[ Info: iteration 28, average log likelihood -1.408906
[ Info: iteration 29, average log likelihood -1.408906
[ Info: iteration 30, average log likelihood -1.408906
[ Info: iteration 31, average log likelihood -1.408906
[ Info: iteration 32, average log likelihood -1.408906
[ Info: iteration 33, average log likelihood -1.408906
[ Info: iteration 34, average log likelihood -1.408906
[ Info: iteration 35, average log likelihood -1.408906
[ Info: iteration 36, average log likelihood -1.408906
[ Info: iteration 37, average log likelihood -1.408906
[ Info: iteration 38, average log likelihood -1.408906
[ Info: iteration 39, average log likelihood -1.408906
[ Info: iteration 40, average log likelihood -1.408906
[ Info: iteration 41, average log likelihood -1.408905
[ Info: iteration 42, average log likelihood -1.408905
[ Info: iteration 43, average log likelihood -1.408905
[ Info: iteration 44, average log likelihood -1.408905
[ Info: iteration 45, average log likelihood -1.408905
[ Info: iteration 46, average log likelihood -1.408905
[ Info: iteration 47, average log likelihood -1.408905
[ Info: iteration 48, average log likelihood -1.408905
[ Info: iteration 49, average log likelihood -1.408905
[ Info: iteration 50, average log likelihood -1.408905
┌ Info: EM with 100000 data points 50 iterations avll -1.408905
└ 952.4 data points per parameter
┌ Info: 1
│   : avll =  = &quot;: avll = &quot;
│   avll =
│    50-element Array{Float64,1}:
│     -1.4143397016163148
│     -1.414247681198198
│      ⋮
└     -1.408905309812306
[ Info: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.408923
[ Info: iteration 2, average log likelihood -1.408829
[ Info: iteration 3, average log likelihood -1.408749
[ Info: iteration 4, average log likelihood -1.408658
[ Info: iteration 5, average log likelihood -1.408556
[ Info: iteration 6, average log likelihood -1.408450
[ Info: iteration 7, average log likelihood -1.408353
[ Info: iteration 8, average log likelihood -1.408273
[ Info: iteration 9, average log likelihood -1.408212
[ Info: iteration 10, average log likelihood -1.408167
[ Info: iteration 11, average log likelihood -1.408135
[ Info: iteration 12, average log likelihood -1.408110
[ Info: iteration 13, average log likelihood -1.408091
[ Info: iteration 14, average log likelihood -1.408076
[ Info: iteration 15, average log likelihood -1.408064
[ Info: iteration 16, average log likelihood -1.408054
[ Info: iteration 17, average log likelihood -1.408044
[ Info: iteration 18, average log likelihood -1.408035
[ Info: iteration 19, average log likelihood -1.408027
[ Info: iteration 20, average log likelihood -1.408017
[ Info: iteration 21, average log likelihood -1.408008
[ Info: iteration 22, average log likelihood -1.407997
[ Info: iteration 23, average log likelihood -1.407985
[ Info: iteration 24, average log likelihood -1.407972
[ Info: iteration 25, average log likelihood -1.407957
[ Info: iteration 26, average log likelihood -1.407941
[ Info: iteration 27, average log likelihood -1.407924
[ Info: iteration 28, average log likelihood -1.407906
[ Info: iteration 29, average log likelihood -1.407888
[ Info: iteration 30, average log likelihood -1.407869
[ Info: iteration 31, average log likelihood -1.407850
[ Info: iteration 32, average log likelihood -1.407833
[ Info: iteration 33, average log likelihood -1.407816
[ Info: iteration 34, average log likelihood -1.407801
[ Info: iteration 35, average log likelihood -1.407788
[ Info: iteration 36, average log likelihood -1.407776
[ Info: iteration 37, average log likelihood -1.407765
[ Info: iteration 38, average log likelihood -1.407756
[ Info: iteration 39, average log likelihood -1.407748
[ Info: iteration 40, average log likelihood -1.407740
[ Info: iteration 41, average log likelihood -1.407734
[ Info: iteration 42, average log likelihood -1.407729
[ Info: iteration 43, average log likelihood -1.407724
[ Info: iteration 44, average log likelihood -1.407720
[ Info: iteration 45, average log likelihood -1.407716
[ Info: iteration 46, average log likelihood -1.407713
[ Info: iteration 47, average log likelihood -1.407710
[ Info: iteration 48, average log likelihood -1.407707
[ Info: iteration 49, average log likelihood -1.407705
[ Info: iteration 50, average log likelihood -1.407703
┌ Info: EM with 100000 data points 50 iterations avll -1.407703
└ 473.9 data points per parameter
┌ Info: 2
│   : avll =  = &quot;: avll = &quot;
│   avll =
│    50-element Array{Float64,1}:
│     -1.4089231994618636
│     -1.4088290083442725
│      ⋮
└     -1.407702632869098
[ Info: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.407712
[ Info: iteration 2, average log likelihood -1.407646
[ Info: iteration 3, average log likelihood -1.407589
[ Info: iteration 4, average log likelihood -1.407521
[ Info: iteration 5, average log likelihood -1.407439
[ Info: iteration 6, average log likelihood -1.407341
[ Info: iteration 7, average log likelihood -1.407231
[ Info: iteration 8, average log likelihood -1.407115
[ Info: iteration 9, average log likelihood -1.407001
[ Info: iteration 10, average log likelihood -1.406897
[ Info: iteration 11, average log likelihood -1.406805
[ Info: iteration 12, average log likelihood -1.406727
[ Info: iteration 13, average log likelihood -1.406661
[ Info: iteration 14, average log likelihood -1.406606
[ Info: iteration 15, average log likelihood -1.406561
[ Info: iteration 16, average log likelihood -1.406523
[ Info: iteration 17, average log likelihood -1.406490
[ Info: iteration 18, average log likelihood -1.406462
[ Info: iteration 19, average log likelihood -1.406437
[ Info: iteration 20, average log likelihood -1.406415
[ Info: iteration 21, average log likelihood -1.406395
[ Info: iteration 22, average log likelihood -1.406377
[ Info: iteration 23, average log likelihood -1.406360
[ Info: iteration 24, average log likelihood -1.406346
[ Info: iteration 25, average log likelihood -1.406332
[ Info: iteration 26, average log likelihood -1.406320
[ Info: iteration 27, average log likelihood -1.406309
[ Info: iteration 28, average log likelihood -1.406300
[ Info: iteration 29, average log likelihood -1.406291
[ Info: iteration 30, average log likelihood -1.406283
[ Info: iteration 31, average log likelihood -1.406276
[ Info: iteration 32, average log likelihood -1.406269
[ Info: iteration 33, average log likelihood -1.406263
[ Info: iteration 34, average log likelihood -1.406257
[ Info: iteration 35, average log likelihood -1.406252
[ Info: iteration 36, average log likelihood -1.406247
[ Info: iteration 37, average log likelihood -1.406242
[ Info: iteration 38, average log likelihood -1.406237
[ Info: iteration 39, average log likelihood -1.406233
[ Info: iteration 40, average log likelihood -1.406228
[ Info: iteration 41, average log likelihood -1.406224
[ Info: iteration 42, average log likelihood -1.406220
[ Info: iteration 43, average log likelihood -1.406216
[ Info: iteration 44, average log likelihood -1.406212
[ Info: iteration 45, average log likelihood -1.406207
[ Info: iteration 46, average log likelihood -1.406203
[ Info: iteration 47, average log likelihood -1.406199
[ Info: iteration 48, average log likelihood -1.406195
[ Info: iteration 49, average log likelihood -1.406190
[ Info: iteration 50, average log likelihood -1.406186
┌ Info: EM with 100000 data points 50 iterations avll -1.406186
└ 236.4 data points per parameter
┌ Info: 3
│   : avll =  = &quot;: avll = &quot;
│   avll =
│    50-element Array{Float64,1}:
│     -1.4077118983891592
│     -1.4076464747754758
│      ⋮
└     -1.4061860126210948
[ Info: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.406190
[ Info: iteration 2, average log likelihood -1.406125
[ Info: iteration 3, average log likelihood -1.406061
[ Info: iteration 4, average log likelihood -1.405984
[ Info: iteration 5, average log likelihood -1.405887
[ Info: iteration 6, average log likelihood -1.405767
[ Info: iteration 7, average log likelihood -1.405627
[ Info: iteration 8, average log likelihood -1.405476
[ Info: iteration 9, average log likelihood -1.405322
[ Info: iteration 10, average log likelihood -1.405175
[ Info: iteration 11, average log likelihood -1.405038
[ Info: iteration 12, average log likelihood -1.404914
[ Info: iteration 13, average log likelihood -1.404804
[ Info: iteration 14, average log likelihood -1.404706
[ Info: iteration 15, average log likelihood -1.404621
[ Info: iteration 16, average log likelihood -1.404547
[ Info: iteration 17, average log likelihood -1.404483
[ Info: iteration 18, average log likelihood -1.404428
[ Info: iteration 19, average log likelihood -1.404379
[ Info: iteration 20, average log likelihood -1.404337
[ Info: iteration 21, average log likelihood -1.404300
[ Info: iteration 22, average log likelihood -1.404267
[ Info: iteration 23, average log likelihood -1.404236
[ Info: iteration 24, average log likelihood -1.404208
[ Info: iteration 25, average log likelihood -1.404182
[ Info: iteration 26, average log likelihood -1.404158
[ Info: iteration 27, average log likelihood -1.404135
[ Info: iteration 28, average log likelihood -1.404114
[ Info: iteration 29, average log likelihood -1.404093
[ Info: iteration 30, average log likelihood -1.404073
[ Info: iteration 31, average log likelihood -1.404053
[ Info: iteration 32, average log likelihood -1.404034
[ Info: iteration 33, average log likelihood -1.404016
[ Info: iteration 34, average log likelihood -1.403998
[ Info: iteration 35, average log likelihood -1.403980
[ Info: iteration 36, average log likelihood -1.403963
[ Info: iteration 37, average log likelihood -1.403947
[ Info: iteration 38, average log likelihood -1.403931
[ Info: iteration 39, average log likelihood -1.403915
[ Info: iteration 40, average log likelihood -1.403899
[ Info: iteration 41, average log likelihood -1.403885
[ Info: iteration 42, average log likelihood -1.403870
[ Info: iteration 43, average log likelihood -1.403856
[ Info: iteration 44, average log likelihood -1.403843
[ Info: iteration 45, average log likelihood -1.403830
[ Info: iteration 46, average log likelihood -1.403817
[ Info: iteration 47, average log likelihood -1.403805
[ Info: iteration 48, average log likelihood -1.403793
[ Info: iteration 49, average log likelihood -1.403782
[ Info: iteration 50, average log likelihood -1.403771
┌ Info: EM with 100000 data points 50 iterations avll -1.403771
└ 118.1 data points per parameter
┌ Info: 4
│   : avll =  = &quot;: avll = &quot;
│   avll =
│    50-element Array{Float64,1}:
│     -1.4061897493191455
│     -1.4061246069743576
│      ⋮
└     -1.4037706731376336
[ Info: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.403769
[ Info: iteration 2, average log likelihood -1.403702
[ Info: iteration 3, average log likelihood -1.403636
[ Info: iteration 4, average log likelihood -1.403557
[ Info: iteration 5, average log likelihood -1.403457
[ Info: iteration 6, average log likelihood -1.403331
[ Info: iteration 7, average log likelihood -1.403180
[ Info: iteration 8, average log likelihood -1.403010
[ Info: iteration 9, average log likelihood -1.402833
[ Info: iteration 10, average log likelihood -1.402659
[ Info: iteration 11, average log likelihood -1.402496
[ Info: iteration 12, average log likelihood -1.402348
[ Info: iteration 13, average log likelihood -1.402215
[ Info: iteration 14, average log likelihood -1.402096
[ Info: iteration 15, average log likelihood -1.401991
[ Info: iteration 16, average log likelihood -1.401898
[ Info: iteration 17, average log likelihood -1.401816
[ Info: iteration 18, average log likelihood -1.401743
[ Info: iteration 19, average log likelihood -1.401678
[ Info: iteration 20, average log likelihood -1.401620
[ Info: iteration 21, average log likelihood -1.401567
[ Info: iteration 22, average log likelihood -1.401519
[ Info: iteration 23, average log likelihood -1.401475
[ Info: iteration 24, average log likelihood -1.401435
[ Info: iteration 25, average log likelihood -1.401397
[ Info: iteration 26, average log likelihood -1.401362
[ Info: iteration 27, average log likelihood -1.401329
[ Info: iteration 28, average log likelihood -1.401297
[ Info: iteration 29, average log likelihood -1.401267
[ Info: iteration 30, average log likelihood -1.401239
[ Info: iteration 31, average log likelihood -1.401212
[ Info: iteration 32, average log likelihood -1.401186
[ Info: iteration 33, average log likelihood -1.401161
[ Info: iteration 34, average log likelihood -1.401137
[ Info: iteration 35, average log likelihood -1.401115
[ Info: iteration 36, average log likelihood -1.401093
[ Info: iteration 37, average log likelihood -1.401072
[ Info: iteration 38, average log likelihood -1.401052
[ Info: iteration 39, average log likelihood -1.401033
[ Info: iteration 40, average log likelihood -1.401014
[ Info: iteration 41, average log likelihood -1.400996
[ Info: iteration 42, average log likelihood -1.400979
[ Info: iteration 43, average log likelihood -1.400963
[ Info: iteration 44, average log likelihood -1.400947
[ Info: iteration 45, average log likelihood -1.400932
[ Info: iteration 46, average log likelihood -1.400917
[ Info: iteration 47, average log likelihood -1.400902
[ Info: iteration 48, average log likelihood -1.400889
[ Info: iteration 49, average log likelihood -1.400875
[ Info: iteration 50, average log likelihood -1.400862
┌ Info: EM with 100000 data points 50 iterations avll -1.400862
└ 59.0 data points per parameter
32×26 Array{Float64,2}:
  0.44295     -0.386386   -0.548111   -0.447707    0.0720675   0.308751    -0.333811    0.484804   -0.194341     0.309912   -0.269741    0.0748886   0.0876006   -0.129924    -0.535189    ┌ Info: 5
│   : avll =  = &quot;: avll = &quot;
│   avll =
│    50-element Array{Float64,1}:
│     -1.4037687746496603
│     -1.4037016885970888
│      ⋮
└     -1.4008618842430398
┌ Info: Total log likelihood: 
│   tll =
│    251-element Array{Float64,1}:
│     -1.4143215802565607
│     -1.4143397016163148
│     -1.414247681198198
│     -1.4141690079699842
│      ⋮
│     -1.400888534442179
│     -1.4008750137577681
└     -1.4008618842430398
0.128326   -0.563803   -0.0854028     0.0596044   -0.00211437   0.184581   -0.173669    0.00174609   0.0752916   0.0142817    0.278585
  0.46454     -0.226042   -0.326353    0.0627079   0.361832    0.0226169   -0.415618   -0.0874131  -0.211408     0.654737    0.816349    0.154114   -0.119863     0.348926    -0.219983    0.268612   -0.456904   -0.216354     -0.431391     0.322252     0.0526761  -0.436703    0.0825634   -0.425457    0.326508     0.123545
  0.364805    -0.0295962  -0.0791796  -0.168766    0.163536    0.0935353   -0.160742    0.339628    0.0384845   -0.537953    0.154865   -0.0698554  -0.345303    -0.205464    -1.11172     0.17154     0.617952    0.217317     -0.159337    -0.126266     0.171781   -0.210722   -0.1505       0.527752    0.386311    -0.255969
  0.59194     -0.437092   -0.204827    0.0187713   0.0978109  -0.150367    -0.297416    0.301659   -0.117304    -0.109799   -0.376201    0.557215    0.0533173    0.0145375    0.235665   -0.299019    0.578551    0.17954      -0.541268    -0.245662    -0.205944   -0.481771   -0.487027     0.325471   -0.159374     0.338733
 -0.0239263   -0.123931   -0.340348   -0.163799   -0.35734    -0.682124     0.175621   -0.305015    0.178839     0.316907   -0.289062    0.281514   -0.603639     0.185804     0.121408    0.518512   -0.173345   -0.409845     -0.908652    -0.523742     0.550814    0.110604   -0.268585    -0.0387857   0.140841    -0.10037
 -0.24041      0.577436    0.100504    0.314856   -0.461859   -0.161151     0.0637579   0.758889    0.0731932    0.0475233  -0.831897    0.440723   -0.553104    -0.486136    -0.0631723   0.471439    0.589246   -0.0733523    -0.468267     0.104311     0.0083058   0.0749308  -0.666414     0.429273   -0.315976     0.11708
  0.0319656   -0.176334   -0.186781    0.233483    0.191088   -0.356187    -0.0782991   0.58912     0.27014     -0.323208    0.171509    0.271194    0.514566    -0.0764765   -0.112373    0.182976   -0.0559292  -0.165143     -0.394674    -0.174843    -0.631852    0.214418    0.146396    -0.206022    0.672002     0.484075
 -0.511224     0.124066   -0.170572    0.218846   -0.115046    0.0455387    0.836174    0.571404    0.203666     0.231514    0.253118    0.342935   -0.200042     0.496511    -0.131771    0.196614   -0.0630631   0.273435     -0.125385     0.151745    -0.482356   -0.315896    0.551838     0.165836    0.141334    -0.338909
 -0.201293     0.260068   -0.168224    0.26419     0.487656   -0.263856    -0.170311   -0.538654   -0.18012     -0.174733    0.334006   -0.416012    0.189576     0.00952471  -0.236644    0.349191   -0.0189182   0.0492687     0.11653     -0.247356     0.0861116   0.277999   -0.163657     0.182522    0.553048    -0.573195
 -0.113735     0.301221    0.0453613  -0.0175488   0.374446   -0.315663     0.218613    0.217701   -0.24377      0.0326046   0.264215   -0.377566   -0.19632     -0.0574732   -0.566855    0.588685    0.20455    -0.397381     -0.102376    -0.098749     0.111623    0.235093    0.196695     0.202494    0.354616     0.287108
 -0.381158     0.57042    -0.163925    0.0296438  -0.164053    0.368915    -0.146085   -0.500438    0.456045    -0.342645   -0.0482669  -0.524465    0.340622    -0.355991     0.579854    0.287104   -0.881393    0.000912945   0.196232     0.11         0.126266    0.465263   -0.195057    -0.350124   -0.00940616  -0.239315
 -0.697411     0.268695    0.574929    0.0420182  -0.337192    0.0612627    0.649923   -0.0226591  -0.00996428   0.301847    0.119066   -0.455906    0.00415905  -0.282753     0.338224    0.0999245  -0.0359399  -0.201094      0.2394       0.360499     0.0458333   0.463874    0.643565    -0.328983    0.026351     0.0102564
  0.00815961  -0.0253853   0.0984223  -0.195341   -0.169642   -0.0288013    0.0616712  -0.031139    0.0111532   -0.0782848  -0.472654   -0.242187    0.0892872    0.0139761    0.159978   -0.0503288   0.114271   -0.0314052     0.153686    -0.0493375    0.0169556   0.146638   -0.135041     0.370247   -0.226186    -0.0397851
  0.0394211   -0.16782     0.0465238  -0.0848181  -0.185405    0.0119681    0.0229795   0.0236813   0.0178662    0.0654428   0.164203   -0.082035   -0.0261283    0.0217765    0.0270083  -0.011132   -0.180153    0.056549     -0.0603946    0.0986242   -0.0049876  -0.0721092   0.0477748   -0.130423    0.12882     -0.104484
  0.126567     0.180368   -0.080468    0.0363677   0.444711    0.181672    -0.150713   -0.0901969   0.144481    -0.0540978   0.0750692   0.645968    0.0718584    0.588551     0.0624599  -0.275607    0.186368    0.104472      0.359315     0.0683243   -0.0429636   0.049394    0.181461     0.0202399  -0.236958    -0.284476
 -0.148127     0.318019   -0.105815    0.52254     0.103134    0.118855    -0.149471    0.0704901   0.0679814   -0.0561676   0.213919    0.475808   -0.180632    -0.28762      0.118048   -0.0559069   0.0555818  -0.0483898    -0.174767    -0.0943441    0.377748   -0.149982   -0.231566    -0.403456   -0.240813     0.306103
 -0.106856    -0.303134    0.233882   -0.080912   -0.820742   -0.202788     0.0241188   0.187503    0.00175119   0.23143    -0.254527   -0.73243     0.00772564  -0.750641    -0.122135    0.17841    -0.355913   -0.0107316    -0.5336       0.300927     0.0755524  -0.177617   -0.34898     -0.144346    0.396486    -0.263411
  0.172101    -0.205441   -0.110763    0.0726207  -0.279249   -0.407028     0.0153774  -0.50385    -0.539153    -0.309897    0.396821   -0.600484   -0.178677    -0.852879     0.0599585   0.14499    -0.427687    0.224609      0.00425397  -0.322168     0.416475   -0.124513   -0.292567     0.0245811  -0.0426638    0.411969
 -0.0459121    0.009812    0.0238744  -0.10386     0.042579    0.00729155   0.112361   -0.0845487  -0.0662748    0.0857643   0.0220739  -0.117459   -0.0744184   -0.0180957   -0.0345036   0.0606525  -0.0212343   0.0425915     0.00216191   0.0028469    0.104773   -0.0306167   0.111064     0.161528    0.00353169  -0.175192
 -0.258247    -0.047719   -0.41911     0.696368   -0.0254245  -0.306049     0.111595    0.896498   -0.295184    -0.215569    0.368933    0.0173045   0.758733    -0.369447    -0.0790307   0.214846   -0.0699648  -0.403051     -0.348579    -0.0488998   -0.187149   -0.0810038  -0.0737538   -0.209665    0.158051     0.840502
  0.303818    -0.305087   -0.0434111  -0.573341   -0.159076    0.166705     0.26748    -0.493112    0.037414     0.301231   -0.321482   -0.416367   -0.533288     0.401534    -0.181489   -0.208246   -0.101291    0.521014      0.497773    -0.117533    -0.335435    0.220368    0.301485     0.523697   -0.065655    -0.520595
 -0.160828     0.411672   -0.303393   -0.311682    0.650946    0.154332     0.31983    -0.0638508  -0.412998     0.440239   -0.208054   -0.416851    0.15727      0.123712    -0.133659   -0.600434    0.125115   -0.081198      0.176115     0.23774      0.436739   -0.546768    0.198412     0.661568   -0.256552    -0.380993
 -0.117601     0.135958    0.211251   -0.32015    -0.048685    0.492001    -0.165422    0.527154    0.215188    -0.123056    0.0251579  -0.161855   -0.0127326   -0.156023    -0.600124    0.310756    0.361362    0.107315      0.146353    -0.0136682    0.0120775   1.20186     0.356888    -0.492874    0.121654     0.0828888
 -0.100134    -0.14815     0.238675    0.062529   -0.0547728   0.114261    -0.41368     0.219373    0.0105338    0.502589   -0.43641    -0.0562645  -0.0627945    1.0266      -0.323512    0.472252    0.435718   -0.490782      0.0111567    0.172047    -0.0599281   0.680312    0.0116496    0.220016    0.225568    -0.589744
 -0.282507    -0.615344   -0.155032   -0.0371454  -0.451781   -0.552582     0.267382   -0.10557    -0.120504    -0.0705113  -0.0723475  -0.26443     0.421165     0.425181     0.875502   -0.216887   -0.281282    0.143385     -0.0773559   -0.620441    -0.65262     0.373673    0.319484    -0.196126   -0.234344     0.25842
 -0.290727     0.352957    0.365209    0.14049     0.356191   -0.396669     0.220106   -0.214133    0.139435     0.0796619  -0.477332    0.11981     0.741907     0.0206678    0.812674   -0.310089    0.248401   -0.546111     -0.0202735   -0.530625     0.3015      0.282132   -0.282898    -0.0816079  -0.412122    -0.124477
 -0.125979    -0.409893    0.171392   -0.183236    0.313184    0.268402    -0.591905   -0.803771    0.0107427   -0.139031    0.718292   -0.266374    0.058891    -0.210817     0.0170854  -0.774645   -0.146929    0.430041      0.487142     0.490547    -0.214716    0.25442     0.493431    -0.115342    0.59        -0.448125
  0.129485    -0.172276    0.0512729  -0.108321    0.566698   -0.443574     0.309313   -0.415147    0.286315    -0.662651    0.195686   -0.39478     0.678939     0.24993      0.432436    0.514561   -0.378741   -0.0243754     0.321752    -0.169176    -0.908577   -0.0127552   0.334057     0.580128    0.416709     0.191429
  0.436662     0.0472833   0.297019    0.254796   -0.520355    0.0697646    0.295788   -0.68259    -0.043801    -0.0832164   0.0985378   0.24686    -0.213415    -0.00709505   0.710225   -0.666945    0.106011    0.715507      0.388671    -0.0895413    0.103014   -0.36226    -0.194427     0.0206222  -0.558856    -0.492398
  0.0598099   -0.0241613   1.08577     0.343981   -0.436223   -0.188768    -0.288457   -0.833486    0.987209    -0.511299    0.130005    0.457423   -0.621478     0.488465     0.461022    0.4546      0.245491    0.346901      0.257016     0.169792     0.10129    -0.242552   -0.50047      0.0624144   0.0886939   -0.0561062
  0.297756     0.16212    -0.171467    0.0511122   0.260088    0.426742    -0.454659    0.35869     0.0682552    0.027345   -0.278857    1.27668     0.0969519    0.428347     0.129388   -0.317493    0.255184    0.0521465     0.329867     0.117815    -0.106094    0.0457365  -0.0636327   -0.342702   -0.507658     0.125387
 -0.227659     0.0696642   0.367562   -0.26339    -0.26144     0.936613     0.300745    0.337378    0.20917     -0.269118    0.740867    0.208798    0.212094    -0.280329    -0.0661738  -0.301917   -0.321099   -0.154332      0.371093     0.552274     0.338694   -0.198494    0.409364    -0.514104   -0.421955     0.151174[ Info: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.400849
[ Info: iteration 2, average log likelihood -1.400837
[ Info: iteration 3, average log likelihood -1.400825
[ Info: iteration 4, average log likelihood -1.400813
[ Info: iteration 5, average log likelihood -1.400802
[ Info: iteration 6, average log likelihood -1.400790
[ Info: iteration 7, average log likelihood -1.400780
[ Info: iteration 8, average log likelihood -1.400769
[ Info: iteration 9, average log likelihood -1.400759
[ Info: iteration 10, average log likelihood -1.400749
┌ Info: EM with 100000 data points 10 iterations avll -1.400749
└ 59.0 data points per parameter
[ Info: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
kind full, method kmeans
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       9.084291e+05
      1       6.916182e+05      -2.168110e+05 |       32
      2       6.789843e+05      -1.263384e+04 |       32
      3       6.741444e+05      -4.839935e+03 |       32
      4       6.716665e+05      -2.477848e+03 |       32
      5       6.700722e+05      -1.594300e+03 |       32
      6       6.688667e+05      -1.205571e+03 |       32
      7       6.679651e+05      -9.015790e+02 |       32
      8       6.672241e+05      -7.409943e+02 |       32
      9       6.665810e+05      -6.430647e+02 |       32
     10       6.659975e+05      -5.835143e+02 |       32
     11       6.654760e+05      -5.215412e+02 |       32
     12       6.649798e+05      -4.961353e+02 |       32
     13       6.645353e+05      -4.445838e+02 |       32
     14       6.641400e+05      -3.952520e+02 |       32
     15       6.637937e+05      -3.463069e+02 |       32
     16       6.635000e+05      -2.936530e+02 |       32
     17       6.632305e+05      -2.695414e+02 |       32
     18       6.630072e+05      -2.232665e+02 |       32
     19       6.628340e+05      -1.732434e+02 |       32
     20       6.626906e+05      -1.434221e+02 |       32
     21       6.625499e+05      -1.406470e+02 |       32
     22       6.624291e+05      -1.208005e+02 |       32
     23       6.623309e+05      -9.821836e+01 |       32
     24       6.622476e+05      -8.330959e+01 |       32
     25       6.621652e+05      -8.237636e+01 |       32
     26       6.620919e+05      -7.334157e+01 |       32
     27       6.620193e+05      -7.262078e+01 |       32
     28       6.619487e+05      -7.052709e+01 |       32
     29       6.618774e+05      -7.137113e+01 |       32
     30       6.618183e+05      -5.902137e+01 |       32
     31       6.617688e+05      -4.952367e+01 |       32
     32       6.617264e+05      -4.243669e+01 |       32
     33       6.616832e+05      -4.312867e+01 |       32
     34       6.616482e+05      -3.499520e+01 |       32
     35       6.616177e+05      -3.055140e+01 |       32
     36       6.615866e+05      -3.107393e+01 |       32
     37       6.615572e+05      -2.937369e+01 |       32
     38       6.615258e+05      -3.145493e+01 |       32
     39       6.614939e+05      -3.186220e+01 |       32
     40       6.614626e+05      -3.130247e+01 |       32
     41       6.614318e+05      -3.083988e+01 |       32
     42       6.614033e+05      -2.846391e+01 |       32
     43       6.613800e+05      -2.336159e+01 |       32
     44       6.613570e+05      -2.300986e+01 |       32
     45       6.613333e+05      -2.363358e+01 |       32
     46       6.613088e+05      -2.447612e+01 |       32
     47       6.612866e+05      -2.227061e+01 |       32
     48       6.612634e+05      -2.318408e+01 |       32
     49       6.612404e+05      -2.299773e+01 |       32
     50       6.612177e+05      -2.272551e+01 |       32
K-means terminated without convergence after 50 iterations (objv = 661217.6655061485)
┌ Info: K-means with 32000 data points using 50 iterations
└ 37.0 data points per parameter
[ Info: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
[ Info: iteration 1, average log likelihood -1.412433
[ Info: iteration 2, average log likelihood -1.407587
[ Info: iteration 3, average log likelihood -1.406310
[ Info: iteration 4, average log likelihood -1.405386
[ Info: iteration 5, average log likelihood -1.404378
[ Info: iteration 6, average log likelihood -1.403362
[ Info: iteration 7, average log likelihood -1.402600
[ Info: iteration 8, average log likelihood -1.402158
[ Info: iteration 9, average log likelihood -1.401916
[ Info: iteration 10, average log likelihood -1.401769
[ Info: iteration 11, average log likelihood -1.401664
[ Info: iteration 12, average log likelihood -1.401583
[ Info: iteration 13, average log likelihood -1.401514
[ Info: iteration 14, average log likelihood -1.401454
[ Info: iteration 15, average log likelihood -1.401400
[ Info: iteration 16, average log likelihood -1.401351
[ Info: iteration 17, average log likelihood -1.401304
[ Info: iteration 18, average log likelihood -1.401260
[ Info: iteration 19, average log likelihood -1.401219
[ Info: iteration 20, average log likelihood -1.401179
[ Info: iteration 21, average log likelihood -1.401141
[ Info: iteration 22, average log likelihood -1.401104
[ Info: iteration 23, average log likelihood -1.401070
[ Info: iteration 24, average log likelihood -1.401037
[ Info: iteration 25, average log likelihood -1.401007
[ Info: iteration 26, average log likelihood -1.400978
[ Info: iteration 27, average log likelihood -1.400951
[ Info: iteration 28, average log likelihood -1.400925
[ Info: iteration 29, average log likelihood -1.400902
[ Info: iteration 30, average log likelihood -1.400879
[ Info: iteration 31, average log likelihood -1.400859
[ Info: iteration 32, average log likelihood -1.400839
[ Info: iteration 33, average log likelihood -1.400821
[ Info: iteration 34, average log likelihood -1.400804
[ Info: iteration 35, average log likelihood -1.400788
[ Info: iteration 36, average log likelihood -1.400773
[ Info: iteration 37, average log likelihood -1.400759
[ Info: iteration 38, average log likelihood -1.400746
[ Info: iteration 39, average log likelihood -1.400733
[ Info: iteration 40, average log likelihood -1.400721
[ Info: iteration 41, average log likelihood -1.400710
[ Info: iteration 42, average log likelihood -1.400699
[ Info: iteration 43, average log likelihood -1.400689
[ Info: iteration 44, average log likelihood -1.400679
[ Info: iteration 45, average log likelihood -1.400669
[ Info: iteration 46, average log likelihood -1.400660
[ Info: iteration 47, average log likelihood -1.400652
[ Info: iteration 48, average log likelihood -1.400643
[ Info: iteration 49, average log likelihood -1.400635
[ Info: iteration 50, average log likelihood -1.400627
┌ Info: EM with 100000 data points 50 iterations avll -1.400627
└ 59.0 data points per parameter
32×26 Array{Float64,2}:
  0.0814261    0.141003   -0.104498     0.20501    -0.191756    -0.77484      -0.106927    0.443895    -0.232917    0.346987    -0.870138    0.129878   -0.695881    -0.129078   -0.11332     0.661719     0.672495    -0.124262    -0.58261     -0.288109    -0.387127     0.321082    -0.549749     0.632639    0.168641    0.158151
 -0.00480447  -0.198864   -0.277888     0.581537    0.0200892   -0.407359      0.120021    0.629688     0.260469   -0.716337     0.319461    0.0253752   0.741109    -0.264862   -0.0866143   0.228597    -0.121211     0.0322536   -0.133543    -0.222033    -0.794989    -0.0039811   -0.0659342   -0.0293364   0.486932    0.690133
  0.0507019    0.0878626   0.326692    -0.242459   -0.0125944   -0.053389      0.274759    0.363275    -0.0335945  -0.443683     0.15439    -0.348245   -0.365576    -0.289436   -0.848849    0.0548976    0.781964    -0.0246649   -0.354789    -0.117032     0.093067     0.00342597   0.0900673    0.518794    0.364656   -0.13824
 -0.015638    -0.350054   -0.177975    -0.480185   -0.147108    -0.146484      0.428106   -0.173835    -0.0744688   0.0536874   -0.139008   -0.619591   -0.140294     0.496359   -0.0918291  -0.126501    -0.106721     0.388276     0.398437    -0.397281    -0.642568     0.584875     0.739841     0.305677   -0.0893141  -0.301343
 -0.498802     0.444029    0.069994    -0.0782922  -0.204655     0.184692      0.0588715  -0.37505      0.267849   -0.194121    -0.0790285  -0.904887    0.320067    -0.387234    0.521451    0.275634    -0.505247    -0.0527083    0.161014     0.118379    -0.00110764   0.703287    -0.00651261  -0.221801    0.178385   -0.258984
  0.161976    -0.0456138   0.718415     0.180213   -0.801058     0.038575      0.185867   -0.733519     0.827577   -0.702195    -0.132893    0.264429   -0.663329     0.264778    0.375426   -0.0671664    0.478986     0.753333     0.507048    -0.290567     0.0111938   -0.0875838   -0.503465     0.231106   -0.223321   -0.313744
 -0.174607     0.482034    0.581799     0.012535    0.779327    -0.525124     -0.25403    -0.241499     0.367957   -0.105218    -0.395546    0.428785    0.435229     0.0467556   0.403613    0.0953927    0.413305    -0.442367     0.00391653  -0.545421     0.66352      0.295237    -0.362156    -0.248851   -0.230104   -0.322533
  0.578542     0.342546    0.0424064    0.596397   -0.0900837    0.40392      -0.0820551  -0.0339973   -0.210434    0.201601     0.407544    0.587143   -0.397321     0.0658881   0.235168    0.177071     0.296053    -0.134876    -0.179923     0.13794      0.582643    -0.696731    -0.477307     0.332614   -0.609933    0.125769
  0.627157    -0.727859   -0.46483     -0.222328    0.259861    -0.00332382   -0.430262    0.145007    -0.295063   -0.0801183   -0.0910038   0.380806    0.110143    -0.0846161  -0.155667   -0.0781553    0.109835     0.157679    -0.188705    -0.318239     0.110823    -0.562061    -0.254131     0.2101     -0.115252    0.26259
 -0.0560822    0.281466   -0.0673996   -0.15814     0.226958    -0.256267      0.545444   -0.30664     -0.297468    0.103867    -0.525182   -0.436115    0.326618     0.195137    0.380967   -0.572516     0.102588    -0.0105987    0.0904868   -0.0369468    0.200842    -0.56307      0.0219515    0.942162   -0.379284   -0.376541
  0.0170589    0.0494091   0.161971    -0.165243    0.0407639    0.458836     -0.556159    0.468106     0.216907    0.10567     -0.0163943  -0.0486824  -0.101381     0.470296   -0.703296    0.763792     0.251887    -0.231045     0.016385     0.190809     0.0454261    0.890416     0.105878    -0.261434    0.31226    -0.0458704
 -0.278353    -0.267923    0.439105    -0.209446    0.0206916    0.321957     -0.0326927  -0.123268     0.12468     0.947398    -0.34801     0.483378   -0.564079     0.5141      0.0403354  -0.177209     0.380337    -0.07244     -0.376035     0.231773     0.0996128   -0.0661861    0.247483     0.153378   -0.0352229  -1.10034
 -0.0642443    0.0370207   0.0545476    0.0611976   0.00502405  -0.0679923     0.0546853   0.00531212   0.217935   -0.181559    -0.132396    0.258809    0.0984965    0.099029    0.383921   -0.205532     0.158671     0.100174    -0.00683523  -0.160329    -0.142713     0.123884     0.0140171   -0.089732   -0.17504     0.0600549
  0.123353    -0.0425753  -0.183954     0.0368206   0.225547    -0.120151     -0.249138    0.103971    -0.0446399  -0.0155436    0.239681   -0.0112988   0.0316189   -0.0813413  -0.268223    0.16547     -0.191199    -0.0422053   -0.193342    -0.0450288    0.00537536   0.00327014  -0.0257899   -0.127048    0.348682    0.0473524
 -0.177062    -0.0716008   0.0758573   -0.344143    0.498142     0.768565     -0.0971733  -0.0285911   -0.0167998  -0.432167     0.626235   -0.0154711   0.753798     0.250759   -0.335896   -0.471887    -0.265544    -0.259323     0.623564     0.381102     0.172126    -0.45777      0.607679    -0.387152   -0.40539    -0.18309
 -0.308774     0.33113    -0.00318734   0.577592   -0.262087     0.168425     -0.493615    0.333524     0.248642   -0.0967445    0.0362092   0.543611   -0.124986    -0.659436   -0.280121   -0.0970108    0.0511276    0.0894825   -0.676671     0.33123      0.417786    -0.248982    -0.789197    -0.521724    0.153184    0.145229
  0.0495544   -0.0444788  -0.60568      0.0461054   0.33872     -0.163212     -0.12495     0.440033    -0.433779    0.930601     0.437862    0.271238    0.189575     0.194811   -0.185279    0.0922332   -0.37487     -0.625363    -0.595659     0.365052    -0.247042    -0.179834     0.397881    -0.364987    0.225418    0.531158
 -0.12374      0.453402   -0.191819    -0.0265539  -0.0597483    0.193702      0.0341083   0.711181     0.172255   -0.162539    -0.924721    0.563888   -0.278336    -0.397331    0.226135   -0.234435     0.241741     0.313586     0.182005     0.159617     0.0310558    0.0988491    0.010384     0.0056417  -0.978241    0.439554
 -0.348306     0.069447    0.550683    -0.0931527  -0.592121     0.582319      0.555905    0.629596     0.139885    0.0472955    0.417847    0.123659   -0.0244711   -0.620005   -0.0438551  -0.0196246   -0.00234092  -0.199795     0.21564      0.23973      0.343032     0.423856     0.523511    -0.49323    -0.114208    0.364266
  0.335168     0.0262858   0.387664     0.0927949  -0.19402      0.190164     -0.215431   -0.519558     0.33227     0.189755     0.248177    0.162975    0.145899     0.0578968   0.661508   -0.493514    -0.570776     0.443704     0.409326     0.644472    -0.0426912   -0.135264    -0.0804095   -0.412521   -0.12122    -0.409609
 -0.295282     0.0826376  -0.0547513    0.0683376  -0.0635891    0.0321092     0.636322    0.330969     0.131399    0.308292     0.176078    0.183492   -0.170002     0.477779   -0.15157     0.168087    -0.0699743    0.0714982   -0.00366798   0.163837    -0.247883    -0.164387     0.472548     0.0727708   0.138136   -0.296668
 -0.0213877    0.0634018  -0.0161576    0.0617669   0.0404804   -0.000530309   0.0473042   0.0148969    0.0112791  -0.0336315    0.0381726   0.154404    0.0892742    0.0153644   0.146986    0.0418922   -0.0235461   -0.00335777  -0.00922135   0.0218671    0.0646001   -0.0623016   -0.0209761    0.0434982  -0.109473    0.0203933
 -0.0322258   -0.0838318   0.124585    -0.199347   -0.110455     0.167454     -0.0979135  -0.112502    -0.217405    0.140393    -0.180527   -0.304588   -0.00381763   0.0897073  -0.152526    0.0314651   -0.0178056   -0.0781636    0.333952     0.173208     0.169676     0.115267    -0.0734191    0.292297   -0.0670787  -0.196103
  0.859809     0.353135   -0.173607    -1.01074     0.412675     0.378005     -0.163116   -0.102976    -0.171114    0.393364    -0.199707   -0.129787   -0.457215    -0.0952052  -0.882703   -0.120058    -0.39819      0.309703     0.487648    -0.0302768    0.208904     0.0331137   -0.291976     0.379565    0.0733333  -0.60997
  0.115596    -0.213163    0.0480262    0.149081   -0.493475    -0.603866      0.0475218  -0.58157     -0.346383   -0.0150716    0.315486   -0.19042    -0.457688    -0.358753    0.471871   -0.00186918  -0.480991     0.255298    -0.0878442   -0.306074     0.221912    -0.0474413   -0.10113     -0.240263   -0.0212316   0.104862
 -0.605161     0.810028   -0.51975      0.314595    0.653241    -0.0312818     0.342982   -0.332515    -0.232632   -0.22899      0.459796   -0.152685    0.0322449   -0.278812   -0.153956    0.111836     0.164838    -0.0267779    0.0893108   -0.144423     0.503751     0.105269     0.10054     -0.0169648   0.129906    0.155743
  0.380514     0.166632   -0.331769     0.105943    0.378931     0.3155       -0.338786    0.219422     0.148668   -0.19443     -0.146226    1.11602     0.16069      1.0111      0.129441   -0.470737     0.548243     0.0658098    0.401185    -0.0328055   -0.359778     0.282192     0.0440546   -0.0493627  -0.181287   -0.147713
 -0.121668    -0.516897   -0.0891382   -0.119807   -0.123036     0.678373     -0.365884   -0.516315    -0.128415   -0.0756942    0.59499    -0.41928    -0.288182    -0.458427   -0.421939   -0.738339     0.108577     0.761669     0.428912     0.460856    -0.275679     0.0922124    0.32415      0.21472     0.336975   -0.328561
  0.0816826   -0.336007   -0.162408    -0.112802   -0.584455    -0.212039      0.135082    0.249037     0.408873    0.154006    -0.486097   -0.109312    0.102231     0.0663836   0.211478    0.250394    -0.204649    -0.514416    -0.782235    -0.293229     0.223294     0.0119078   -0.387486    -0.154955   -0.0891374   0.0758489
 -0.435009    -0.29648     0.0976994    0.293672   -0.275898    -0.440839      0.448308    0.0614766   -0.362611    0.153332    -0.249385   -0.0626726   0.912251     0.164764    1.07203    -0.356041     0.0116591   -0.293017     [ Info: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
0.140926    -0.63163     -0.289797     0.312291    -0.00630852  -0.2897     -0.424965    0.341853
 -0.125467    -0.372802   -0.101548    -0.0816117  -0.41383     -0.353209      0.116446    0.0275279   -0.286445    0.00557139   0.021693   -1.02956     0.0465017   -0.6469     -0.401696    0.558542    -0.443728    -0.0799875   -0.421331    -0.00633028   0.320021    -0.207664    -0.135513     0.110573    0.536383   -0.0427608
  0.156441    -0.221518    0.221031    -0.0671528   0.758733    -0.433433      0.0837915  -0.6769       0.222239   -0.40454      0.417358   -0.272107    0.477478     0.357868    0.415918    0.509758    -0.252736    -0.0956033    0.25449     -0.0951106   -0.718492     0.0914202    0.537312     0.427382    0.534597   -0.0196502[ Info: iteration 1, average log likelihood -1.400620
[ Info: iteration 2, average log likelihood -1.400612
[ Info: iteration 3, average log likelihood -1.400605
[ Info: iteration 4, average log likelihood -1.400599
[ Info: iteration 5, average log likelihood -1.400592
[ Info: iteration 6, average log likelihood -1.400586
[ Info: iteration 7, average log likelihood -1.400579
[ Info: iteration 8, average log likelihood -1.400573
[ Info: iteration 9, average log likelihood -1.400568
[ Info: iteration 10, average log likelihood -1.400562
┌ Info: EM with 100000 data points 10 iterations avll -1.400562
└ 59.0 data points per parameter
[ Info: Initializing GMM, 2 Gaussians diag covariance 2 dimensions using 900 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.678561e+05
      1       2.230230e+04      -1.455538e+05 |        2
      2       7.823675e+03      -1.447862e+04 |        0
      3       7.823675e+03       0.000000e+00 |        0
K-means converged with 3 iterations (objv = 7823.67549422947)
┌ Info: K-means with 900 data points using 3 iterations
└ 150.0 data points per parameter
[ Info: Running 10 iterations EM on full cov GMM with 2 Gaussians in 2 dimensions
[ Info: iteration 1, average log likelihood -2.043155
[ Info: iteration 2, average log likelihood -2.043154
[ Info: iteration 3, average log likelihood -2.043154
[ Info: iteration 4, average log likelihood -2.043154
[ Info: iteration 5, average log likelihood -2.043154
[ Info: iteration 6, average log likelihood -2.043154
[ Info: iteration 7, average log likelihood -2.043154
[ Info: iteration 8, average log likelihood -2.043154
[ Info: iteration 9, average log likelihood -2.043154
[ Info: iteration 10, average log likelihood -2.043154
┌ Info: EM with 900 data points 10 iterations avll -2.043154
└ 81.8 data points per parameter
   Testing GaussianMixtures tests passed 
</pre>
      </div>


  </div>
  </body>

  <script type="text/javascript">// handle collapsibles
var coll = document.getElementsByClassName("collapsible");
for (var i = 0; i < coll.length; i++) {
    coll[i].textContent = "▸ " + coll[i].textContent
    coll[i].addEventListener("click", function() {
        this.classList.toggle("active");
        var content = this.nextElementSibling;
        if (content.style.display === "block") {
            this.textContent = "▸" + this.textContent.substr(1)
            content.style.display = "none";
        } else {
            this.textContent = "▾" + this.textContent.substr(1)
            content.style.display = "block";
        }
    });
}
</script>
</html>

